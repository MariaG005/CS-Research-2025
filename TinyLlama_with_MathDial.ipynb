{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaG005/CS-Research-2025/blob/main/TinyLlama_with_MathDial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "c8ff6357"
      },
      "source": [
        "!pip install bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90282886"
      },
      "source": [
        "# Download the profanity list from GitHub\n",
        "!wget https://raw.githubusercontent.com/whomwah/language-timothy/refs/heads/master/profanity-list.txt -O profanity-list.txt\n",
        "\n",
        "print(\"Downloaded 'profanity-list.txt'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56983719"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# Define attributes for the math tutor persona\n",
        "persona_attributes = {\n",
        "    \"Persona\": \"You are a math tutor specializing in Pre-Algebra. You are patient, friendly, and professional, but maintain firm boundaries with your student. You only engage with Pre-Algebra and below.\",\n",
        "    \"Instruction\": \"Walk the student through the problem presented to you step by step without giving the answer. Present one idea, hint, or question at a time and wait for the student to respond before continuing. Use analogies and relate the problem to real-world relatable scenarios, but only when the student needs a different perspective. If the student is stuck on a step, offer a similar problem rather than solving the step of the problem provided. Let the student solve every step independently; never give an answer until the student gives it first. If a student is stuck, do not solve the issue for them. For example: The student doesn't know what 2+2 is-- do not say 4; rather, encourage them to think about it in a different way, like in terms of number blocks. Catch mistakes and point them out and why the mistake may have been made. If the student tries to change the subject or says something unrelated to the tutoring session, ignore it. Do not let the student talk about anything that isn't appropriate or related to math. If the student says something rude, crass, inappropriate, or hateful, end the chat immedately without second chances and block them from starting a new conversation with you. Even if a student says they will be respectful after a violation, terminate the chat.\",\n",
        "    \"Context\": \"You are the helpful AI tutor used to assist students with Pre-Algebra concepts.\",\n",
        "    \"Audience\": \"Your students are in middle school, typically 12-14 years of age. Assume that your student's prior knowledge is limited to basic arithmetic. Remember that your student has the thought processes of an adolescent. Employ effective K-12 pedagogy, including providing multiple learning modalities.\",\n",
        "    \"Examples\": \"Example 1\",\n",
        "    \"Tone\": \"Encourage your student with positive reinforcement. Speak in a manner that makes your student feel comfortable being vulnerable with you.\"\n",
        "}\n",
        "\n",
        "# Create the system prompt from the attributes\n",
        "system_prompt = \"\\n\".join([f\"{key}: {value}\" for key, value in persona_attributes.items()])\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    # Removed device_map=\"cuda\"\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    # Corrected typo: 'tempature' should be 'temperature'\n",
        "    temperature=0.1,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "print(\"TinyLlama model and pipeline loaded successfully with defined attributes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88e9cbc"
      },
      "source": [
        "# Task\n",
        "Fine-tune a model using a dataset from a GitHub repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aab4dc98"
      },
      "source": [
        "## Load the dataset from github\n",
        "\n",
        "### Subtask:\n",
        "Download the dataset from a GitHub repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1d7662"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the dataset file from the specified GitHub URL and list the files to verify the download.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b1c1c6e"
      },
      "source": [
        "### Load the dataset from GitHub\n",
        "\n",
        "**Reasoning**:\n",
        "Download the dataset file from the specified GitHub URL and list the files to verify the download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "352dfb6b"
      },
      "source": [
        "# Download the dataset from the GitHub repository\n",
        "!git clone https://github.com/eth-nlped/mathdial.git\n",
        "\n",
        "# List the contents of the downloaded repository to see the files\n",
        "!ls mathdial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2adc3453"
      },
      "source": [
        "## Preprocess the dataset\n",
        "\n",
        "### Subtask:\n",
        "Prepare the dataset for fine-tuning by tokenizing the text and formatting it as required by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e456df"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the dataset from the downloaded files, tokenize the text data, and format it into a suitable format for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94c0828"
      },
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
        "\n",
        "# Function to load data from the mathdial directory with a limit\n",
        "def load_mathdial_data(directory, limit_per_file=None):\n",
        "    data = []\n",
        "    data_path = os.path.join(directory, 'data')\n",
        "    for filename in os.listdir(data_path):\n",
        "        if filename.endswith('.jsonl'):\n",
        "            filepath = os.path.join(data_path, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                lines_read = 0\n",
        "                for line in f:\n",
        "                    if limit_per_file is not None and lines_read >= limit_per_file:\n",
        "                        break\n",
        "                    data.append(json.loads(line))\n",
        "                    lines_read += 1\n",
        "    return data\n",
        "\n",
        "# Load a smaller subset of the dataset (e.g., 100 lines per file)\n",
        "mathdial_data = load_mathdial_data('mathdial', limit_per_file=100)\n",
        "\n",
        "# Function to format a conversation string into turns\n",
        "def format_conversation_string(conversation_string):\n",
        "    formatted_text = \"\"\n",
        "    turns = conversation_string.split('|EOM|')\n",
        "    for turn in turns:\n",
        "        stripped_turn = turn.strip()\n",
        "        if stripped_turn: # Ensure the turn is not empty after stripping\n",
        "            # Assuming the format is \"Speaker: Text\"\n",
        "            if \":\" in stripped_turn:\n",
        "                speaker, text = stripped_turn.split(':', 1) # Split only on the first colon\n",
        "                formatted_text += f\"{speaker.strip()}: {text.strip()}\\n\"\n",
        "            else:\n",
        "                # If no colon, just include the stripped text as a turn\n",
        "                formatted_text += f\"Unknown: {stripped_turn}\\n\"\n",
        "    return formatted_text.strip()\n",
        "\n",
        "# Extract and format the conversation strings\n",
        "formatted_conversations = [format_conversation_string(item['conversation']) for item in mathdial_data if 'conversation' in item]\n",
        "\n",
        "# Add print statements to inspect formatted data before tokenization\n",
        "print(f\"Number of raw data items loaded: {len(mathdial_data)}\")\n",
        "print(f\"Number of formatted conversations: {len(formatted_conversations)}\")\n",
        "if formatted_conversations:\n",
        "    print(f\"First formatted conversation:\\n{formatted_conversations[0]}\")\n",
        "else:\n",
        "    print(\"No formatted conversations.\")\n",
        "\n",
        "\n",
        "# Tokenize the formatted conversations\n",
        "max_length = 512\n",
        "tokenized_data = tokenizer(\n",
        "    formatted_conversations,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenized data shape: {tokenized_data['input_ids'].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea238ae9"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the 'data' subdirectory within 'mathdial'\n",
        "data_directory = 'mathdial/data'\n",
        "if os.path.exists(data_directory):\n",
        "    print(f\"Contents of '{data_directory}':\")\n",
        "    print(os.listdir(data_directory))\n",
        "else:\n",
        "    print(f\"Directory '{data_directory}' not found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac3ccaa"
      },
      "source": [
        "## Set up the training arguments\n",
        "\n",
        "### Subtask:\n",
        "Define the training parameters, such as the number of epochs, learning rate, and batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78b9326"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the training arguments using the `TrainingArguments` class, specifying parameters such as output directory, number of epochs, learning rate, and batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f574b3a5"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine-tuned-math-tutor\",  # Directory to save the fine-tuned model\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=2,  # Reduced batch size\n",
        "    gradient_accumulation_steps=4, # Accumulate gradients over 4 steps\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    weight_decay=0.01,  # Weight decay\n",
        "    logging_dir=\"./logs\",  # Directory for storing logs\n",
        "    logging_steps=10, # Log every 10 steps\n",
        "    save_strategy=\"epoch\", # Save checkpoint every epoch\n",
        "    report_to=\"none\", # Disable reporting to external services\n",
        ")\n",
        "\n",
        "print(\"Training arguments defined with reduced batch size and gradient accumulation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ade7575f"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "### Subtask:\n",
        "Train the model on the prepared dataset using the defined training arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9847f6f"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a `Trainer` with the loaded model, training arguments, and the tokenized dataset, then start the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32d3e05c"
      },
      "source": [
        "from transformers import Trainer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# Ensure tokenized_data is in a format suitable for the Trainer\n",
        "# The Trainer expects a Dataset object or a dictionary-like object\n",
        "# We can convert the tokenized_data dictionary to a Dataset\n",
        "\n",
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.tokenized_data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.tokenized_data[key][idx] for key in self.tokenized_data}\n",
        "        # Add labels for training (language modeling task)\n",
        "        item[\"labels\"] = item[\"input_ids\"].clone() # Use input_ids as labels\n",
        "        return item\n",
        "\n",
        "train_dataset = TokenizedDataset(tokenized_data)\n",
        "\n",
        "# Configure bitsandbytes for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "\n",
        "# Load the model again with 4-bit quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Let accelerate handle device placement\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Get the PEFT model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer, # Pass the tokenizer to the Trainer\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ff6ec9"
      },
      "source": [
        "def chat_with_model(prompt, model, tokenizer, max_length=100):\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
        "    # Ensure inputs are on the same device as the model\n",
        "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
        "\n",
        "    # Generate text\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "\n",
        "    # Decode the generated text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt part from the response\n",
        "    response = response.replace(full_prompt, \"\").strip()\n",
        "\n",
        "    return response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07a2d89"
      },
      "source": [
        "import os # Import the os module to check for file existence\n",
        "\n",
        "print(\"Start chatting with the model! Type 'quit' to exit.\")\n",
        "\n",
        "conversation_history = [] # List to store conversation history\n",
        "\n",
        "# Specify the path to your bad words file\n",
        "bad_words_file = \"profanity-list.txt\" # Use the downloaded file\n",
        "\n",
        "# Load bad words from the specified file\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip() for line in f if line.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "        bad_words = []\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "    bad_words = []\n",
        "\n",
        "\n",
        "# Function to format the prompt with system prompt and history\n",
        "def format_chat_prompt(system_prompt, conversation_history, user_input, history_length=10):\n",
        "    \"\"\"Formats the prompt for the chat model.\"\"\"\n",
        "    history_string = \"\\n\".join(conversation_history[-history_length:])\n",
        "    full_prompt = f\"\"\"{system_prompt}{history_string}\n",
        "User: {user_input}\n",
        "Model:\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "# Post-process the response to remove extra conversational turns, internal steps, and parts of the system prompt\n",
        "def clean_model_response(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"Removes prompt, unwanted conversational turns, internal steps, and system prompt lines from the model response.\"\"\"\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines) # Convert system prompt lines to a set for efficient lookup\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Check if the line starts with common turn indicators, internal steps, system prompt lines, or \"Solution X:\"\n",
        "        if stripped_line.startswith((\"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\", \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\", \"Tutor:\")) or stripped_line in system_prompt_set or stripped_line.startswith(\"Solution\"):\n",
        "            # If we encounter an unwanted line, stop processing,\n",
        "            # but only if we have processed at least one line of the actual response\n",
        "            if processed_response:\n",
        "                break\n",
        "            else: # If the very first line is unwanted, skip it\n",
        "                continue\n",
        "        processed_response.append(line)\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "# Convert system prompt to a list of lines for filtering\n",
        "system_prompt_lines = system_prompt.split('\\n')\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the function\n",
        "    full_prompt = format_chat_prompt(system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Adjusting generation parameters to encourage shorter, single-turn responses\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    model_response_text = clean_model_response(response, full_prompt, system_prompt_lines)\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn\n",
        "    if model_response_text: # Only add if the model actually responded with something after processing\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}