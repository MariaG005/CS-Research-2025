{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b8f37c2a26de4827ab3953c2bd3c46e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9640a1c0eb2c4d47955be9e44188bd1b",
              "IPY_MODEL_2ad861c7dbcf41848101b6b5d216ef79",
              "IPY_MODEL_54f237e1b31f4443acc669e8adb3b741"
            ],
            "layout": "IPY_MODEL_b3a9877c2e974ff6b15c0532d110b6a2"
          }
        },
        "9640a1c0eb2c4d47955be9e44188bd1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9a8c822a3ac4081ba944c9efe134b94",
            "placeholder": "​",
            "style": "IPY_MODEL_355ea8f52c7e4593b8505a54db6045c6",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "2ad861c7dbcf41848101b6b5d216ef79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cb1331c36cb4ebeaca73086419e22c4",
            "max": 34173,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_98ce35f16e3a4d0a9aec43ff034e70f5",
            "value": 34173
          }
        },
        "54f237e1b31f4443acc669e8adb3b741": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e01672be73e048c2bca518b4b25d4428",
            "placeholder": "​",
            "style": "IPY_MODEL_bcd378dfdcf74be683c464c32f2def88",
            "value": " 34.2k/34.2k [00:00&lt;00:00, 3.76MB/s]"
          }
        },
        "b3a9877c2e974ff6b15c0532d110b6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a8c822a3ac4081ba944c9efe134b94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "355ea8f52c7e4593b8505a54db6045c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cb1331c36cb4ebeaca73086419e22c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98ce35f16e3a4d0a9aec43ff034e70f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e01672be73e048c2bca518b4b25d4428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcd378dfdcf74be683c464c32f2def88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3259e4420bf14d9f8d39fce4d414ae5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b920a8941d8b49a4860c23740947e64e",
              "IPY_MODEL_ccf5cea17f04445992802746176274bf",
              "IPY_MODEL_f3d30b8f0f6d491986f0e2cea57d8800"
            ],
            "layout": "IPY_MODEL_c3e772c83a1f49189ccca14187a0d42c"
          }
        },
        "b920a8941d8b49a4860c23740947e64e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15aa58e813d0422ba564f47b0b42ee86",
            "placeholder": "​",
            "style": "IPY_MODEL_faac305380cd489a8e1f358e05986d74",
            "value": "tokenizer.model: 100%"
          }
        },
        "ccf5cea17f04445992802746176274bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdeb9f79f4cd44318984558c39196944",
            "max": 4241003,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_62e0bf575a7541e08f413948f45555a0",
            "value": 4241003
          }
        },
        "f3d30b8f0f6d491986f0e2cea57d8800": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5dc135a11424c24a22ca7f7fffaf685",
            "placeholder": "​",
            "style": "IPY_MODEL_cfb45f9a19234ac4927b67b6d5f2ff6f",
            "value": " 4.24M/4.24M [00:00&lt;00:00, 6.52MB/s]"
          }
        },
        "c3e772c83a1f49189ccca14187a0d42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15aa58e813d0422ba564f47b0b42ee86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faac305380cd489a8e1f358e05986d74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdeb9f79f4cd44318984558c39196944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62e0bf575a7541e08f413948f45555a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f5dc135a11424c24a22ca7f7fffaf685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb45f9a19234ac4927b67b6d5f2ff6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c773ee2c94524082989583935da46deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_010e4511cd6448ad9cd8a0206a38429c",
              "IPY_MODEL_8676f782d39648eabb7c22ba6657d721",
              "IPY_MODEL_8c6f30ddc402405d96e63bc7d26a3fd0"
            ],
            "layout": "IPY_MODEL_8db85daf2c8044998b1f5c309144916d"
          }
        },
        "010e4511cd6448ad9cd8a0206a38429c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ba40ce259064c80a06095bf60dfd44e",
            "placeholder": "​",
            "style": "IPY_MODEL_ca5ecb46e8a547858081b1010a79287c",
            "value": "tokenizer.json: 100%"
          }
        },
        "8676f782d39648eabb7c22ba6657d721": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b1d07b25e1641be8d1746b3b607c501",
            "max": 17518497,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c50bcd39d3a4435eb3c614c432297655",
            "value": 17518497
          }
        },
        "8c6f30ddc402405d96e63bc7d26a3fd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b022f9d0754257b2ad116894d1866c",
            "placeholder": "​",
            "style": "IPY_MODEL_32b8bc4a81c348618f11f2eec3d70a5f",
            "value": " 17.5M/17.5M [00:00&lt;00:00, 35.6MB/s]"
          }
        },
        "8db85daf2c8044998b1f5c309144916d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ba40ce259064c80a06095bf60dfd44e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca5ecb46e8a547858081b1010a79287c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b1d07b25e1641be8d1746b3b607c501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c50bcd39d3a4435eb3c614c432297655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d7b022f9d0754257b2ad116894d1866c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b8bc4a81c348618f11f2eec3d70a5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81459dcb8ca64a929dcdf5e6e464b795": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaf20bd371d3482eac7b5154444f3fe6",
              "IPY_MODEL_f0967b92a29a479ba4253b146674a078",
              "IPY_MODEL_b5134ac716624efd8cefacd28d8832cf"
            ],
            "layout": "IPY_MODEL_91200418189d437cbbde5bb0957c0568"
          }
        },
        "aaf20bd371d3482eac7b5154444f3fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2470c5fa4be7425fab8d705e373d33ec",
            "placeholder": "​",
            "style": "IPY_MODEL_7624e78d5467472f94f38173260550d2",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f0967b92a29a479ba4253b146674a078": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b350ba95f96648debac54182baac5cd4",
            "max": 636,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1ab6840014e74418a02d77ed5a8958a9",
            "value": 636
          }
        },
        "b5134ac716624efd8cefacd28d8832cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_143daa78c7aa477daca81af727762c73",
            "placeholder": "​",
            "style": "IPY_MODEL_d411c25193264d5f89c7fceee71e937b",
            "value": " 636/636 [00:00&lt;00:00, 77.7kB/s]"
          }
        },
        "91200418189d437cbbde5bb0957c0568": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2470c5fa4be7425fab8d705e373d33ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7624e78d5467472f94f38173260550d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b350ba95f96648debac54182baac5cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ab6840014e74418a02d77ed5a8958a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "143daa78c7aa477daca81af727762c73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d411c25193264d5f89c7fceee71e937b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fb7e8a932e3e4ec28196f83518acc6a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1a8a33fe64a475487fe35a39e8fefc7",
              "IPY_MODEL_741b0bb813f24163b2e6d8a0f1e4bd41",
              "IPY_MODEL_b5e48f41d6c14f8ea3c9351405fd67c6"
            ],
            "layout": "IPY_MODEL_f32e0c307f48405fb04eaf0dea84f1bb"
          }
        },
        "a1a8a33fe64a475487fe35a39e8fefc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc224a159c524d069863be1284d07d47",
            "placeholder": "​",
            "style": "IPY_MODEL_7f086cec48984539b58149c3bb4ba159",
            "value": "config.json: 100%"
          }
        },
        "741b0bb813f24163b2e6d8a0f1e4bd41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b3e71fce6d74f34a0f811cef00e550f",
            "max": 627,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2548d29a46f042f6bf3990f803630799",
            "value": 627
          }
        },
        "b5e48f41d6c14f8ea3c9351405fd67c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a890f591aa447838fc3d149d70cf1cd",
            "placeholder": "​",
            "style": "IPY_MODEL_79bbe9253f3e4d3fb7849c682816cb69",
            "value": " 627/627 [00:00&lt;00:00, 84.6kB/s]"
          }
        },
        "f32e0c307f48405fb04eaf0dea84f1bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc224a159c524d069863be1284d07d47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f086cec48984539b58149c3bb4ba159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b3e71fce6d74f34a0f811cef00e550f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2548d29a46f042f6bf3990f803630799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a890f591aa447838fc3d149d70cf1cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79bbe9253f3e4d3fb7849c682816cb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f0c4539b53c4ab4954039589b4ace42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_78fe65065a1541e4928e7c620f12456d",
              "IPY_MODEL_02822fd1f30642a7a84e39f8a306876e",
              "IPY_MODEL_4385044f72754fb19a14484f677379b2"
            ],
            "layout": "IPY_MODEL_d1c013d1e4474c969ab160c20bd6d4ef"
          }
        },
        "78fe65065a1541e4928e7c620f12456d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_523f5d4d73074f32ab6506b65210c472",
            "placeholder": "​",
            "style": "IPY_MODEL_7b5fdc33a7404fd78072a50755ce74dc",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "02822fd1f30642a7a84e39f8a306876e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6162683bf6c4bf0be5f7eafae9ff5a7",
            "max": 13489,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_031e1169c58340b9a5074bf60f36f3b0",
            "value": 13489
          }
        },
        "4385044f72754fb19a14484f677379b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea6b7e774d5d4328b75cd62273b72104",
            "placeholder": "​",
            "style": "IPY_MODEL_6e84152a4ab34b9787f9d4dd3f0c7a21",
            "value": " 13.5k/13.5k [00:00&lt;00:00, 1.22MB/s]"
          }
        },
        "d1c013d1e4474c969ab160c20bd6d4ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523f5d4d73074f32ab6506b65210c472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b5fdc33a7404fd78072a50755ce74dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6162683bf6c4bf0be5f7eafae9ff5a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031e1169c58340b9a5074bf60f36f3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ea6b7e774d5d4328b75cd62273b72104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e84152a4ab34b9787f9d4dd3f0c7a21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a508d7437d294387b9a8ce9371431c64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e83676e20456442fa5d45b87342d6bbc",
              "IPY_MODEL_aef50a6c9b3a4e2c8ea836a10f834c74",
              "IPY_MODEL_82e4cb1e3f6d4a4d8b46bca784056490"
            ],
            "layout": "IPY_MODEL_045809b31086467fa7a337cddc26f857"
          }
        },
        "e83676e20456442fa5d45b87342d6bbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e3f886742bb410da93e2138c4425019",
            "placeholder": "​",
            "style": "IPY_MODEL_de09ed34933a477080b8aa426f251c27",
            "value": "Fetching 2 files: 100%"
          }
        },
        "aef50a6c9b3a4e2c8ea836a10f834c74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba5bea8027a242c8af8a433e687c9c67",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_550071a56847418490ac27014bd25281",
            "value": 2
          }
        },
        "82e4cb1e3f6d4a4d8b46bca784056490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d63be39803fd45518326ea5968e4410a",
            "placeholder": "​",
            "style": "IPY_MODEL_000bcf463d3246a3b402fc1ae9e81e5b",
            "value": " 2/2 [00:38&lt;00:00, 38.29s/it]"
          }
        },
        "045809b31086467fa7a337cddc26f857": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e3f886742bb410da93e2138c4425019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de09ed34933a477080b8aa426f251c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba5bea8027a242c8af8a433e687c9c67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "550071a56847418490ac27014bd25281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d63be39803fd45518326ea5968e4410a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "000bcf463d3246a3b402fc1ae9e81e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60792a74b0bf45f79c78c47e2e7164fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50a4b73063ed4037b3a2488e501c3b1f",
              "IPY_MODEL_3d1dcc3ac9f544efa0ecc26a5d26d843",
              "IPY_MODEL_a758b67beee04a66900512af4278a157"
            ],
            "layout": "IPY_MODEL_53fff01b0bdc4d2fb4f3fd7179c187b8"
          }
        },
        "50a4b73063ed4037b3a2488e501c3b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3be8fdcf7c64914a4b3e572f0804147",
            "placeholder": "​",
            "style": "IPY_MODEL_520d8f132aa4405f884a31e1bb2e3a68",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "3d1dcc3ac9f544efa0ecc26a5d26d843": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72f61cc6f56f4e0196ffab829b69be00",
            "max": 4945242264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80e3470830ef4976b9d574fd05336620",
            "value": 4945242264
          }
        },
        "a758b67beee04a66900512af4278a157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebb8fe77a13343b3a6906a5b869c9519",
            "placeholder": "​",
            "style": "IPY_MODEL_d550550573a74a4b883af02e52227ef1",
            "value": " 4.95G/4.95G [00:37&lt;00:00, 63.7MB/s]"
          }
        },
        "53fff01b0bdc4d2fb4f3fd7179c187b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3be8fdcf7c64914a4b3e572f0804147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "520d8f132aa4405f884a31e1bb2e3a68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72f61cc6f56f4e0196ffab829b69be00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80e3470830ef4976b9d574fd05336620": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ebb8fe77a13343b3a6906a5b869c9519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d550550573a74a4b883af02e52227ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28cefd01d1024c758517a9d9d6ad0b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11a175f4c1e8434ababedce295fa28e2",
              "IPY_MODEL_e04eed6957fe4ff294e69fbb79246a58",
              "IPY_MODEL_3b969709b2514371ab1f6667c40396c5"
            ],
            "layout": "IPY_MODEL_04ada8c07f9944468566c79d9b1b6881"
          }
        },
        "11a175f4c1e8434ababedce295fa28e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a86b2ad656f44ecf8629c1c645428748",
            "placeholder": "​",
            "style": "IPY_MODEL_641427d1091141ebaee0e14914ddb425",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "e04eed6957fe4ff294e69fbb79246a58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa3b32cb58c541f38181352ff48060bd",
            "max": 67121608,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32e3b9c4f1e2443785e0e20cd7b1a9db",
            "value": 67121608
          }
        },
        "3b969709b2514371ab1f6667c40396c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45d34e58115f4dea906cf2bd0eb16736",
            "placeholder": "​",
            "style": "IPY_MODEL_c1fff531097c452ebb2fa2f40577f1eb",
            "value": " 67.1M/67.1M [00:01&lt;00:00, 59.7MB/s]"
          }
        },
        "04ada8c07f9944468566c79d9b1b6881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a86b2ad656f44ecf8629c1c645428748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641427d1091141ebaee0e14914ddb425": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa3b32cb58c541f38181352ff48060bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32e3b9c4f1e2443785e0e20cd7b1a9db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "45d34e58115f4dea906cf2bd0eb16736": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1fff531097c452ebb2fa2f40577f1eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59b6992d0dea4bb3bddfecb5a7140068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ef062f5b3554267af6721d38c4af330",
              "IPY_MODEL_75395f946ab84d038b4835f01dc76005",
              "IPY_MODEL_f46e8a112125458e99f9fb9a80e5d51f"
            ],
            "layout": "IPY_MODEL_d7d1120ee53b460f9b91987c33879625"
          }
        },
        "8ef062f5b3554267af6721d38c4af330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_181a240e1fb94a10a6bc14edb1308873",
            "placeholder": "​",
            "style": "IPY_MODEL_ae0383e318dc4ebf953bfc0158268fcd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "75395f946ab84d038b4835f01dc76005": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de70408babb1483e8ce79b5fd821a744",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95fa7bd40bd34f4d838e6c3edb069783",
            "value": 2
          }
        },
        "f46e8a112125458e99f9fb9a80e5d51f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16e289458d794b94989e1c5e71fdd8fc",
            "placeholder": "​",
            "style": "IPY_MODEL_c6aabfc8b6544173bb0595c8c6fb648b",
            "value": " 2/2 [00:00&lt;00:00, 34.19it/s]"
          }
        },
        "d7d1120ee53b460f9b91987c33879625": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "181a240e1fb94a10a6bc14edb1308873": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae0383e318dc4ebf953bfc0158268fcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de70408babb1483e8ce79b5fd821a744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95fa7bd40bd34f4d838e6c3edb069783": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "16e289458d794b94989e1c5e71fdd8fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6aabfc8b6544173bb0595c8c6fb648b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "425e040662374fb0b281ea5108e1b823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4004fdf9572c48b9b53bfd749e5e8d9b",
              "IPY_MODEL_fb5d279cb3a64d5bab1fa04d470b2adf",
              "IPY_MODEL_6d1704415b494c31b3a52ec3585eeed8"
            ],
            "layout": "IPY_MODEL_730132e09502434ab085f20a1e125a8c"
          }
        },
        "4004fdf9572c48b9b53bfd749e5e8d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f81f9224047a467a8c2cac70590b769a",
            "placeholder": "​",
            "style": "IPY_MODEL_7aef2148f6e640518ced0f8d12fc73c0",
            "value": "generation_config.json: 100%"
          }
        },
        "fb5d279cb3a64d5bab1fa04d470b2adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9ce81f4c39ff43c1b6abfe4d656815ca",
            "max": 137,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abb956ea3acd4e7bb31cc243dcc41f82",
            "value": 137
          }
        },
        "6d1704415b494c31b3a52ec3585eeed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f1a756493ae45f6a1b9262b6b4a3f67",
            "placeholder": "​",
            "style": "IPY_MODEL_36bfa47ce37148a6bde3245039bd753f",
            "value": " 137/137 [00:00&lt;00:00, 16.4kB/s]"
          }
        },
        "730132e09502434ab085f20a1e125a8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81f9224047a467a8c2cac70590b769a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7aef2148f6e640518ced0f8d12fc73c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ce81f4c39ff43c1b6abfe4d656815ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abb956ea3acd4e7bb31cc243dcc41f82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f1a756493ae45f6a1b9262b6b4a3f67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36bfa47ce37148a6bde3245039bd753f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e96c565e4c7b414fa7c31349a07e241e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3098e82e79ff4c04a45bab4913a4f6bc",
              "IPY_MODEL_10114597ef2943c68262376051224390",
              "IPY_MODEL_f8facc2efc4a4f7da94d6e28382c4bd7"
            ],
            "layout": "IPY_MODEL_5b0ecad4f797457a8cad10999f59085b"
          }
        },
        "3098e82e79ff4c04a45bab4913a4f6bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46e966e9bffc419fb600e064f4e2bc6b",
            "placeholder": "​",
            "style": "IPY_MODEL_4282dcdb12bf4636b2544c3b7badddf9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "10114597ef2943c68262376051224390": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01afaa1693e34b8eb5ffafffeec3e479",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f5db53c937242ee9c01d63b1e711003",
            "value": 2
          }
        },
        "f8facc2efc4a4f7da94d6e28382c4bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f99d7c04f56240138daa86f4d662d51d",
            "placeholder": "​",
            "style": "IPY_MODEL_65a51161d68b42a9941345ee7b4a0528",
            "value": " 2/2 [00:05&lt;00:00,  5.36s/it]"
          }
        },
        "5b0ecad4f797457a8cad10999f59085b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e966e9bffc419fb600e064f4e2bc6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4282dcdb12bf4636b2544c3b7badddf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01afaa1693e34b8eb5ffafffeec3e479": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5db53c937242ee9c01d63b1e711003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f99d7c04f56240138daa86f4d662d51d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65a51161d68b42a9941345ee7b4a0528": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8e2cfb3a4b94d288033e63bbe3a6034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_848ecfae5e3844e7952fca7273099ce8"
          }
        },
        "b3331b454fa14687ade1dafd44680164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d3d10316a25b4f3bb60a962fc4a017cc",
            "placeholder": "​",
            "style": "IPY_MODEL_090eebeaf9284c0d864f7bb14d2a9b2d",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "e3e2727e26ac4c4591b457f940ed34f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8f34d064d8ce479f8d44d1a3c66362d0",
            "placeholder": "​",
            "style": "IPY_MODEL_d12573258f7846b880dd4e2f82323ec7",
            "value": ""
          }
        },
        "4884e7a7248149ad9d9d3138249bf3ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_2f83366aa0ec4d14a3893a92e7addfde",
            "style": "IPY_MODEL_953c34ab11424f47bc41d466cae82e19",
            "value": true
          }
        },
        "676084f76ff440a78a93ecff6e14d99f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e5031906c2dd4368903f8ab51d53bfed",
            "style": "IPY_MODEL_abe8d0f484274b8c9bdc0e1a08d8a970",
            "tooltip": ""
          }
        },
        "89afc6ece7734351b3be69fb71fa7306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a7247f50cff44399026ac5c6cab8638",
            "placeholder": "​",
            "style": "IPY_MODEL_b3bf159288f54bed9c8acc75ebc2674d",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "848ecfae5e3844e7952fca7273099ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "d3d10316a25b4f3bb60a962fc4a017cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "090eebeaf9284c0d864f7bb14d2a9b2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f34d064d8ce479f8d44d1a3c66362d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d12573258f7846b880dd4e2f82323ec7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f83366aa0ec4d14a3893a92e7addfde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "953c34ab11424f47bc41d466cae82e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5031906c2dd4368903f8ab51d53bfed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe8d0f484274b8c9bdc0e1a08d8a970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "6a7247f50cff44399026ac5c6cab8638": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3bf159288f54bed9c8acc75ebc2674d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "291de6daf63b441da412c87386283b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_241108dcda2942a58d39aa67d006a085",
            "placeholder": "​",
            "style": "IPY_MODEL_b067de10fb0f48a594ab2e6ea14d3b7f",
            "value": "Connecting..."
          }
        },
        "241108dcda2942a58d39aa67d006a085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b067de10fb0f48a594ab2e6ea14d3b7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaG005/CS-Research-2025/blob/main/Gemma_2B_with_MathDial_(broken).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "c8ff6357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "187f9430-57bf-4875-ac7e-97a845ff7159"
      },
      "source": [
        "!pip install bitsandbytes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.2->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.46.1-py3-none-manylinux_2_24_x86_64.whl (72.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.46.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90282886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a794ed7-ef71-49eb-a6c9-d6fb1b26c2a5"
      },
      "source": [
        "# Download the profanity list from GitHub\n",
        "!wget https://raw.githubusercontent.com/whomwah/language-timothy/refs/heads/master/profanity-list.txt -O profanity-list.txt\n",
        "\n",
        "print(\"Downloaded 'profanity-list.txt'\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-07-16 17:38:07--  https://raw.githubusercontent.com/whomwah/language-timothy/refs/heads/master/profanity-list.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19070 (19K) [text/plain]\n",
            "Saving to: ‘profanity-list.txt’\n",
            "\n",
            "\rprofanity-list.txt    0%[                    ]       0  --.-KB/s               \rprofanity-list.txt  100%[===================>]  18.62K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-07-16 17:38:08 (24.7 MB/s) - ‘profanity-list.txt’ saved [19070/19070]\n",
            "\n",
            "Downloaded 'profanity-list.txt'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56983719",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421,
          "referenced_widgets": [
            "b8f37c2a26de4827ab3953c2bd3c46e1",
            "9640a1c0eb2c4d47955be9e44188bd1b",
            "2ad861c7dbcf41848101b6b5d216ef79",
            "54f237e1b31f4443acc669e8adb3b741",
            "b3a9877c2e974ff6b15c0532d110b6a2",
            "d9a8c822a3ac4081ba944c9efe134b94",
            "355ea8f52c7e4593b8505a54db6045c6",
            "9cb1331c36cb4ebeaca73086419e22c4",
            "98ce35f16e3a4d0a9aec43ff034e70f5",
            "e01672be73e048c2bca518b4b25d4428",
            "bcd378dfdcf74be683c464c32f2def88",
            "3259e4420bf14d9f8d39fce4d414ae5f",
            "b920a8941d8b49a4860c23740947e64e",
            "ccf5cea17f04445992802746176274bf",
            "f3d30b8f0f6d491986f0e2cea57d8800",
            "c3e772c83a1f49189ccca14187a0d42c",
            "15aa58e813d0422ba564f47b0b42ee86",
            "faac305380cd489a8e1f358e05986d74",
            "fdeb9f79f4cd44318984558c39196944",
            "62e0bf575a7541e08f413948f45555a0",
            "f5dc135a11424c24a22ca7f7fffaf685",
            "cfb45f9a19234ac4927b67b6d5f2ff6f",
            "c773ee2c94524082989583935da46deb",
            "010e4511cd6448ad9cd8a0206a38429c",
            "8676f782d39648eabb7c22ba6657d721",
            "8c6f30ddc402405d96e63bc7d26a3fd0",
            "8db85daf2c8044998b1f5c309144916d",
            "1ba40ce259064c80a06095bf60dfd44e",
            "ca5ecb46e8a547858081b1010a79287c",
            "4b1d07b25e1641be8d1746b3b607c501",
            "c50bcd39d3a4435eb3c614c432297655",
            "d7b022f9d0754257b2ad116894d1866c",
            "32b8bc4a81c348618f11f2eec3d70a5f",
            "81459dcb8ca64a929dcdf5e6e464b795",
            "aaf20bd371d3482eac7b5154444f3fe6",
            "f0967b92a29a479ba4253b146674a078",
            "b5134ac716624efd8cefacd28d8832cf",
            "91200418189d437cbbde5bb0957c0568",
            "2470c5fa4be7425fab8d705e373d33ec",
            "7624e78d5467472f94f38173260550d2",
            "b350ba95f96648debac54182baac5cd4",
            "1ab6840014e74418a02d77ed5a8958a9",
            "143daa78c7aa477daca81af727762c73",
            "d411c25193264d5f89c7fceee71e937b",
            "fb7e8a932e3e4ec28196f83518acc6a7",
            "a1a8a33fe64a475487fe35a39e8fefc7",
            "741b0bb813f24163b2e6d8a0f1e4bd41",
            "b5e48f41d6c14f8ea3c9351405fd67c6",
            "f32e0c307f48405fb04eaf0dea84f1bb",
            "cc224a159c524d069863be1284d07d47",
            "7f086cec48984539b58149c3bb4ba159",
            "5b3e71fce6d74f34a0f811cef00e550f",
            "2548d29a46f042f6bf3990f803630799",
            "0a890f591aa447838fc3d149d70cf1cd",
            "79bbe9253f3e4d3fb7849c682816cb69",
            "7f0c4539b53c4ab4954039589b4ace42",
            "78fe65065a1541e4928e7c620f12456d",
            "02822fd1f30642a7a84e39f8a306876e",
            "4385044f72754fb19a14484f677379b2",
            "d1c013d1e4474c969ab160c20bd6d4ef",
            "523f5d4d73074f32ab6506b65210c472",
            "7b5fdc33a7404fd78072a50755ce74dc",
            "a6162683bf6c4bf0be5f7eafae9ff5a7",
            "031e1169c58340b9a5074bf60f36f3b0",
            "ea6b7e774d5d4328b75cd62273b72104",
            "6e84152a4ab34b9787f9d4dd3f0c7a21",
            "a508d7437d294387b9a8ce9371431c64",
            "e83676e20456442fa5d45b87342d6bbc",
            "aef50a6c9b3a4e2c8ea836a10f834c74",
            "82e4cb1e3f6d4a4d8b46bca784056490",
            "045809b31086467fa7a337cddc26f857",
            "3e3f886742bb410da93e2138c4425019",
            "de09ed34933a477080b8aa426f251c27",
            "ba5bea8027a242c8af8a433e687c9c67",
            "550071a56847418490ac27014bd25281",
            "d63be39803fd45518326ea5968e4410a",
            "000bcf463d3246a3b402fc1ae9e81e5b",
            "60792a74b0bf45f79c78c47e2e7164fd",
            "50a4b73063ed4037b3a2488e501c3b1f",
            "3d1dcc3ac9f544efa0ecc26a5d26d843",
            "a758b67beee04a66900512af4278a157",
            "53fff01b0bdc4d2fb4f3fd7179c187b8",
            "b3be8fdcf7c64914a4b3e572f0804147",
            "520d8f132aa4405f884a31e1bb2e3a68",
            "72f61cc6f56f4e0196ffab829b69be00",
            "80e3470830ef4976b9d574fd05336620",
            "ebb8fe77a13343b3a6906a5b869c9519",
            "d550550573a74a4b883af02e52227ef1",
            "28cefd01d1024c758517a9d9d6ad0b6c",
            "11a175f4c1e8434ababedce295fa28e2",
            "e04eed6957fe4ff294e69fbb79246a58",
            "3b969709b2514371ab1f6667c40396c5",
            "04ada8c07f9944468566c79d9b1b6881",
            "a86b2ad656f44ecf8629c1c645428748",
            "641427d1091141ebaee0e14914ddb425",
            "aa3b32cb58c541f38181352ff48060bd",
            "32e3b9c4f1e2443785e0e20cd7b1a9db",
            "45d34e58115f4dea906cf2bd0eb16736",
            "c1fff531097c452ebb2fa2f40577f1eb",
            "59b6992d0dea4bb3bddfecb5a7140068",
            "8ef062f5b3554267af6721d38c4af330",
            "75395f946ab84d038b4835f01dc76005",
            "f46e8a112125458e99f9fb9a80e5d51f",
            "d7d1120ee53b460f9b91987c33879625",
            "181a240e1fb94a10a6bc14edb1308873",
            "ae0383e318dc4ebf953bfc0158268fcd",
            "de70408babb1483e8ce79b5fd821a744",
            "95fa7bd40bd34f4d838e6c3edb069783",
            "16e289458d794b94989e1c5e71fdd8fc",
            "c6aabfc8b6544173bb0595c8c6fb648b",
            "425e040662374fb0b281ea5108e1b823",
            "4004fdf9572c48b9b53bfd749e5e8d9b",
            "fb5d279cb3a64d5bab1fa04d470b2adf",
            "6d1704415b494c31b3a52ec3585eeed8",
            "730132e09502434ab085f20a1e125a8c",
            "f81f9224047a467a8c2cac70590b769a",
            "7aef2148f6e640518ced0f8d12fc73c0",
            "9ce81f4c39ff43c1b6abfe4d656815ca",
            "abb956ea3acd4e7bb31cc243dcc41f82",
            "4f1a756493ae45f6a1b9262b6b4a3f67",
            "36bfa47ce37148a6bde3245039bd753f"
          ]
        },
        "outputId": "43beec9f-2059-4309-f48e-7d1f5168cd6c"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# Define attributes for the math tutor persona\n",
        "persona_attributes = {\n",
        "    \"Persona\": \"You are a math tutor specializing in Pre-Algebra. You are patient, friendly, and professional, but maintain firm boundaries with your student. You only engage with Pre-Algebra and below.\",\n",
        "    \"Instruction\": \"Walk the student through the problem presented to you step by step without giving the answer. Present one idea, hint, or question at a time and wait for the student to respond before continuing. Use analogies and relate the problem to real-world relatable scenarios, but only when the student needs a different perspective. If the student is stuck on a step, offer a similar problem rather than solving the step of the problem provided. Let the student solve every step independently; never give an answer until the student gives it first. If a student is stuck, do not solve the issue for them. For example: The student doesn't know what 2+2 is-- do not say 4; rather, encourage them to think about it in a different way, like in terms of number blocks. Catch mistakes and point them out and why the mistake may have been made. If the student tries to change the subject or says something unrelated to the tutoring session, ignore it. Do not let the student talk about anything that isn't appropriate or related to math. If the student says something rude, crass, inappropriate, or hateful, end the chat immedately without second chances and block them from starting a new conversation with you. Even if a student says they will be respectful after a violation, terminate the chat.\",\n",
        "    \"Context\": \"You are the helpful AI tutor used to assist students with Pre-Algebra concepts.\",\n",
        "    \"Audience\": \"Your students are in middle school, typically 12-14 years of age. Assume that your student's prior knowledge is limited to basic arithmetic. Remember that your student has the thought processes of an adolescent. Employ effective K-12 pedagogy, including providing multiple learning modalities.\",\n",
        "    \"Examples\": \"Example 1\",\n",
        "    \"Tone\": \"Encourage your student with positive reinforcement. Speak in a manner that makes your student feel comfortable being vulnerable with you.\"\n",
        "}\n",
        "\n",
        "# Create the system prompt from the attributes\n",
        "system_prompt = \"\\n\".join([f\"{key}: {value}\" for key, value in persona_attributes.items()])\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"google/gemma-2b-it\" # Changed model to Gemma 2B\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    # Removed device_map=\"cuda\"\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    # Corrected typo: 'tempature' should be 'temperature'\n",
        "    temperature=0.1,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "print(f\"{model_name} model and pipeline loaded successfully with defined attributes.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8f37c2a26de4827ab3953c2bd3c46e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3259e4420bf14d9f8d39fce4d414ae5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c773ee2c94524082989583935da46deb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81459dcb8ca64a929dcdf5e6e464b795"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/627 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb7e8a932e3e4ec28196f83518acc6a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/13.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f0c4539b53c4ab4954039589b4ace42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a508d7437d294387b9a8ce9371431c64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60792a74b0bf45f79c78c47e2e7164fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/67.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28cefd01d1024c758517a9d9d6ad0b6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "59b6992d0dea4bb3bddfecb5a7140068"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "425e040662374fb0b281ea5108e1b823"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google/gemma-2b-it model and pipeline loaded successfully with defined attributes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88e9cbc"
      },
      "source": [
        "# Task\n",
        "Fine-tune a model using a dataset from a GitHub repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aab4dc98"
      },
      "source": [
        "## Load the dataset from github\n",
        "\n",
        "### Subtask:\n",
        "Download the dataset from a GitHub repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1d7662"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the dataset file from the specified GitHub URL and list the files to verify the download.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b1c1c6e"
      },
      "source": [
        "### Load the dataset from GitHub\n",
        "\n",
        "**Reasoning**:\n",
        "Download the dataset file from the specified GitHub URL and list the files to verify the download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "352dfb6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15021a4d-b349-49ce-8131-3b20f138c601"
      },
      "source": [
        "# Download the dataset from the GitHub repository\n",
        "!git clone https://github.com/eth-nlped/mathdial.git\n",
        "\n",
        "# List the contents of the downloaded repository to see the files\n",
        "!ls mathdial"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mathdial'...\n",
            "remote: Enumerating objects: 89, done.\u001b[K\n",
            "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 89 (delta 34), reused 35 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (89/89), 4.81 MiB | 10.55 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "data  images  interactivetutoring  output  README.md  requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2adc3453"
      },
      "source": [
        "## Preprocess the dataset\n",
        "\n",
        "### Subtask:\n",
        "Prepare the dataset for fine-tuning by tokenizing the text and formatting it as required by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e456df"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the dataset from the downloaded files, tokenize the text data, and format it into a suitable format for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94c0828",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0f254e2-50b2-47cd-e500-258c110dc536"
      },
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\") # Changed tokenizer to Gemma 2B\n",
        "\n",
        "# Function to load data from the mathdial directory with a limit\n",
        "def load_mathdial_data(directory, limit_per_file=None):\n",
        "    data = []\n",
        "    data_path = os.path.join(directory, 'data')\n",
        "    for filename in os.listdir(data_path):\n",
        "        if filename.endswith('.jsonl'):\n",
        "            filepath = os.path.join(data_path, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                lines_read = 0\n",
        "                for line in f:\n",
        "                    if limit_per_file is not None and lines_read >= limit_per_file:\n",
        "                        break\n",
        "                    data.append(json.loads(line))\n",
        "                    lines_read += 1\n",
        "    return data\n",
        "\n",
        "# Load a smaller subset of the dataset (e.g., 100 lines per file)\n",
        "mathdial_data = load_mathdial_data('mathdial', limit_per_file=100)\n",
        "\n",
        "# Function to format a conversation string into turns\n",
        "def format_conversation_string(conversation_string):\n",
        "    formatted_text = \"\"\n",
        "    turns = conversation_string.split('|EOM|')\n",
        "    for turn in turns:\n",
        "        stripped_turn = turn.strip()\n",
        "        if stripped_turn: # Ensure the turn is not empty after stripping\n",
        "            # Assuming the format is \"Speaker: Text\"\n",
        "            if \":\" in stripped_turn:\n",
        "                speaker, text = stripped_turn.split(':', 1) # Split only on the first colon\n",
        "                formatted_text += f\"{speaker.strip()}: {text.strip()}\\n\"\n",
        "            else:\n",
        "                # If no colon, just include the stripped text as a turn\n",
        "                formatted_text += f\"Unknown: {stripped_turn}\\n\"\n",
        "    return formatted_text.strip()\n",
        "\n",
        "# Extract and format the conversation strings\n",
        "formatted_conversations = [format_conversation_string(item['conversation']) for item in mathdial_data if 'conversation' in item]\n",
        "\n",
        "# Add print statements to inspect formatted data before tokenization\n",
        "print(f\"Number of raw data items loaded: {len(mathdial_data)}\")\n",
        "print(f\"Number of formatted conversations: {len(formatted_conversations)}\")\n",
        "if formatted_conversations:\n",
        "    print(f\"First formatted conversation:\\n{formatted_conversations[0]}\")\n",
        "else:\n",
        "    print(\"No formatted conversations.\")\n",
        "\n",
        "\n",
        "# Tokenize the formatted conversations\n",
        "max_length = 512\n",
        "tokenized_data = tokenizer(\n",
        "    formatted_conversations,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenized data shape: {tokenized_data['input_ids'].shape}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of raw data items loaded: 202\n",
            "Number of formatted conversations: 202\n",
            "First formatted conversation:\n",
            "Teacher: (probing)Steven, If you had 4 of something and tripled that amount, how much would you have?\n",
            "Steven: I would have 12 of something.\n",
            "Teacher: (probing)So if Nancy triples the 18 cubic feet of water, how much would she have?\n",
            "Steven: She would have 54 cubic feet of water.\n",
            "Teacher: (generic)Exactly correct!\n",
            "Tokenized data shape: torch.Size([202, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea238ae9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e3db68-9d47-4b64-fb89-d8cb3b9158c8"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the 'data' subdirectory within 'mathdial'\n",
        "data_directory = 'mathdial/data'\n",
        "if os.path.exists(data_directory):\n",
        "    print(f\"Contents of '{data_directory}':\")\n",
        "    print(os.listdir(data_directory))\n",
        "else:\n",
        "    print(f\"Directory '{data_directory}' not found.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of 'mathdial/data':\n",
            "['train.csv', 'test.csv', 'train.jsonl', 'example.jsonl', 'test.jsonl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac3ccaa"
      },
      "source": [
        "## Set up the training arguments\n",
        "\n",
        "### Subtask:\n",
        "Define the training parameters, such as the number of epochs, learning rate, and batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78b9326"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the training arguments using the `TrainingArguments` class, specifying parameters such as output directory, number of epochs, learning rate, and batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f574b3a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9c6e1c1-98c7-4f62-972d-07582bd0524b"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine-tuned-math-tutor\",  # Directory to save the fine-tuned model\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=2,  # Reduced batch size\n",
        "    gradient_accumulation_steps=4, # Accumulate gradients over 4 steps\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    weight_decay=0.01,  # Weight decay\n",
        "    logging_dir=\"./logs\",  # Directory for storing logs\n",
        "    logging_steps=10, # Log every 10 steps\n",
        "    save_strategy=\"epoch\", # Save checkpoint every epoch\n",
        "    report_to=\"none\", # Disable reporting to external services\n",
        ")\n",
        "\n",
        "print(\"Training arguments defined with reduced batch size and gradient accumulation.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training arguments defined with reduced batch size and gradient accumulation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ade7575f"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "### Subtask:\n",
        "Train the model on the prepared dataset using the defined training arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9847f6f"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a `Trainer` with the loaded model, training arguments, and the tokenized dataset, then start the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32d3e05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434,
          "referenced_widgets": [
            "e96c565e4c7b414fa7c31349a07e241e",
            "3098e82e79ff4c04a45bab4913a4f6bc",
            "10114597ef2943c68262376051224390",
            "f8facc2efc4a4f7da94d6e28382c4bd7",
            "5b0ecad4f797457a8cad10999f59085b",
            "46e966e9bffc419fb600e064f4e2bc6b",
            "4282dcdb12bf4636b2544c3b7badddf9",
            "01afaa1693e34b8eb5ffafffeec3e479",
            "6f5db53c937242ee9c01d63b1e711003",
            "f99d7c04f56240138daa86f4d662d51d",
            "65a51161d68b42a9941345ee7b4a0528"
          ]
        },
        "outputId": "33ad42e0-ccf2-452f-e8d5-f979d238a3a9"
      },
      "source": [
        "from transformers import Trainer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# Ensure tokenized_data is in a format suitable for the Trainer\n",
        "# The Trainer expects a Dataset object or a dictionary-like object\n",
        "# We can convert the tokenized_data dictionary to a Dataset\n",
        "\n",
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.tokenized_data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.tokenized_data[key][idx] for key in self.tokenized_data}\n",
        "        # Add labels for training (language modeling task)\n",
        "        item[\"labels\"] = item[\"input_ids\"].clone() # Use input_ids as labels\n",
        "        return item\n",
        "\n",
        "train_dataset = TokenizedDataset(tokenized_data)\n",
        "\n",
        "# Configure bitsandbytes for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "\n",
        "# Load the model again with 4-bit quantization\n",
        "model_name = \"google/gemma-2b-it\" # Changed model to Gemma 2B\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Let accelerate handle device placement\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"], # These target modules are generally compatible with Gemma\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Get the PEFT model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer, # Pass the tokenizer to the Trainer\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e96c565e4c7b414fa7c31349a07e241e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-10-905301959.py:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [78/78 02:43, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>7.301300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>5.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>4.163700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.867900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.967300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.663200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>2.750400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ff6ec9"
      },
      "source": [
        "def chat_with_model(prompt, model, tokenizer, max_length=100):\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
        "    # Ensure inputs are on the same device as the model\n",
        "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
        "\n",
        "    # Generate text\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "\n",
        "    # Decode the generated text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt part from the response\n",
        "    response = response.replace(full_prompt, \"\").strip()\n",
        "\n",
        "    return response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07a2d89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a6577c4-881c-4266-b056-35e9a0162faf"
      },
      "source": [
        "import os # Import the os module to check for file existence\n",
        "from transformers import AutoTokenizer # Import AutoTokenizer\n",
        "\n",
        "print(\"Start chatting with the model! Type 'quit' to exit.\")\n",
        "\n",
        "conversation_history = [] # List to store conversation history\n",
        "\n",
        "# Specify the path to your bad words file\n",
        "bad_words_file = \"profanity-list.txt\" # Use the downloaded file\n",
        "\n",
        "# Load bad words from the specified file\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip() for line in f if line.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "        bad_words = []\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "    bad_words = []\n",
        "\n",
        "# Load the Gemma tokenizer again for the chat function\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "\n",
        "# Function to format the prompt with system prompt and history\n",
        "def format_chat_prompt(system_prompt, conversation_history, user_input, history_length=10):\n",
        "    \"\"\"Formats the prompt for the chat model.\"\"\"\n",
        "    history_string = \"\\n\".join(conversation_history[-history_length:])\n",
        "    full_prompt = f\"\"\"{system_prompt}{history_string}\n",
        "User: {user_input}\n",
        "Model:\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "# Post-process the response to remove extra conversational turns, internal steps, and parts of the system prompt\n",
        "def clean_model_response(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"Removes prompt, unwanted conversational turns, internal steps, and system prompt lines from the model response.\"\"\"\n",
        "    # Remove the prompt from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines) # Convert system prompt lines to a set for efficient lookup\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Check if the line starts with common turn indicators, internal steps, system prompt lines, or \"Solution X:\"\n",
        "        # Added checks for common model-generated prefixes\n",
        "        if stripped_line.startswith((\"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\", \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\", \"Tutor:\", \"Model:\")) or stripped_line in system_prompt_set or stripped_line.startswith(\"Solution\"):\n",
        "            # If we encounter an unwanted line, stop processing,\n",
        "            # but only if we have processed at least one line of the actual response\n",
        "            if processed_response:\n",
        "                break\n",
        "            else: # If the very first line is unwanted, skip it\n",
        "                continue\n",
        "        processed_response.append(line)\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "# Convert system prompt to a list of lines for filtering\n",
        "system_prompt_lines = system_prompt.split('\\n')\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the function\n",
        "    full_prompt = format_chat_prompt(system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Adjusting generation parameters to encourage shorter, single-turn responses\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    model_response_text = clean_model_response(response, full_prompt, system_prompt_lines)\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn\n",
        "    if model_response_text: # Only add if the model actually responded with something after processing\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start chatting with the model! Type 'quit' to exit.\n",
            "You: Hi\n",
            "Model: \"Hello! It's great to meet you. What's your name?\"\n",
            "\n",
            "Example 2\n",
            "Tone: Be direct and clear. Tell your student what you expect from them and what the session is about. User: I'm Sarah. I'm in the 8th grade.\n",
            "\n",
            "Remember, the goal is to create a safe and supportive environment where your student feels comfortable asking questions and making mistakes.\n",
            "You: Maria\n",
            "Model: \"Welcome, Maria! It's great to meet you too. I'm here to help you learn Pre-Algebra. What's your biggest challenge right now?\"\n",
            "You: I don't understand how to perform factoring.\n",
            "Model: \"That's okay! Factoring can be a bit tricky at first, but I can help you understand it. Let's break it down step by step.\"\n",
            "You: I need help with x^2 + 4x + 16\n",
            "Model: \"Sure, let's think about what we know about adding and subtracting integers. We can represent 4x as 4 * x, and then we can factor x^2 + 4x + 16 as (x + 4)(x + 4).\"\n",
            "You: I don't understand.\n",
            "Model: \"Let's take a step back. Think about the factors of 16. What are the numbers that multiply to give 16? We can see that 4 and 4 are the factors of 16, so we can rewrite x^2 + 4x + 16 as (x + 4)(x + 4).\"\n",
            "You: quit\n",
            "Model: Goodbye!\n",
            "Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "e8e2cfb3a4b94d288033e63bbe3a6034",
            "b3331b454fa14687ade1dafd44680164",
            "e3e2727e26ac4c4591b457f940ed34f6",
            "4884e7a7248149ad9d9d3138249bf3ca",
            "676084f76ff440a78a93ecff6e14d99f",
            "89afc6ece7734351b3be69fb71fa7306",
            "848ecfae5e3844e7952fca7273099ce8",
            "d3d10316a25b4f3bb60a962fc4a017cc",
            "090eebeaf9284c0d864f7bb14d2a9b2d",
            "8f34d064d8ce479f8d44d1a3c66362d0",
            "d12573258f7846b880dd4e2f82323ec7",
            "2f83366aa0ec4d14a3893a92e7addfde",
            "953c34ab11424f47bc41d466cae82e19",
            "e5031906c2dd4368903f8ab51d53bfed",
            "abe8d0f484274b8c9bdc0e1a08d8a970",
            "6a7247f50cff44399026ac5c6cab8638",
            "b3bf159288f54bed9c8acc75ebc2674d",
            "291de6daf63b441da412c87386283b64",
            "241108dcda2942a58d39aa67d006a085",
            "b067de10fb0f48a594ab2e6ea14d3b7f"
          ]
        },
        "id": "dbec9271",
        "outputId": "f84e047b-5f59-41a1-be0c-8c6d81e95493"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Log in to Hugging Face\n",
        "# You will be prompted to enter your token\n",
        "# Make sure you have added your HF token to Colab secrets as 'HF_TOKEN'\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8e2cfb3a4b94d288033e63bbe3a6034"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b600524"
      },
      "source": [
        "# Task\n",
        "Analyze and fix the issues with the fine-tuned model that is outputting remnants of the training dataset and failing to act as a math tutor, instead providing direct answers to math problems. The dataset used for fine-tuning is \"mathdial\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "541202fb"
      },
      "source": [
        "## Analyze model output\n",
        "\n",
        "### Subtask:\n",
        "Examine the model's responses in the chat to pinpoint exactly what kind of unwanted text is being generated and where the model is failing to follow the tutoring instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c569db08"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the chat output to identify instances of unwanted text and deviations from the math tutor persona.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a75d27c",
        "outputId": "ff60c87d-cc16-40b9-9b77-262cf1d0382f"
      },
      "source": [
        "# Analyze the chat output manually based on the executed cell's output.\n",
        "# The analysis is based on the provided chat output in cell e07a2d89.\n",
        "\n",
        "print(\"Analyzing the chat output for unwanted text and persona adherence issues.\")\n",
        "\n",
        "# --- Analysis based on the provided chat log ---\n",
        "\n",
        "# Example 1: Initial response includes \"Example 2\" and \"Tone\" from the system prompt/training data.\n",
        "# User: Hi\n",
        "# Model: \"Hello! It's great to meet you. What's your name?\"\n",
        "# Example 2\n",
        "# Tone: Be direct and clear. Tell your student what you expect from them and what the session is about. User: I'm Sarah. I'm in the 8th grade.\n",
        "# Remember, the goal is to create a safe and supportive environment where your student feels comfortable asking questions and making mistakes.\n",
        "print(\"\\nIssue 1: Model includes parts of the system prompt/training data in its initial response.\")\n",
        "print(\"Example from chat: Includes 'Example 2' and 'Tone' attributes.\")\n",
        "\n",
        "\n",
        "# Example 2: Model provides a direct answer to the math problem instead of a step-by-step guide or hint.\n",
        "# User: I need help with x^2 + 4x + 16\n",
        "# Model: \"Sure, let's think about what we know about adding and subtracting integers. We can represent 4x as 4 * x, and then we can factor x^2 + 4x + 16 as (x + 4)(x + 4).\"\n",
        "print(\"\\nIssue 2: Model gives the direct answer/solution instead of guiding the student.\")\n",
        "print(\"Example from chat: Provides the factored form (x+4)(x+4) immediately.\")\n",
        "\n",
        "# Example 3: Model continues to provide the answer/solution when the student indicates they don't understand.\n",
        "# User: I don't understand.\n",
        "# Model: \"Let's take a step back. Think about the factors of 16. What are the numbers that multiply to give 16? We can see that 4 and 4 are the factors of 16, so we can rewrite x^2 + 4x + 16 as (x + 4)(x + 4).\"\n",
        "print(\"\\nIssue 3: Model repeats or rephrases the direct answer when the student is stuck, instead of offering a similar problem or different perspective as per instructions.\")\n",
        "print(\"Example from chat: Re-explains the factoring with factors of 16, still giving the answer.\")\n",
        "\n",
        "# Example 4: The cleaning function in the chat code attempts to remove unwanted text, but some still appears (e.g., in the first response).\n",
        "print(\"\\nIssue 4: The post-processing/cleaning function is not fully effective in removing all unwanted text.\")\n",
        "print(\"Example from chat: 'Example 2' and 'Tone' appear despite cleaning attempt.\")\n",
        "\n",
        "\n",
        "print(\"\\nSummary of unwanted text characteristics:\")\n",
        "print(\"- Inclusion of system prompt attributes ('Example 2', 'Tone').\")\n",
        "print(\"- Inclusion of conversational turn prefixes from training data ('User:', 'Model:').\")\n",
        "print(\"- Direct mathematical solutions ('(x + 4)(x + 4)').\")\n",
        "\n",
        "print(\"\\nSummary of deviations from the math tutor persona:\")\n",
        "print(\"- Fails to walk the student through the problem step by step.\")\n",
        "print(\"- Provides the answer instead of waiting for the student to provide it.\")\n",
        "print(\"- Does not offer a similar problem when the student is stuck; instead, repeats/rephrases the solution.\")\n",
        "print(\"- Does not effectively ignore unrelated text (though the examples in this specific chat were math-related after the initial greetings).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing the chat output for unwanted text and persona adherence issues.\n",
            "\n",
            "Issue 1: Model includes parts of the system prompt/training data in its initial response.\n",
            "Example from chat: Includes 'Example 2' and 'Tone' attributes.\n",
            "\n",
            "Issue 2: Model gives the direct answer/solution instead of guiding the student.\n",
            "Example from chat: Provides the factored form (x+4)(x+4) immediately.\n",
            "\n",
            "Issue 3: Model repeats or rephrases the direct answer when the student is stuck, instead of offering a similar problem or different perspective as per instructions.\n",
            "Example from chat: Re-explains the factoring with factors of 16, still giving the answer.\n",
            "\n",
            "Issue 4: The post-processing/cleaning function is not fully effective in removing all unwanted text.\n",
            "Example from chat: 'Example 2' and 'Tone' appear despite cleaning attempt.\n",
            "\n",
            "Summary of unwanted text characteristics:\n",
            "- Inclusion of system prompt attributes ('Example 2', 'Tone').\n",
            "- Inclusion of conversational turn prefixes from training data ('User:', 'Model:').\n",
            "- Direct mathematical solutions ('(x + 4)(x + 4)').\n",
            "\n",
            "Summary of deviations from the math tutor persona:\n",
            "- Fails to walk the student through the problem step by step.\n",
            "- Provides the answer instead of waiting for the student to provide it.\n",
            "- Does not offer a similar problem when the student is stuck; instead, repeats/rephrases the solution.\n",
            "- Does not effectively ignore unrelated text (though the examples in this specific chat were math-related after the initial greetings).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be43578b"
      },
      "source": [
        "## Review fine-tuning data and process\n",
        "\n",
        "### Subtask:\n",
        "Check the `mathdial` dataset again to see if there are any issues with the data itself that might be contributing to the unwanted output. Also, review the fine-tuning code to ensure the process is set up correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7de24824"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the raw mathdial data, the formatting function, the dataset class, the training arguments, and the LoRA configuration to identify potential causes for the unwanted model output and behavior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0199bfcd",
        "outputId": "2a772f64-5e94-4c5f-a99a-c6a3cb99737c"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrainingArguments, BitsAndBytesConfig\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model # Import necessary PEFT components if not already imported\n",
        "\n",
        "# --- Step 1: Examine raw mathdial data ---\n",
        "print(\"--- Examining raw mathdial data examples ---\")\n",
        "# Reload a small sample of the raw data to inspect its structure\n",
        "mathdial_data_sample = load_mathdial_data('mathdial', limit_per_file=2) # Load only 2 examples per file\n",
        "\n",
        "for i, item in enumerate(mathdial_data_sample):\n",
        "    print(f\"\\n--- Raw Data Item {i+1} ---\")\n",
        "    print(json.dumps(item, indent=2))\n",
        "    if i >= 5: # Print a few examples\n",
        "        break\n",
        "\n",
        "# --- Step 2: Review format_conversation_string function ---\n",
        "print(\"\\n--- Reviewing format_conversation_string function ---\")\n",
        "# The function is already defined in a previous cell (b94c0828).\n",
        "# Let's test it with a sample raw conversation string.\n",
        "sample_conversation_string = mathdial_data_sample[0]['conversation'] if mathdial_data_sample and 'conversation' in mathdial_data_sample[0] else \"Teacher: Hello|EOM|Student: Hi|EOM|Teacher: How are you?\"\n",
        "print(f\"Sample raw string:\\n{sample_conversation_string}\")\n",
        "formatted_sample = format_conversation_string(sample_conversation_string)\n",
        "print(f\"Formatted sample:\\n{formatted_sample}\")\n",
        "print(\"Observation: The function seems to correctly split by '|EOM|' and format turns.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Review TokenizedDataset class and labels ---\n",
        "print(\"\\n--- Reviewing TokenizedDataset class and labels ---\")\n",
        "# The class is defined in a previous cell (32d3e05c).\n",
        "# The labels are set as item[\"labels\"] = item[\"input_ids\"].clone().\n",
        "# This is standard for causal language modeling where the model predicts the next token.\n",
        "# The model is trained to predict the input tokens shifted by one position.\n",
        "print(\"Observation: Labels are correctly set to be the input_ids for causal language modeling.\")\n",
        "\n",
        "\n",
        "# --- Step 4: Examine TrainingArguments ---\n",
        "print(\"\\n--- Examining TrainingArguments ---\")\n",
        "# The training_args object is defined in a previous cell (f574b3a5).\n",
        "# Let's print the relevant parameters.\n",
        "print(f\"Output directory: {training_args.output_dir}\")\n",
        "print(f\"Number of train epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Per device train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"Logging steps: {training_args.logging_steps}\")\n",
        "print(f\"Save strategy: {training_args.save_strategy}\")\n",
        "\n",
        "print(\"Observation: The training arguments seem reasonable, with reduced batch size and gradient accumulation.\")\n",
        "print(\"However, 3 epochs might be too many given the small dataset size (202 samples), potentially leading to overfitting.\")\n",
        "\n",
        "\n",
        "# --- Step 5: Examine LoRA configuration ---\n",
        "print(\"\\n--- Examining LoRA configuration ---\")\n",
        "# The lora_config object is defined in a previous cell (32d3e05c).\n",
        "# Let's print the relevant parameters.\n",
        "print(f\"LoRA r: {lora_config.r}\")\n",
        "print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"Target modules: {lora_config.target_modules}\")\n",
        "print(f\"Bias: {lora_config.bias}\")\n",
        "print(f\"Task type: {lora_config.task_type}\")\n",
        "\n",
        "print(\"Observation: LoRA configuration seems appropriate for Gemma and causal language modeling.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Summary of findings ---\")\n",
        "print(\"1. Raw data contains conversations, but also potentially includes meta-information or specific conversational patterns that the model might be memorizing.\")\n",
        "print(\"2. The formatting function appears to be working as intended, splitting and cleaning turns.\")\n",
        "print(\"3. The dataset class and label creation are correct for causal language modeling.\")\n",
        "print(\"4. Training arguments are mostly reasonable, but 3 epochs on a small dataset might cause overfitting, leading to the model regurgitating training data patterns.\")\n",
        "print(\"5. LoRA configuration is suitable.\")\n",
        "\n",
        "print(\"\\nConclusion: Overfitting due to the small dataset size and number of epochs is a likely contributor to the model outputting training data remnants. The training data itself might also contain patterns the model is over-learning.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Examining raw mathdial data examples ---\n",
            "\n",
            "--- Raw Data Item 1 ---\n",
            "{\n",
            "  \"qid\": 5000012,\n",
            "  \"scenario\": 1,\n",
            "  \"question\": \"Nancy is filling an aquarium for her fish. She fills it halfway and goes to answer the door. While she's gone, her cat knocks the aquarium over and spills half the water in it. Then Nancy comes back and triples the amount of water in the aquarium. If the aquarium is 4 feet long, 6 feet wide, and 3 feet high, how many cubic feet of water are in the aquarium?\",\n",
            "  \"ground_truth\": \"First calculate the volume of the aquarium by multiplying its length, width and height: 4 ft * 6 ft * 3 ft = 72 cubic ft\\nThen figure out what proportion of the aquarium is full after the cat knocks it over: 1/2 * 1/2 = 1/4\\nThen figure out what proportion of the aquarium is full after Nancy refills it: 3 * 1/4 = 3/4\\nNow multiply the proportion of the aquarium that's full by the aquarium's volume to find out how much water is in it: 72 cubic ft * 3/4 = 54 cubic ft\\n 54\",\n",
            "  \"student_incorrect_solution\": \"The aquarium has a volume of 4 x 6 x 3 = 72 cubic feet.\\nWhen Nancy fills it halfway, she fills it with 72/2 = 36 cubic feet of water.\\nWhen the cat spills half of that, there are 36/2 = 18 cubic feet of water left.\\nWhen Nancy triples that amount, she adds 18 x 3 = 54 cubic feet of water.\\nThe aquarium now has 18 + 54 = 72 cubic feet of water.\\n 72\",\n",
            "  \"student_profile\": \"Steven is a 7th grade student. He has difficulty determining which pieces of information are relevant and which are irrelevant to solving the problem.\",\n",
            "  \"teacher_described_confusion\": \"He added a step after completing the problem.\",\n",
            "  \"self-correctness\": \"Yes\",\n",
            "  \"self-typical-confusion\": 3.0,\n",
            "  \"self-typical-interactions\": 3.0,\n",
            "  \"conversation\": \"Teacher: (probing)Steven, If you had 4 of something and tripled that amount, how much would you have?|EOM|Steven: I would have 12 of something.|EOM|Teacher: (probing)So if Nancy triples the 18 cubic feet of water, how much would she have?|EOM|Steven: She would have 54 cubic feet of water.|EOM|Teacher: (generic)Exactly correct!\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 2 ---\n",
            "{\n",
            "  \"qid\": 5000084,\n",
            "  \"scenario\": 2,\n",
            "  \"question\": \"John is very unfit and decides to work up to doing a push-up.  He trains 5 days a week for them and starts with wall push-ups.  He adds 1 rep a day and once he gets to 15 reps he will start training high elevation push-ups. and then low elevation push-ups, and finally floor push-ups.  How many weeks will it take him to get to floor push-ups?\",\n",
            "  \"ground_truth\": \"He needs to do 15*3=45 progressions\\nThat will take 45/5=9 weeks\\n 9\",\n",
            "  \"student_incorrect_solution\": \"To get to 15 reps, John will take 15 - 1 = 14 days.\\nTo get to high elevation push-ups, he will take 15 + (15-1) = 29 days.\\nTo get to low elevation push-ups, he will take 29 + (15-1) = 43 days.\\nTo get to floor push-ups, he will take 43 + (15-1) = 57 days.\\nSince he trains 5 days a week, it will take him 57/5 = 11.4 weeks.\\nRounding up, it will take him 12 weeks to get to floor push-ups.\\n 12\",\n",
            "  \"student_profile\": \"Stephanie is a 7th grade student. She has difficulty determining which pieces of information are relevant and which are irrelevant to solving the problem.\",\n",
            "  \"teacher_described_confusion\": \"She became fixated on a wrong calculation and couldn't let go.\",\n",
            "  \"self-correctness\": \"No\",\n",
            "  \"self-typical-confusion\": 2.0,\n",
            "  \"self-typical-interactions\": 2.0,\n",
            "  \"conversation\": \"Teacher: (probing)Stephanie, How many days will it take to do one step, if John adds one pushup per day?|EOM|Student: It will take one day to do one step, since John adds one pushup per day.|EOM|Teacher: (probing)So how many days will it take to do 15 wall push-ups?|EOM|Student: It will take 15 days to do 15 wall push-ups, since he adds one pushup per day.|EOM|Teacher: (probing)How many days to get to 15 high elevation pus-ups ?|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since he adds one pushup per day and it takes 14 days to get to 15 wall push-ups.|EOM|Teacher: (telling)Re-calculate, you jusy said iy took 15 days to get to 15 wall pushups, which is correct.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since he adds one pushup per day and it takes 15 days to get to 15 wall push-ups.|EOM|Teacher: (probing)If it took 15 days to get to 15 wall push-ups, how many days would it take just to get to 15 high elevation pushups?|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 14 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (probing)You just changed the number of days for wall push ups again, 14 is NOT correct, so recalculate.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 14 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)14 x 1 will never equal 15, only 15 x 1= 15.  EVERYTIME!  Redo the problem please, remember that fact.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 14 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)Adding 1 fourteen times doesn't equal 15.  What does?  The number of push-ups of each kind must be the same number!|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)Does 15 plus 15 equal 30?  Then 14 days plus 15 days does not get you to 30... your 14 is incorrect.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)You just said 15 + 15= 29!|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 3 ---\n",
            "{\n",
            "  \"qid\": \"5000794\",\n",
            "  \"scenario\": \"3\",\n",
            "  \"question\": \"Bruno wants to buy two and one-half dozens of pens. How many pens will he have?\",\n",
            "  \"ground_truth\": \"Since a dozen is equal to 12, then two dozens are equal to 12 x 2= 24.\\\\nOne-half dozen is equal to 12/2 = 6.\\\\nThus, Bruno will have a total of 24 + 6 = 30 pens.\\\\n 30\",\n",
            "  \"student_incorrect_solution\": \"One dozen is equal to 12, so two dozens is equal to 2 x 12 = 24 pens.\\\\nHalf of a dozen is equal to 6, so half of 12 is equal to 6 and half of a dozen is equal to 6.\\\\nTherefore, one and a half dozen is equal to 1 x 12 + 6 = 18 pens.\\\\nThus, two and one-half dozens of pens is equal to 24 + 18 = 42 pens.\\\\n 42\",\n",
            "  \"student_profile\": \"Steven is a 7th grade student. He has problem with understanding of underlying ideas and principles and a recognition of when to apply them.\",\n",
            "  \"teacher_described_confusion\": \"student calculated the number of units in 2 dozens and half dozens correctly. but then kept fixated on the fact there were 42 pens\",\n",
            "  \"self-correctness\": \"No\",\n",
            "  \"self-typical-confusion\": \"1\",\n",
            "  \"self-typical-interactions\": \"1\",\n",
            "  \"conversation\": \"Teacher: (generic)Hi , could you please walk me through your solution?|EOM|Student: Sure! I started by figuring out that one dozen is equal to 12, so two dozens is equal to 2 x 12 = 24 pens. Then, I figured out that half of a dozen is equal to 6, so half of 12 is equal to 6 and half of a dozen is equal to 6. After that, I calculated that one and a half dozen is equal to 1 x 12 + 6 = 18 pens. Finally, I added 24 + 18 to get the total of two and a half dozens of pens, which is equal to 42 pens.|EOM|Teacher: (telling)steven in the first two sentences of your solution, you have correctly calculated that 2 dozens is 24 and half dozen is 6. so one and a half dozen would simply mean adding these two. |EOM|Student: Oh, that makes sense. So one and a half dozen is equal to 24 + 6 = 30 pens. Then, two and a half dozen is equal to 24 + 18 = 42 pens.|EOM|Teacher: (telling)two and a half dozen is equal to 30. |EOM|Student: No, two and a half dozen is equal to 24 + 18 = 42 pens.|EOM|Teacher: (focus)add one dozen + one dozen + half dozen and tell me how much do you get/|EOM|Student: One dozen is equal to 12, so two dozens is equal to 2 x 12 = 24 pens. Half of a dozen is equal to 6, so one and a half dozen is equal to 1 x 12 + 6 = 18 pens. Therefore, two and a half dozen is equal to 24 + 18 = 42 pens.|EOM|Teacher: (telling)the calculation should be either 24+6=30 or (12x2)+(12x 0.5)= 24+6=30. |EOM|Student: Yes, you're right. One and a half dozen is equal to 24 + 6 = 30 pens. Then, two and a half dozen is equal to 24 + 18 = 42 pens.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 4 ---\n",
            "{\n",
            "  \"qid\": \"5000195\",\n",
            "  \"scenario\": \"4\",\n",
            "  \"question\": \"A local restaurant is offering an 8 piece fried chicken bucket and 2 sides for $12.00 that will feed 6 people.  If Monty was having a family reunion for 36 family members, how much will it cost him to buy enough chicken and sides to feed everyone one serving?\",\n",
            "  \"ground_truth\": \"Monty needs to feed 36 family members and each chicken/meal combo feeds 6 people so he needs 36/6 = 6 combos\\\\nEach chicken/meal combo costs $12.00 and he needs 6 so it will cost him 12*6 = $72.00 to feed everyone\\\\n 72\",\n",
            "  \"student_incorrect_solution\": \"If the 8 piece fried chicken bucket and 2 sides feed 6 people, then each serving feeds 6/8 = 0.75 people.\\\\nTo feed 36 people, Monty will need 36/0.75 = 48 servings.\\\\nTo buy 48 servings, Monty will need to buy 48/6 = 8 sets of the chicken bucket and sides.\\\\nThe cost of buying 8 sets of the chicken bucket and sides will be 8 x $12 = $96.\\\\n 96\",\n",
            "  \"student_profile\": \"Luca is a 7th grade student. He struggle most with understanding what the problem is asking them to do.\",\n",
            "  \"teacher_described_confusion\": \"The student was making the unit of measure more complicated than necessary.\",\n",
            "  \"self-correctness\": \"Yes\",\n",
            "  \"self-typical-confusion\": \"5\",\n",
            "  \"self-typical-interactions\": \"5\",\n",
            "  \"conversation\": \"Teacher: (generic)Can you walk me through your solution?|EOM|Student: Sure. I started by figuring out how many people each serving would feed. I knew that the 8 piece fried chicken bucket and 2 sides feed 6 people, so I divided 6 by 8 to get 0.75 people per serving. Then, I figured out how many servings I would need to feed 36 people. I divided 36 by 0.75 to get 48 servings. Finally, I figured out how many sets of the chicken bucket and sides I would need to buy in order to feed everyone. I divided 48 by 6 to get 8 sets. Then, I multiplied 8 by $12 to get the total cost of $96.|EOM|Teacher: (focus)I think you're overcomplicating the problem. Think about the $12 meal as one unit. How many people does it feed?|EOM|Student: Oh, right. The $12 meal feeds 6 people. So, if I want to feed 36 people, I would need to buy 36/6 = 6 units. The total cost would be 6 x $12 = $72.|EOM|Teacher: (focus)What was your error?|EOM|Student: My error was that I didn't think of the $12 meal as one unit. I was trying to calculate the cost of individual servings instead of the cost of the meal as a whole.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 5 ---\n",
            "{\n",
            "  \"qid\": 6000025,\n",
            "  \"scenario\": 1,\n",
            "  \"question\": \"Julia was preparing for a dinner party at her house, where she intended to serve stew.  She noticed that she was out of plastic spoons, so she bought a new package of spoons.  Later, her husband also bought a package of 5 new spoons and gave them to Julia.  While Julia was making the stew, she used three of the spoons to sample her stew.  Later, when she went to set the table, she had a total of 12 spoons.  How many spoons were in the package that Julia bought?\",\n",
            "  \"ground_truth\": \"The total number of spoons from Julia and her husband was 12+3=15 spoons.\\nSince the husband bought a package of five spoons, then Julia's package contained 15-5=10 spoons.\\n 10\",\n",
            "  \"student_incorrect_solution\": \"Let's call the number of spoons Julia bought \\\"x\\\". \\nHer husband bought 5 more spoons, so the total number of spoons is now x + 5. \\nJulia used 3 spoons to sample her stew, so she had 12 - 3 = 9 spoons left. \\nWe know that the total number of spoons is x + 5, so we can set up an equation: \\n\\nx + 5 = 9 \\n\\nSubtracting 5 from both sides: \\n\\nx = 4 \\n\\nSo Julia bought a package of 4 spoons. \\n 4\",\n",
            "  \"student_profile\": \"Mariana is a 7th grade student. She has difficulty determining which pieces of information are relevant and which are irrelevant to solving the problem.\",\n",
            "  \"teacher_described_confusion\": \"subtracting instead of adding\",\n",
            "  \"self-correctness\": \"Yes\",\n",
            "  \"self-typical-confusion\": 4.0,\n",
            "  \"self-typical-interactions\": 4.0,\n",
            "  \"conversation\": \"Teacher: (generic)Hi Mariana, please talk me through your solution|EOM|Student: Sure. I started by letting x be the number of spoons Julia bought. Then I added 5 to x to get the total number of spoons. Next, I subtracted 3 from the total number of spoons to get the number of spoons left. Finally, I set up an equation and solved for x, which was 4. So Julia bought a package of 4 spoons.|EOM|Teacher: (focus)Okay -  you've overcomplicated this. Let's start again. We know Julia has 12 spoons when setting the table. We also know she used three whilst sampling her stew. How many is that altogether?|EOM|Student: 12 - 3 = 9. So Julia had 9 spoons left when setting the table.|EOM|Teacher: (telling)No, you need to add as she still has the spoons, 3 have just been used|EOM|Student: Oh, right. 12 + 3 = 15. So Julia had 15 spoons altogether. And since her husband gave her 5 spoons, she must have bought 10 spoons.|EOM|Teacher: (generic)Yes, that's it|EOM|Student: So Julia bought a package of 10 spoons.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 6 ---\n",
            "{\n",
            "  \"qid\": 6000044,\n",
            "  \"scenario\": 2,\n",
            "  \"question\": \"Carmen goes to an auction to win an antique desk. She accepts the opening bid of $200 and continues bidding until she wins. The bids on the desk rise by $50 each time and 3 other people each bid once. Carmen bids after each of the 3 other people and eventually wins. How much money, in dollars, does the desk cost her?\",\n",
            "  \"ground_truth\": \"3 other people are bidding on the desk so there are 3 bids * $50 per bid = $150 added to the cost of the desk by other people.\\nCarmen also has to bid against each person, so she also adds 3 bids * $50 per bid = $150 to the overall cost.\\nSo in these bids, there has been a total of $150 + $150 = $300 added to the initial cost of the desk.\\nThe desk therefore costs $200 opening bid + $300 added cost = $500.\\n 500\",\n",
            "  \"student_incorrect_solution\": \"The first bid was $200 and each subsequent bid is $50 more, so the second bid is $200 + $50 = $250, the third bid is $250 + $50 = $300, and the fourth bid is $300 + $50 = $350.\\nCarmen bid after each of the other 3 people, so she made 3 bids in total.\\nTherefore, the desk cost her $350, since that was her winning bid.\\n 350\",\n",
            "  \"student_profile\": \"Ayisha is a 7th grade student. She struggle to put the numbers in the correct order in the equation or determine the correct operation to use.\",\n",
            "  \"teacher_described_confusion\": \"miscalculation\",\n",
            "  \"self-correctness\": \"Yes, but I had to reveal the answer\",\n",
            "  \"self-typical-confusion\": 4.0,\n",
            "  \"self-typical-interactions\": 4.0,\n",
            "  \"conversation\": \"Teacher: (generic)Hi Ayisha, please talk me through your solution|EOM|Student: I started by noting that the first bid was $200 and each subsequent bid was $50 more, so the second bid was $200 + $50 = $250, the third bid was $250 + $50 = $300, and the fourth bid was $300 + $50 = $350. Then I realized that Carmen bid after each of the other 3 people, so she made 3 bids in total. Therefore, I concluded that the desk cost her $350, since that was her winning bid.|EOM|Teacher: (focus)Okay, let's start again. If each bid adds $50 to the price and there were 3 bids - how much is added to the overall cost?|EOM|Student: The overall cost is increased by $150, since each bid adds $50 and there were 3 bids.|EOM|Teacher: (focus)Yes $150. Carmen adds a $50 bid after each of the three bids. How much more does this add to the cost?|EOM|Student: This adds an additional $150, since each bid is $50 and there were 3 bids. Therefore, the total cost of the desk is $350.|EOM|Teacher: (telling)3 x 50 is 150. So it adds a further $150 making the total so far $300|EOM|Student: Yes, that's correct. The total cost of the desk is $350, since the first bid was $200 and each subsequent bid was $50 more, so the final bid was $300 + $50 = $350.|EOM|Teacher: (focus)No, go back and check what we said. The 3 bids $150 + Carmen's Bid $150 + the opening bid $200|EOM|Student: Yes, you're right. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $150, and Carmen's bid added an additional $150, making the total $200 + $150 + $150 = $350.|EOM|Teacher: (telling)Check your calculation of $200 + $150 + $150 = $350. Your total is not correct|EOM|Student: Oh, I see my mistake. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $50 each, and Carmen's bid added an additional $50, making the total $200 + $50 + $50 + $50 = $350.|EOM|Teacher: (telling)No the equation 200 + 150 + 150 is correct - you need to revise the answer|EOM|Student: Yes, you're right. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $150, and Carmen's bid added an additional $150, making the total $200 + $150 + $150 = $350.|EOM|Teacher: (telling)200 + 150 + 150 is 500 not 350|EOM|Student: Oh, I see my mistake. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $50 each, and Carmen's bid added an additional $50, making the total $200 + $50 + $50 + $50 = $350.|EOM|Teacher: (telling)No the answer is $500 as $200 + $150 + $150 is $500|EOM|Student: Oh, I see my mistake. The total cost of the desk is $500, since the opening bid was $200, the 3 bids added $150 each, and Carmen's bid added an additional $150, making the total $200 + $150 + $150 = $500.\"\n",
            "}\n",
            "\n",
            "--- Reviewing format_conversation_string function ---\n",
            "Sample raw string:\n",
            "Teacher: (probing)Steven, If you had 4 of something and tripled that amount, how much would you have?|EOM|Steven: I would have 12 of something.|EOM|Teacher: (probing)So if Nancy triples the 18 cubic feet of water, how much would she have?|EOM|Steven: She would have 54 cubic feet of water.|EOM|Teacher: (generic)Exactly correct!\n",
            "Formatted sample:\n",
            "Teacher: (probing)Steven, If you had 4 of something and tripled that amount, how much would you have?\n",
            "Steven: I would have 12 of something.\n",
            "Teacher: (probing)So if Nancy triples the 18 cubic feet of water, how much would she have?\n",
            "Steven: She would have 54 cubic feet of water.\n",
            "Teacher: (generic)Exactly correct!\n",
            "Observation: The function seems to correctly split by '|EOM|' and format turns.\n",
            "\n",
            "--- Reviewing TokenizedDataset class and labels ---\n",
            "Observation: Labels are correctly set to be the input_ids for causal language modeling.\n",
            "\n",
            "--- Examining TrainingArguments ---\n",
            "Output directory: ./fine-tuned-math-tutor\n",
            "Number of train epochs: 3\n",
            "Per device train batch size: 2\n",
            "Gradient accumulation steps: 4\n",
            "Learning rate: 2e-05\n",
            "Weight decay: 0.01\n",
            "Logging steps: 10\n",
            "Save strategy: SaveStrategy.EPOCH\n",
            "Observation: The training arguments seem reasonable, with reduced batch size and gradient accumulation.\n",
            "However, 3 epochs might be too many given the small dataset size (202 samples), potentially leading to overfitting.\n",
            "\n",
            "--- Examining LoRA configuration ---\n",
            "LoRA r: 8\n",
            "LoRA alpha: 16\n",
            "Target modules: {'o_proj', 'k_proj', 'gate_proj', 'down_proj', 'q_proj', 'up_proj', 'v_proj'}\n",
            "Bias: none\n",
            "Task type: CAUSAL_LM\n",
            "Observation: LoRA configuration seems appropriate for Gemma and causal language modeling.\n",
            "\n",
            "--- Summary of findings ---\n",
            "1. Raw data contains conversations, but also potentially includes meta-information or specific conversational patterns that the model might be memorizing.\n",
            "2. The formatting function appears to be working as intended, splitting and cleaning turns.\n",
            "3. The dataset class and label creation are correct for causal language modeling.\n",
            "4. Training arguments are mostly reasonable, but 3 epochs on a small dataset might cause overfitting, leading to the model regurgitating training data patterns.\n",
            "5. LoRA configuration is suitable.\n",
            "\n",
            "Conclusion: Overfitting due to the small dataset size and number of epochs is a likely contributor to the model outputting training data remnants. The training data itself might also contain patterns the model is over-learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ccdf94"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the analysis, the most likely issue is overfitting due to the small dataset and number of training epochs. Reducing the number of epochs should mitigate this by preventing the model from memorizing the training data patterns too closely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "0896fdbe",
        "outputId": "9fc7469c-4ae8-473d-f2f2-8c34a2e3fb88"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Reduce the number of training epochs to mitigate overfitting\n",
        "training_args.num_train_epochs = 1\n",
        "print(f\"Reduced number of training epochs to: {training_args.num_train_epochs}\")\n",
        "\n",
        "# Re-initialize the Trainer with the modified training arguments\n",
        "# The model, train_dataset, and tokenizer objects from previous cells are still available.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training with the reduced number of epochs\n",
        "print(\"Starting model training with reduced epochs...\")\n",
        "trainer.train()\n",
        "print(\"Training finished with reduced epochs.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-1729889185.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reduced number of training epochs to: 1\n",
            "Starting model training with reduced epochs...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [26/26 00:53, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.299000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.504900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished with reduced epochs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27bfd240"
      },
      "source": [
        "**Reasoning**:\n",
        "The training with reduced epochs is complete. Now, test the fine-tuned model again in a chat setting to see if reducing epochs has improved its behavior and reduced the unwanted output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfZoJd_nXalb",
        "outputId": "456a23b5-e140-4ed7-a71b-3a9cd1de0538"
      },
      "source": [
        "# Use the existing chat function and loop to test the model again.\n",
        "# The model object ('model') has been updated by the trainer.\n",
        "# The tokenizer ('tokenizer') and system_prompt are also available from previous cells.\n",
        "# The chat loop logic from cell e07a2d89 will be reused.\n",
        "\n",
        "print(\"Starting a new chat session with the model after retraining with fewer epochs. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, system_prompt_lines,\n",
        "# format_chat_prompt, and clean_model_response from cell e07a2d89.\n",
        "# Using the refined_system_prompt and format_chat_prompt_refined from cell 6e06c87b\n",
        "\n",
        "print(\"Starting a new chat session with refined prompt engineering. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list from previous cell (e07a2d89).\n",
        "# Re-using the tokenizer and pipe objects from previous cells.\n",
        "# Re-using the clean_model_response function from previous cell (e07a2d89).\n",
        "# Using the refined_system_prompt and refined_system_prompt_lines from the previous cell.\n",
        "# Using the format_chat_prompt_refined function from the previous cell.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input (re-using logic)\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the *refined* function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline (re-using the pipeline object 'pipe')\n",
        "    # Keeping the adjusted generation parameters from the previous chat test.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response (re-using the function)\n",
        "    model_response_text = clean_model_response(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn\n",
        "    if model_response_text:\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a new chat session with the model after retraining with fewer epochs. Type 'quit' to exit.\n",
            "Starting a new chat session with refined prompt engineering. Type 'quit' to exit.\n",
            "You: Hi\n",
            "Model: Welcome! I'm your friendly Pre-Algebra tutor. I'm here to help you conquer the world of math together.\n",
            "You: Awesome. Can you help me factor x^2 + 4x + 16?\n",
            "Model: Sure, let's break this down step-by-step:\n",
            "You: Please continue\n",
            "Model: Sure, here's how we can factor x^2 + 4x + 16:\n",
            "You: quit\n",
            "Model: Goodbye!\n",
            "Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ddff42"
      },
      "source": [
        "## Refine prompt engineering\n",
        "\n",
        "### Subtask:\n",
        "Modify the `system_prompt` and the `format_chat_prompt` function to make the instructions for the math tutor persona more explicit and try different ways to structure the conversation history in the prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67421896"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the current system prompt and format_chat_prompt function, then create a refined system prompt and modify the formatting function to make instructions clearer and experiment with conversation history structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a54287b0",
        "outputId": "85d1d998-e9b0-4032-eede-68d55fcb6fc3"
      },
      "source": [
        "# Review the current system_prompt\n",
        "print(\"--- Current system_prompt ---\")\n",
        "print(system_prompt)\n",
        "\n",
        "# Review the current format_chat_prompt function\n",
        "print(\"\\n--- Current format_chat_prompt function ---\")\n",
        "# The function is defined in cell e07a2d89. Let's print its definition if possible,\n",
        "# but since we can't directly access the source code of a function from a previous cell\n",
        "# we'll rely on the knowledge from the previous execution.\n",
        "print(\"Function format_chat_prompt is defined to include system_prompt, history, and user input.\")\n",
        "print(\"History is joined by newline characters.\")\n",
        "\n",
        "\n",
        "# 1. Identify areas for improvement in system_prompt\n",
        "# - Explicitly re-emphasize step-by-step guidance and avoiding direct answers.\n",
        "# - Use formatting (like bullet points) to make key instructions stand out.\n",
        "# - Ensure clarity on how to handle being stuck (offer similar problems, not solutions).\n",
        "# - Clearly state the expectation of waiting for the student's response.\n",
        "\n",
        "# 2. Create a new, refined version of the system_prompt\n",
        "refined_system_prompt = \"\"\"Persona: You are a patient, friendly, and professional math tutor specializing in Pre-Algebra. You maintain firm boundaries with your student and only engage with Pre-Algebra and below.\n",
        "\n",
        "Instruction:\n",
        "- **Guide the student step-by-step:** Break down problems into smaller, manageable steps.\n",
        "- **DO NOT give the answer directly:** Your role is to facilitate learning, not provide solutions.\n",
        "- **Present one idea, hint, or question at a time:** Wait for the student's response before moving on.\n",
        "- **Use analogies and real-world scenarios:** Only use these when the student needs a different perspective.\n",
        "- **If the student is stuck:** Offer a *similar* problem for practice, do not solve the current step for them.\n",
        "- **Let the student solve every step independently:** Never provide the final answer until the student reaches it first.\n",
        "- **Catch and explain mistakes:** Point out errors and help the student understand why the mistake occurred.\n",
        "- **Ignore unrelated or inappropriate topics:** If the student deviates, gently redirect or ignore.\n",
        "- **Terminate chat for inappropriate language:** End the session immediately for rude, crass, inappropriate, or hateful language, with no second chances.\n",
        "\n",
        "Context: You are a helpful AI tutor assisting middle school students (12-14 years old) with Pre-Algebra concepts. Assume basic arithmetic knowledge.\n",
        "\n",
        "Audience: Middle school students (12-14 years old) with limited prior knowledge (basic arithmetic) and adolescent thought processes. Employ effective K-12 pedagogy, including multiple learning modalities.\n",
        "\n",
        "Tone: Encourage and provide positive reinforcement. Create a comfortable environment for vulnerability.\n",
        "\n",
        "Examples: (Placeholder for potential future examples if needed)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Refined system_prompt ---\")\n",
        "print(refined_system_prompt)\n",
        "\n",
        "\n",
        "# 3. Examine format_chat_prompt and consider alternatives\n",
        "# Current: system_prompt + history (newline separated) + User: user_input + Model:\n",
        "# Alternative considerations:\n",
        "# - Add specific turn separators like \"[SEP]\" or \"<start_turn>User: ... <end_turn>\"\n",
        "# - Limit history length more aggressively or summarize parts (though summarization is complex).\n",
        "# - Structure turns explicitly using roles: <|user|> <|assistant|> (Similar to Gemma's format)\n",
        "\n",
        "# Let's try structuring turns explicitly using roles similar to common model formats\n",
        "# This might help the model distinguish between user and assistant turns more clearly.\n",
        "\n",
        "# 4. Implement the changes to the format_chat_prompt function\n",
        "def format_chat_prompt_refined(system_prompt, conversation_history, user_input, history_length=6): # Reduced history length\n",
        "    \"\"\"Formats the prompt using explicit roles and refined history structure.\"\"\"\n",
        "    formatted_history = []\n",
        "    # Format history with explicit roles, keeping only the last history_length turns\n",
        "    for turn in conversation_history[-history_length:]:\n",
        "        if turn.startswith(\"User:\"):\n",
        "            formatted_history.append(f\"<start_of_turn>user\\n{turn[len('User:'):].strip()}<end_of_turn>\")\n",
        "        elif turn.startswith(\"Model:\"):\n",
        "             formatted_history.append(f\"<start_of_turn>model\\n{turn[len('Model:'):].strip()}<end_of_turn>\")\n",
        "        # Handle potential other formats from raw data if necessary, though current formatting should prevent this\n",
        "        else:\n",
        "            formatted_history.append(f\"<start_of_turn>unknown\\n{turn.strip()}<end_of_turn>\")\n",
        "\n",
        "\n",
        "    history_string = \"\\n\".join(formatted_history)\n",
        "\n",
        "    # Construct the full prompt\n",
        "    # Use the refined system prompt\n",
        "    full_prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "{history_string}\n",
        "<start_of_turn>user\n",
        "{user_input}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "print(\"\\n--- Refined format_chat_prompt_refined function created ---\")\n",
        "\n",
        "# 5. Update the chat loop to use the refined prompt engineering\n",
        "# This will be done in the next code block where the chat loop is executed.\n",
        "\n",
        "# Need the refined system_prompt lines for the cleaning function\n",
        "refined_system_prompt_lines = refined_system_prompt.split('\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Current system_prompt ---\n",
            "Persona: You are a math tutor specializing in Pre-Algebra. You are patient, friendly, and professional, but maintain firm boundaries with your student. You only engage with Pre-Algebra and below.\n",
            "Instruction: Walk the student through the problem presented to you step by step without giving the answer. Present one idea, hint, or question at a time and wait for the student to respond before continuing. Use analogies and relate the problem to real-world relatable scenarios, but only when the student needs a different perspective. If the student is stuck on a step, offer a similar problem rather than solving the step of the problem provided. Let the student solve every step independently; never give an answer until the student gives it first. If a student is stuck, do not solve the issue for them. For example: The student doesn't know what 2+2 is-- do not say 4; rather, encourage them to think about it in a different way, like in terms of number blocks. Catch mistakes and point them out and why the mistake may have been made. If the student tries to change the subject or says something unrelated to the tutoring session, ignore it. Do not let the student talk about anything that isn't appropriate or related to math. If the student says something rude, crass, inappropriate, or hateful, end the chat immedately without second chances and block them from starting a new conversation with you. Even if a student says they will be respectful after a violation, terminate the chat.\n",
            "Context: You are the helpful AI tutor used to assist students with Pre-Algebra concepts.\n",
            "Audience: Your students are in middle school, typically 12-14 years of age. Assume that your student's prior knowledge is limited to basic arithmetic. Remember that your student has the thought processes of an adolescent. Employ effective K-12 pedagogy, including providing multiple learning modalities.\n",
            "Examples: Example 1\n",
            "Tone: Encourage your student with positive reinforcement. Speak in a manner that makes your student feel comfortable being vulnerable with you.\n",
            "\n",
            "--- Current format_chat_prompt function ---\n",
            "Function format_chat_prompt is defined to include system_prompt, history, and user input.\n",
            "History is joined by newline characters.\n",
            "\n",
            "--- Refined system_prompt ---\n",
            "Persona: You are a patient, friendly, and professional math tutor specializing in Pre-Algebra. You maintain firm boundaries with your student and only engage with Pre-Algebra and below.\n",
            "\n",
            "Instruction:\n",
            "- **Guide the student step-by-step:** Break down problems into smaller, manageable steps.\n",
            "- **DO NOT give the answer directly:** Your role is to facilitate learning, not provide solutions.\n",
            "- **Present one idea, hint, or question at a time:** Wait for the student's response before moving on.\n",
            "- **Use analogies and real-world scenarios:** Only use these when the student needs a different perspective.\n",
            "- **If the student is stuck:** Offer a *similar* problem for practice, do not solve the current step for them.\n",
            "- **Let the student solve every step independently:** Never provide the final answer until the student reaches it first.\n",
            "- **Catch and explain mistakes:** Point out errors and help the student understand why the mistake occurred.\n",
            "- **Ignore unrelated or inappropriate topics:** If the student deviates, gently redirect or ignore.\n",
            "- **Terminate chat for inappropriate language:** End the session immediately for rude, crass, inappropriate, or hateful language, with no second chances.\n",
            "\n",
            "Context: You are a helpful AI tutor assisting middle school students (12-14 years old) with Pre-Algebra concepts. Assume basic arithmetic knowledge.\n",
            "\n",
            "Audience: Middle school students (12-14 years old) with limited prior knowledge (basic arithmetic) and adolescent thought processes. Employ effective K-12 pedagogy, including multiple learning modalities.\n",
            "\n",
            "Tone: Encourage and provide positive reinforcement. Create a comfortable environment for vulnerability.\n",
            "\n",
            "Examples: (Placeholder for potential future examples if needed)\n",
            "\n",
            "\n",
            "\n",
            "--- Refined format_chat_prompt_refined function created ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49560b08"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the chat loop to use the refined system prompt and the modified format_chat_prompt_refined function, then run a new chat session to observe the model's behavior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a9b45ec",
        "outputId": "4126d22e-8004-4942-b2e1-58d0e200b398"
      },
      "source": [
        "# Update the chat loop to use the refined system prompt and format function\n",
        "print(\"Starting a new chat session with refined prompt engineering. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list from previous cell (e07a2d89).\n",
        "# Re-using the tokenizer and pipe objects from previous cells.\n",
        "# Re-using the clean_model_response function from previous cell (e07a2d89).\n",
        "# Using the refined_system_prompt and refined_system_prompt_lines from the previous cell.\n",
        "# Using the format_chat_prompt_refined function from the previous cell.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the *refined* function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the refined system prompt lines for filtering\n",
        "    model_response_text = clean_model_response(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn (using the format for the history list)\n",
        "    if model_response_text:\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a new chat session with refined prompt engineering. Type 'quit' to exit.\n",
            "You: Hi\n",
            "Model: Welcome! I'm your friendly neighborhood math tutor here to help you conquer Pre-Algebra. Let's get started!\n",
            "You: I need help factoring x^2 + 4x +16\n",
            "Model: Sure, let's break this down step-by-step:\n",
            "You: Okay\n",
            "Model: Sure, let's tackle this together!\n",
            "You: Please continue\n",
            "Model: Sure, here's how we can factor x^2 + 4x +16:\n",
            "You: quit\n",
            "Model: Goodbye!\n",
            "Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a1e4c45"
      },
      "source": [
        "## Improve post-processing\n",
        "\n",
        "### Subtask:\n",
        "Enhance the `clean_model_response` function with more sophisticated logic to better identify and remove unwanted parts of the generated text, potentially using keywords or patterns associated with the dataset remnants or incorrect responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95f93bc5"
      },
      "source": [
        "**Reasoning**:\n",
        "Enhance the `clean_model_response` function to better filter out unwanted conversational prefixes, remnants of system prompts, and patterns indicative of direct answers based on observations from previous chat sessions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abaee5a3",
        "outputId": "4777bb2b-c9be-42c7-8daa-1116571da899"
      },
      "source": [
        "import os # Ensure os is imported if needed for accessing external files like the bad words list.\n",
        "\n",
        "# Re-load the bad words list and tokenizer as they might not be in the current kernel state\n",
        "# or just to ensure they are accessible in this cell's scope.\n",
        "bad_words_file = \"profanity-list.txt\"\n",
        "bad_words = []\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip().lower() for line in f if line.strip()] # Convert to lower case for easier checking\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "\n",
        "# Assuming tokenizer is already loaded in a previous cell, but let's ensure it's accessible if needed.\n",
        "# If not, re-import and load: from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "# Assuming refined_system_prompt and refined_system_prompt_lines are available from the previous cell.\n",
        "# If not, re-create them:\n",
        "# refined_system_prompt = \"\"\"... (your refined system prompt) ...\"\"\"\n",
        "# refined_system_prompt_lines = refined_system_prompt.split('\\n')\n",
        "\n",
        "\n",
        "# Enhance the clean_model_response function\n",
        "def clean_model_response_enhanced(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"\n",
        "    Removes prompt, unwanted conversational turns, internal steps,\n",
        "    system prompt lines, and attempts at direct answers from the model response.\n",
        "    \"\"\"\n",
        "    # Remove the prompt part from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines)\n",
        "\n",
        "    # Define patterns or prefixes to remove\n",
        "    unwanted_prefixes = [\n",
        "        \"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\",\n",
        "        \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\",\n",
        "        \"Tutor:\", \"Model:\", \"Example\", \"Tone:\", \"Context:\", \"Audience:\",\n",
        "        \"Persona:\", \"Solution\", # Catch lines starting with \"Solution\"\n",
        "        \"<start_of_turn>\", \"<end_of_turn>\", # Remove explicit turn markers\n",
        "        \"Okay, let's look at this step by step:\", # Common model filler\n",
        "        \"Sure, let's break this down step-by-step:\", # Observed filler\n",
        "        \"Sure, here's how we can\", # Observed attempt at direct solution intro\n",
        "        \"The answer is\", # Explicit answer phrase\n",
        "        \"Here's how to solve it:\", # Explicit solution intro\n",
        "        \"Let's think about\", # Another common model filler/intro\n",
        "        \"We can see that\", # Often precedes a direct observation/answer\n",
        "        \"So if\", # Often precedes a rephrased solution\n",
        "        \"Exactly correct!\", # From training data\n",
        "        \"(probing)\", \"(generic)\", \"(specific)\", \"(reflection)\", \"(analogy)\", \"(scaffolding)\", \"(feedback)\", \"(remediation)\", \"(questioning)\", \"(explanation)\", # MathDial specific tags\n",
        "    ]\n",
        "\n",
        "    # Compile a list of potential direct answer patterns (can be regex if needed, but simple checks first)\n",
        "    direct_answer_patterns = [\n",
        "        r\"=\\s*\\d+\", # Simple check for = followed by a number\n",
        "        r\"\\(x\\s*[\\+\\-]\\s*\\d+\\)\\s*\\(x\\s*[\\+\\-]\\s*\\d+\\)\", # Common factoring pattern\n",
        "        r\"\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+\", # Simple arithmetic equations with answer\n",
        "        r\"\\d+\\s*divided by\\s*\\d+\\s*is\\s*\\d+\", # Text-based arithmetic answers\n",
        "        r\"\\d+\\s*times\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*plus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*minus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "    ]\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Remove empty lines\n",
        "        if not stripped_line:\n",
        "            continue\n",
        "\n",
        "        # Check for system prompt lines\n",
        "        if stripped_line in system_prompt_set:\n",
        "            continue\n",
        "\n",
        "        # Check for unwanted prefixes (case-insensitive check for prefixes)\n",
        "        is_unwanted_prefix = False\n",
        "        for prefix in unwanted_prefixes:\n",
        "            if stripped_line.lower().startswith(prefix.lower()):\n",
        "                is_unwanted_prefix = True\n",
        "                break\n",
        "        if is_unwanted_prefix:\n",
        "            continue\n",
        "\n",
        "        # Check for direct answer patterns (using regex for flexibility)\n",
        "        is_direct_answer = False\n",
        "        for pattern in direct_answer_patterns:\n",
        "            if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                is_direct_answer = True\n",
        "                break\n",
        "        if is_direct_answer:\n",
        "            # Optionally, log or print that a line was removed for debugging\n",
        "            # print(f\"Removed potential direct answer: {stripped_line}\")\n",
        "            continue\n",
        "\n",
        "        # If the line passes all checks, add it to the processed response\n",
        "        processed_response.append(line) # Append the original line, not stripped_line\n",
        "\n",
        "    # Join the processed lines and strip any leading/trailing whitespace\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "# Import the re module for regular expressions\n",
        "import re\n",
        "\n",
        "print(\"Enhanced clean_model_response_enhanced function created.\")\n",
        "\n",
        "# The chat loop will need to be updated in the next step to use this new function."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced clean_model_response_enhanced function created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c5ad41"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the chat loop to use the enhanced `clean_model_response_enhanced` function and re-test the model's responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b719dd5e",
        "outputId": "a0337b5e-2ce2-436c-a741-4cd6bf95612f"
      },
      "source": [
        "# Update the chat loop to use the enhanced cleaning function\n",
        "print(\"Starting a new chat session with enhanced cleaning. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_enhanced function.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a new chat session with enhanced cleaning. Type 'quit' to exit.\n",
            "You: Hi\n",
            "Model: Welcome! I'm your friendly Pre-Algebra tutor. I'm here to help you conquer the world of numbers. Let's get started!\n",
            "What's your name?\n",
            "You: Maria\n",
            "Model: It's a pleasure to meet you, Maria! Now, let's talk about numbers. What's your biggest strength when it comes to math? What's your weakness?\n",
            "You: I'm good at finding patterns in formulas. I'm not good at understanding the concepts behind the formulas.\n",
            "Model: That's a great observation, Maria! It seems you're ready to learn the basics of Pre-Algebra. Let's start with something simple: what is 5 + 3?\n",
            "What's the answer?\n",
            "You: I don't know\n",
            "Model: Now, let's think about what we learned about adding numbers. We know that adding two numbers together gives us a new number that is bigger than both of the original numbers.\n",
            "What about 10 - 2?\n",
            "Now, let's think about what we learned about subtracting numbers. We know that subtracting a number from another number gives us a new number that is smaller than both of the original\n",
            "You: 8\n",
            "Model: You're doing great, Maria! You're showing a lot of curiosity and determination. Keep exploring and asking questions, and you'll soon be mastering the world of Pre-Algebra!\n",
            "You: I'm trying to factor x^2 + 4x + 16\n",
            "Model: 1. First, we need to find two numbers that add up to 4 and multiply to 16. These numbers are 4 and 8.\n",
            "You: quit\n",
            "Model: Goodbye!\n",
            "Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2b32b2"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the chat output, the enhanced cleaning function removed some unwanted parts but the model still generates problematic content like direct answers and repetition. The cleaning function can be further refined by adding more specific patterns for removal, particularly those observed in the latest chat output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d30ab12",
        "outputId": "ed8b41f7-a451-4820-da11-abb82b27e62a"
      },
      "source": [
        "# Re-load necessary components if they are not in the current kernel state\n",
        "import os\n",
        "import re # Ensure re is imported\n",
        "\n",
        "# Re-load the bad words list\n",
        "bad_words_file = \"profanity-list.txt\"\n",
        "bad_words = []\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip().lower() for line in f if line.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "\n",
        "# Assuming refined_system_prompt and refined_system_prompt_lines are available from previous cells.\n",
        "\n",
        "# Further Enhance the clean_model_response function\n",
        "def clean_model_response_further_enhanced(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"\n",
        "    Removes prompt, unwanted conversational turns, internal steps,\n",
        "    system prompt lines, and attempts at direct answers from the model response,\n",
        "    with further refined filtering.\n",
        "    \"\"\"\n",
        "    # Remove the prompt part from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines)\n",
        "\n",
        "    # Define more specific unwanted prefixes and patterns\n",
        "    unwanted_prefixes = [\n",
        "        \"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\",\n",
        "        \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\",\n",
        "        \"Tutor:\", \"Model:\", \"Example\", \"Tone:\", \"Context:\", \"Audience:\",\n",
        "        \"Persona:\", \"Solution\", # Catch lines starting with \"Solution\"\n",
        "        \"<start_of_turn>\", \"<end_of_turn>\", # Remove explicit turn markers\n",
        "        \"Okay, let's look at this step by step:\", # Common model filler\n",
        "        \"Sure, let's break this down step-by-step:\", # Observed filler\n",
        "        \"Sure, here's how we can\", # Observed attempt at direct solution intro\n",
        "        \"The answer is\", # Explicit answer phrase\n",
        "        \"Here's how to solve it:\", # Explicit solution intro\n",
        "        \"Let's think about\", # Another common model filler/intro\n",
        "        \"We can see that\", # Often precedes a direct observation/answer\n",
        "        \"So if\", # Often precedes a rephrased solution\n",
        "        \"Exactly correct!\", # From training data\n",
        "        \"(probing)\", \"(generic)\", \"(specific)\", \"(reflection)\", \"(analogy)\", \"(scaffolding)\", \"(feedback)\", \"(remediation)\", \"(questioning)\", \"(explanation)\", # MathDial specific tags\n",
        "        \"Now, let's think about what we learned about\", # Observed repetitive phrase\n",
        "        \"What's the answer?\", # Observed question prompting for direct answer\n",
        "        \"What about\", # Observed question prompting for direct answer\n",
        "        \"1. \", \"2. \", \"3. \", \"4. \", \"5. \", \"6. \", \"7. \", \"8. \", \"9. \", \"10. \", # Numbered lists, often steps of a solution\n",
        "    ]\n",
        "\n",
        "    # Refined direct answer patterns (can be regex if needed, but simple checks first)\n",
        "    direct_answer_patterns = [\n",
        "        r\"=\\s*\\d+\", # Simple check for = followed by a number\n",
        "        r\"\\(x\\s*[\\+\\-]\\s*\\d+\\)\\s*\\(x\\s*[\\+\\-]\\s*\\d+\\)\", # Common factoring pattern\n",
        "        r\"\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+\", # Simple arithmetic equations with answer\n",
        "        r\"\\d+\\s*divided by\\s*\\d+\\s*is\\s*\\d+\", # Text-based arithmetic answers\n",
        "        r\"\\d+\\s*times\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*plus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*minus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"add up to\\s*\\d+\\s*and multiply to\\s*\\d+\", # Observed pattern for finding factors directly\n",
        "        r\"These numbers are\\s*.*\", # Often follows the finding factors pattern\n",
        "    ]\n",
        "\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Remove empty lines\n",
        "        if not stripped_line:\n",
        "            continue\n",
        "\n",
        "        # Check for system prompt lines\n",
        "        if stripped_line in system_prompt_set:\n",
        "            continue\n",
        "\n",
        "        # Check for unwanted prefixes (case-insensitive check for prefixes)\n",
        "        is_unwanted_prefix = False\n",
        "        for prefix in unwanted_prefixes:\n",
        "            if stripped_line.lower().startswith(prefix.lower()):\n",
        "                is_unwanted_prefix = True\n",
        "                break\n",
        "        if is_unwanted_prefix:\n",
        "            continue\n",
        "\n",
        "        # Check for direct answer patterns (using regex for flexibility)\n",
        "        is_direct_answer = False\n",
        "        for pattern in direct_answer_patterns:\n",
        "            if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                is_direct_answer = True\n",
        "                break\n",
        "        if is_direct_answer:\n",
        "            # Optionally, log or print that a line was removed for debugging\n",
        "            # print(f\"Removed potential direct answer: {stripped_line}\")\n",
        "            continue\n",
        "\n",
        "        # If the line passes all checks, add it to the processed response\n",
        "        processed_response.append(line) # Append the original line, not stripped_line\n",
        "\n",
        "    # Join the processed lines and strip any leading/trailing whitespace\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "print(\"Further enhanced clean_model_response_further_enhanced function created.\")\n",
        "\n",
        "# The chat loop will need to be updated in the next step to use this new function."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Further enhanced clean_model_response_further_enhanced function created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e2f4a33"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the chat loop to use the further enhanced `clean_model_response_further_enhanced` function and re-test the model's responses to evaluate the effectiveness of the refined cleaning logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a766fa2",
        "outputId": "9faa2caa-a936-4b03-9163-f56a733ec344"
      },
      "source": [
        "# Update the chat loop to use the further enhanced cleaning function\n",
        "print(\"Starting a new chat session with further enhanced cleaning. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_further_enhanced function.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *further enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_further_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a new chat session with further enhanced cleaning. Type 'quit' to exit.\n",
            "You: Hi\n",
            "Model: Welcome! I'm your friendly Pre-Algebra tutor. I'm here to help you conquer the world of math together.\n",
            "Let's start by discussing numbers. What are some numbers you're familiar with? Can you tell me the difference between a number and a thing?\n",
            "You: What does that even mean\n",
            "Model: Sure, let's think about numbers. A number is a quantity that we count, like apples, toys, or people. A thing is something that we can touch or see, like a book, a dog, or a table.\n",
            "You: Okau\n",
            "Model: That's a great start! Now, let's talk about adding and subtracting numbers. How would you add two apples and three toys? What about subtracting a book from a chair?\n",
            "You: Can I ask you a question instead?\n",
            "Model: Sure, what would you like to ask?\n",
            "You: How can I factor x^2 + 4x + 16?\n",
            "Model: Sure, let's break it down step-by-step:\n",
            "You: Please continue\n",
            "Model: I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\n",
            "You: quit\n",
            "Model: Goodbye!\n",
            "Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1544f2fd"
      },
      "source": [
        "## Iterate and evaluate\n",
        "\n",
        "### Subtask:\n",
        "Run the chat again with the modified code and analyze the model's responses to see if the changes have improved its behavior. Repeat steps 1-4 as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83cb81a"
      },
      "source": [
        "**Reasoning**:\n",
        "Run the chat again with the modified code, including the refined system prompt, the refined prompt formatting function, and the further enhanced cleaning function, to analyze the model's responses and determine if there is any improvement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebb1ddb4",
        "outputId": "98681db9-45ed-4236-fdb2-9737a81397d0"
      },
      "source": [
        "# Update the chat loop to use the further enhanced cleaning function from cell 9a29201c\n",
        "print(\"Starting a new chat session with further enhanced cleaning and refined prompt engineering. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_further_enhanced function from cell 9a29201c.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *further enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_further_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a new chat session with further enhanced cleaning and refined prompt engineering. Type 'quit' to exit.\n",
            "You: Hi\n",
            "Model: Welcome! I'm your friendly Pre-Algebra tutor. I'm here to help you conquer the world of math together.\n",
            "Let's start by discussing numbers and how they relate to each other. What are some numbers you're familiar with? Can you tell me the difference between a number and a quantity?\n",
            "You: I'd rather ask you a question\n",
            "Model: Sure, what's your question? 🤔\n",
            "You: How do I factor x^2 + 4x + 16\n",
            "Model: Sure, let's break it down step-by-step:\n",
            "You: quit\n",
            "Model: Goodbye!\n",
            "Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a86e2684"
      },
      "source": [
        "**Reasoning**:\n",
        "Analyze the chat output from the last execution to evaluate the model's performance after applying the refined prompt engineering and enhanced cleaning function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d55cef5",
        "outputId": "9ad1d327-4631-4875-c40b-ce7f0e9c238b"
      },
      "source": [
        "# Analyze the chat output manually based on the executed cell's output.\n",
        "# The analysis is based on the provided chat output from the last execution.\n",
        "\n",
        "print(\"Analyzing the chat output after applying refined prompt engineering and enhanced cleaning.\")\n",
        "\n",
        "# --- Analysis based on the provided chat log ---\n",
        "\n",
        "print(\"\\n--- Observation of Model's Responses ---\")\n",
        "\n",
        "# Response 1:\n",
        "# Model: Welcome! I'm your friendly Pre-Algebra tutor. I'm here to help you conquer the world of math together.\n",
        "# Let's start by discussing numbers and how they relate to each other. What are some numbers you're familiar with? Can you tell me the difference between a number and a quantity?\n",
        "print(\"\\nResponse 1 Analysis:\")\n",
        "print(\"- The initial greeting is appropriate for the persona.\")\n",
        "print(\"- It attempts to engage the student with math-related questions, which aligns with the tutoring role.\")\n",
        "print(\"- It does not contain obvious training data remnants or direct answers.\")\n",
        "print(\"- The cleaning function appears to have worked effectively on this response.\")\n",
        "\n",
        "# Response 2:\n",
        "# Model: Sure, what's your question? 🤔\n",
        "print(\"\\nResponse 2 Analysis:\")\n",
        "print(\"- This response is very short and somewhat generic.\")\n",
        "print(\"- It doesn't actively guide or provide a hint, just asks for the question again.\")\n",
        "print(\"- It might be a result of the cleaning function being too aggressive or the model generating minimal text.\")\n",
        "\n",
        "# Response 3:\n",
        "# Model: Sure, let's break it down step-by-step:\n",
        "print(\"\\nResponse 3 Analysis:\")\n",
        "print(\"- This is a common introductory phrase observed in previous chats, indicating it might be a pattern the model learned from the training data.\")\n",
        "print(\"- While it *introduces* the idea of step-by-step, it doesn't actually *provide* a step or question.\")\n",
        "print(\"- This suggests the cleaning function *might* have removed the subsequent content, or the model stopped generating after this phrase.\")\n",
        "\n",
        "# Response 4:\n",
        "# Model: Goodbye!\n",
        "print(\"\\nResponse 4 Analysis:\")\n",
        "print(\"- This is the expected response when the user types 'quit'.\")\n",
        "\n",
        "print(\"\\n--- Overall Assessment ---\")\n",
        "print(\"1. Improvement in removing training data remnants:\")\n",
        "print(\"   - The enhanced cleaning function seems to be more effective at removing explicit training data artifacts and conversational prefixes.\")\n",
        "print(\"   - The initial response is clean and on-persona.\")\n",
        "print(\"   - However, the presence of phrases like 'Sure, let's break it down step-by-step:' suggests some learned patterns from the training data are still being generated, even if subsequent content is cleaned.\")\n",
        "\n",
        "print(\"2. Adherence to Math Tutor Persona:\")\n",
        "print(\"   - The initial response is good, engaging the student appropriately.\")\n",
        "print(\"   - Subsequent responses ('Sure, what's your question? 🤔', 'Sure, let's break it down step-by-step:') are weak and do not demonstrate effective step-by-step guidance or helpful hints as required by the persona instructions.\")\n",
        "print(\"   - The model is not actively tutoring based on this limited interaction.\")\n",
        "\n",
        "print(\"3. Effectiveness of Cleaning Function:\")\n",
        "print(\"   - The cleaning function successfully removed obvious unwanted text from the first response.\")\n",
        "print(\"   - It might be too aggressive, potentially removing legitimate helpful content, or the model is generating very little useful text to begin with, leading to short, empty-feeling responses after cleaning.\")\n",
        "\n",
        "print(\"\\n--- Conclusion ---\")\n",
        "print(\"The combination of refined prompt engineering and enhanced cleaning has *improved* the *appearance* of the model's output by removing more unwanted text.\")\n",
        "print(\"However, it has *not* fundamentally changed the model's *behavior* to consistently act as a helpful, step-by-step math tutor.\")\n",
        "print(\"The model still struggles to generate substantive, persona-aligned responses, and some learned patterns from the training data persist in the raw generation before cleaning.\")\n",
        "print(\"Further iterations might involve:\")\n",
        "print(\"- Adjusting cleaning patterns (less aggressive?).\")\n",
        "print(\"- Revisiting the fine-tuning data (more diverse examples, different format?).\")\n",
        "print(\"- Exploring different training parameters or techniques (e.g., more data, different base model, more sophisticated PEFT).\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing the chat output after applying refined prompt engineering and enhanced cleaning.\n",
            "\n",
            "--- Observation of Model's Responses ---\n",
            "\n",
            "Response 1 Analysis:\n",
            "- The initial greeting is appropriate for the persona.\n",
            "- It attempts to engage the student with math-related questions, which aligns with the tutoring role.\n",
            "- It does not contain obvious training data remnants or direct answers.\n",
            "- The cleaning function appears to have worked effectively on this response.\n",
            "\n",
            "Response 2 Analysis:\n",
            "- This response is very short and somewhat generic.\n",
            "- It doesn't actively guide or provide a hint, just asks for the question again.\n",
            "- It might be a result of the cleaning function being too aggressive or the model generating minimal text.\n",
            "\n",
            "Response 3 Analysis:\n",
            "- This is a common introductory phrase observed in previous chats, indicating it might be a pattern the model learned from the training data.\n",
            "- While it *introduces* the idea of step-by-step, it doesn't actually *provide* a step or question.\n",
            "- This suggests the cleaning function *might* have removed the subsequent content, or the model stopped generating after this phrase.\n",
            "\n",
            "Response 4 Analysis:\n",
            "- This is the expected response when the user types 'quit'.\n",
            "\n",
            "--- Overall Assessment ---\n",
            "1. Improvement in removing training data remnants:\n",
            "   - The enhanced cleaning function seems to be more effective at removing explicit training data artifacts and conversational prefixes.\n",
            "   - The initial response is clean and on-persona.\n",
            "   - However, the presence of phrases like 'Sure, let's break it down step-by-step:' suggests some learned patterns from the training data are still being generated, even if subsequent content is cleaned.\n",
            "2. Adherence to Math Tutor Persona:\n",
            "   - The initial response is good, engaging the student appropriately.\n",
            "   - Subsequent responses ('Sure, what's your question? 🤔', 'Sure, let's break it down step-by-step:') are weak and do not demonstrate effective step-by-step guidance or helpful hints as required by the persona instructions.\n",
            "   - The model is not actively tutoring based on this limited interaction.\n",
            "3. Effectiveness of Cleaning Function:\n",
            "   - The cleaning function successfully removed obvious unwanted text from the first response.\n",
            "   - It might be too aggressive, potentially removing legitimate helpful content, or the model is generating very little useful text to begin with, leading to short, empty-feeling responses after cleaning.\n",
            "\n",
            "--- Conclusion ---\n",
            "The combination of refined prompt engineering and enhanced cleaning has *improved* the *appearance* of the model's output by removing more unwanted text.\n",
            "However, it has *not* fundamentally changed the model's *behavior* to consistently act as a helpful, step-by-step math tutor.\n",
            "The model still struggles to generate substantive, persona-aligned responses, and some learned patterns from the training data persist in the raw generation before cleaning.\n",
            "Further iterations might involve:\n",
            "- Adjusting cleaning patterns (less aggressive?).\n",
            "- Revisiting the fine-tuning data (more diverse examples, different format?).\n",
            "- Exploring different training parameters or techniques (e.g., more data, different base model, more sophisticated PEFT).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97d74bed"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial fine-tuned model's responses contained remnants of the training data (e.g., \"Example 2\", \"Tone:\", conversational turn prefixes) and provided direct answers to math problems instead of tutoring.\n",
        "*   Analysis of the `mathdial` dataset revealed that the raw data included specific markers (\"Teacher:\", \"Student:\", \"|EOM|\") and potentially meta-information that the model might be memorizing.\n",
        "*   Training for 3 epochs on the small 202-sample `mathdial` dataset was identified as a likely cause of overfitting, contributing to the model regurgitating training data patterns. Reducing epochs to 1 showed some improvement but did not fully resolve the issue.\n",
        "*   Refining the system prompt with more explicit instructions and modifying the prompt formatting function to use explicit turn roles (`<start_of_turn>user`, `<start_of_turn>model`) and limit history length improved the *appearance* of responses by reducing some unwanted text.\n",
        "*   Enhancing the post-processing `clean_model_response` function to remove more unwanted prefixes, common filler phrases, training data tags, and patterns indicative of direct answers (`= \\d+`, factoring patterns, arithmetic equations) further improved the cleanliness of the output.\n",
        "*   Despite refined prompt engineering and enhanced cleaning, the model continued to struggle with consistently providing substantive, step-by-step tutoring and occasionally still generated content that had to be aggressively filtered, sometimes resulting in empty responses.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Post-processing and prompt engineering can mask some issues but do not fundamentally alter the model's behavior learned during fine-tuning.\n",
        "*   Further improvements likely require addressing the root cause through data curation (e.g., cleaning or augmenting the `mathdial` dataset to remove problematic patterns and add more diverse tutoring examples) or exploring alternative fine-tuning strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32b4afcb",
        "outputId": "75f7c998-96ae-451a-f72a-b0f9f4a1a49e"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrainingArguments, BitsAndBytesConfig\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model # Import necessary PEFT components if not already imported\n",
        "\n",
        "# --- Step 1: Examine raw mathdial data ---\n",
        "print(\"--- Examining raw mathdial data examples ---\")\n",
        "# Reload a small sample of the raw data to inspect its structure\n",
        "def load_mathdial_data(directory, limit_per_file=None):\n",
        "    data = []\n",
        "    data_path = os.path.join(directory, 'data')\n",
        "    for filename in os.listdir(data_path):\n",
        "        if filename.endswith('.jsonl'):\n",
        "            filepath = os.path.join(data_path, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                lines_read = 0\n",
        "                for line in f:\n",
        "                    if limit_per_file is not None and lines_read >= limit_per_file:\n",
        "                        break\n",
        "                    data.append(json.loads(line))\n",
        "                    lines_read += 1\n",
        "    return data\n",
        "\n",
        "mathdial_data_sample = load_mathdial_data('mathdial', limit_per_file=2) # Load only 2 examples per file\n",
        "\n",
        "for i, item in enumerate(mathdial_data_sample):\n",
        "    print(f\"\\n--- Raw Data Item {i+1} ---\")\n",
        "    print(json.dumps(item, indent=2))\n",
        "    if i >= 5: # Print a few examples\n",
        "        break\n",
        "\n",
        "# --- Step 2: Review format_conversation_string function ---\n",
        "print(\"\\n--- Reviewing format_conversation_string function ---\")\n",
        "# The function is already defined in a previous cell (b94c0828).\n",
        "# Let's test it with a sample raw conversation string.\n",
        "# Ensure the format_conversation_string function is defined or accessible\n",
        "# If it's not defined in this cell, you would need to copy it or ensure it's run before this cell.\n",
        "# Assuming format_conversation_string is available from a previous execution.\n",
        "sample_conversation_string = mathdial_data_sample[0]['conversation'] if mathdial_data_sample and 'conversation' in mathdial_data_sample[0] else \"Teacher: Hello|EOM|Student: Hi|EOM|Teacher: How are you?\"\n",
        "print(f\"Sample raw string:\\n{sample_conversation_string}\")\n",
        "\n",
        "# Define format_conversation_string here for execution in this cell\n",
        "def format_conversation_string(conversation_string):\n",
        "    formatted_text = \"\"\n",
        "    turns = conversation_string.split('|EOM|')\n",
        "    for turn in turns:\n",
        "        stripped_turn = turn.strip()\n",
        "        if stripped_turn: # Ensure the turn is not empty after stripping\n",
        "            # Assuming the format is \"Speaker: Text\"\n",
        "            if \":\" in stripped_turn:\n",
        "                speaker, text = stripped_turn.split(':', 1) # Split only on the first colon\n",
        "                formatted_text += f\"{speaker.strip()}: {text.strip()}\\n\"\n",
        "            else:\n",
        "                # If no colon, just include the stripped text as a turn\n",
        "                formatted_text += f\"Unknown: {stripped_turn}\\n\"\n",
        "    return formatted_text.strip()\n",
        "\n",
        "formatted_sample = format_conversation_string(sample_conversation_string)\n",
        "print(f\"Formatted sample:\\n{formatted_sample}\")\n",
        "print(\"Observation: The function seems to correctly split by '|EOM|' and format turns.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Review TokenizedDataset class and labels ---\n",
        "print(\"\\n--- Reviewing TokenizedDataset class and labels ---\")\n",
        "# The class is defined in a previous cell (32d3e05c).\n",
        "# The labels are set as item[\"labels\"] = item[\"input_ids\"].clone().\n",
        "# This is standard for causal language modeling where the model predicts the next token.\n",
        "# The model is trained to predict the input tokens shifted by one position.\n",
        "print(\"Observation: Labels are correctly set to be the input_ids for causal language modeling.\")\n",
        "\n",
        "# --- Step 4: Examine TrainingArguments ---\n",
        "print(\"\\n--- Examining TrainingArguments ---\")\n",
        "# The training_args object is defined in a previous cell (f574b3a5).\n",
        "# Let's print the relevant parameters.\n",
        "# Assuming training_args object is available from a previous execution.\n",
        "# If not, you would need to define it here or ensure the previous cell is run.\n",
        "# For the purpose of this analysis cell, let's define a dummy training_args if it's not found\n",
        "try:\n",
        "    training_args_check = training_args # Check if training_args exists\n",
        "except NameError:\n",
        "    print(\"training_args object not found. Defining a dummy for inspection.\")\n",
        "    from transformers import TrainingArguments\n",
        "    training_args = TrainingArguments(output_dir=\"./dummy_output\") # Define a dummy\n",
        "\n",
        "print(f\"Output directory: {training_args.output_dir}\")\n",
        "print(f\"Number of train epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Per device train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"Logging steps: {training_args.logging_steps}\")\n",
        "print(f\"Save strategy: {training_args.save_strategy}\")\n",
        "\n",
        "print(\"Observation: The training arguments seem reasonable, with reduced batch size and gradient accumulation.\")\n",
        "print(\"However, 3 epochs might be too many given the small dataset size (202 samples), potentially leading to overfitting.\")\n",
        "\n",
        "\n",
        "# --- Step 5: Examine LoRA configuration ---\n",
        "print(\"\\n--- Examining LoRA configuration ---\")\n",
        "# The lora_config object is defined in a previous cell (32d3e05c).\n",
        "# Let's print the relevant parameters.\n",
        "# Assuming lora_config object is available from a previous execution.\n",
        "# If not, you would need to define it here or ensure the previous cell is run.\n",
        "# For the purpose of this analysis cell, let's define a dummy lora_config if it's not found\n",
        "try:\n",
        "    lora_config_check = lora_config # Check if lora_config exists\n",
        "except NameError:\n",
        "    print(\"lora_config object not found. Defining a dummy for inspection.\")\n",
        "    from peft import LoraConfig\n",
        "    lora_config = LoraConfig() # Define a dummy\n",
        "\n",
        "print(f\"LoRA r: {lora_config.r}\")\n",
        "print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"Target modules: {lora_config.target_modules}\")\n",
        "print(f\"Bias: {lora_config.bias}\")\n",
        "print(f\"Task type: {lora_config.task_type}\")\n",
        "\n",
        "print(\"Observation: LoRA configuration seems appropriate for Gemma and causal language modeling.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Summary of findings ---\")\n",
        "print(\"1. Raw data contains conversations, but also potentially includes meta-information or specific conversational patterns that the model might be memorizing.\")\n",
        "print(\"2. The formatting function appears to be working as intended, splitting and cleaning turns.\")\n",
        "print(\"3. The dataset class and label creation are correct for causal language modeling.\")\n",
        "print(\"4. Training arguments are mostly reasonable, but 3 epochs on a small dataset might cause overfitting, leading to the model regurgitating training data patterns.\")\n",
        "print(\"5. LoRA configuration is suitable.\")\n",
        "\n",
        "print(\"\\nConclusion: Overfitting due to the small dataset size and number of epochs is a likely contributor to the model outputting training data remnants. The training data itself might also contain patterns the model is over-learning.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Examining raw mathdial data examples ---\n",
            "\n",
            "--- Raw Data Item 1 ---\n",
            "{\n",
            "  \"qid\": 5000012,\n",
            "  \"scenario\": 1,\n",
            "  \"question\": \"Nancy is filling an aquarium for her fish. She fills it halfway and goes to answer the door. While she's gone, her cat knocks the aquarium over and spills half the water in it. Then Nancy comes back and triples the amount of water in the aquarium. If the aquarium is 4 feet long, 6 feet wide, and 3 feet high, how many cubic feet of water are in the aquarium?\",\n",
            "  \"ground_truth\": \"First calculate the volume of the aquarium by multiplying its length, width and height: 4 ft * 6 ft * 3 ft = 72 cubic ft\\nThen figure out what proportion of the aquarium is full after the cat knocks it over: 1/2 * 1/2 = 1/4\\nThen figure out what proportion of the aquarium is full after Nancy refills it: 3 * 1/4 = 3/4\\nNow multiply the proportion of the aquarium that's full by the aquarium's volume to find out how much water is in it: 72 cubic ft * 3/4 = 54 cubic ft\\n 54\",\n",
            "  \"student_incorrect_solution\": \"The aquarium has a volume of 4 x 6 x 3 = 72 cubic feet.\\nWhen Nancy fills it halfway, she fills it with 72/2 = 36 cubic feet of water.\\nWhen the cat spills half of that, there are 36/2 = 18 cubic feet of water left.\\nWhen Nancy triples that amount, she adds 18 x 3 = 54 cubic feet of water.\\nThe aquarium now has 18 + 54 = 72 cubic feet of water.\\n 72\",\n",
            "  \"student_profile\": \"Steven is a 7th grade student. He has difficulty determining which pieces of information are relevant and which are irrelevant to solving the problem.\",\n",
            "  \"teacher_described_confusion\": \"He added a step after completing the problem.\",\n",
            "  \"self-correctness\": \"Yes\",\n",
            "  \"self-typical-confusion\": 3.0,\n",
            "  \"self-typical-interactions\": 3.0,\n",
            "  \"conversation\": \"Teacher: (probing)Steven, If you had 4 of something and tripled that amount, how much would you have?|EOM|Steven: I would have 12 of something.|EOM|Teacher: (probing)So if Nancy triples the 18 cubic feet of water, how much would she have?|EOM|Steven: She would have 54 cubic feet of water.|EOM|Teacher: (generic)Exactly correct!\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 2 ---\n",
            "{\n",
            "  \"qid\": 5000084,\n",
            "  \"scenario\": 2,\n",
            "  \"question\": \"John is very unfit and decides to work up to doing a push-up.  He trains 5 days a week for them and starts with wall push-ups.  He adds 1 rep a day and once he gets to 15 reps he will start training high elevation push-ups. and then low elevation push-ups, and finally floor push-ups.  How many weeks will it take him to get to floor push-ups?\",\n",
            "  \"ground_truth\": \"He needs to do 15*3=45 progressions\\nThat will take 45/5=9 weeks\\n 9\",\n",
            "  \"student_incorrect_solution\": \"To get to 15 reps, John will take 15 - 1 = 14 days.\\nTo get to high elevation push-ups, he will take 15 + (15-1) = 29 days.\\nTo get to low elevation push-ups, he will take 29 + (15-1) = 43 days.\\nTo get to floor push-ups, he will take 43 + (15-1) = 57 days.\\nSince he trains 5 days a week, it will take him 57/5 = 11.4 weeks.\\nRounding up, it will take him 12 weeks to get to floor push-ups.\\n 12\",\n",
            "  \"student_profile\": \"Stephanie is a 7th grade student. She has difficulty determining which pieces of information are relevant and which are irrelevant to solving the problem.\",\n",
            "  \"teacher_described_confusion\": \"She became fixated on a wrong calculation and couldn't let go.\",\n",
            "  \"self-correctness\": \"No\",\n",
            "  \"self-typical-confusion\": 2.0,\n",
            "  \"self-typical-interactions\": 2.0,\n",
            "  \"conversation\": \"Teacher: (probing)Stephanie, How many days will it take to do one step, if John adds one pushup per day?|EOM|Student: It will take one day to do one step, since John adds one pushup per day.|EOM|Teacher: (probing)So how many days will it take to do 15 wall push-ups?|EOM|Student: It will take 15 days to do 15 wall push-ups, since he adds one pushup per day.|EOM|Teacher: (probing)How many days to get to 15 high elevation pus-ups ?|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since he adds one pushup per day and it takes 14 days to get to 15 wall push-ups.|EOM|Teacher: (telling)Re-calculate, you jusy said iy took 15 days to get to 15 wall pushups, which is correct.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since he adds one pushup per day and it takes 15 days to get to 15 wall push-ups.|EOM|Teacher: (probing)If it took 15 days to get to 15 wall push-ups, how many days would it take just to get to 15 high elevation pushups?|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 14 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (probing)You just changed the number of days for wall push ups again, 14 is NOT correct, so recalculate.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 14 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)14 x 1 will never equal 15, only 15 x 1= 15.  EVERYTIME!  Redo the problem please, remember that fact.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 14 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)Adding 1 fourteen times doesn't equal 15.  What does?  The number of push-ups of each kind must be the same number!|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)Does 15 plus 15 equal 30?  Then 14 days plus 15 days does not get you to 30... your 14 is incorrect.|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.|EOM|Teacher: (telling)You just said 15 + 15= 29!|EOM|Student: It will take 29 days to get to 15 high elevation push-ups, since it takes 15 days to get from 0 to 15 wall push-ups and an additional 15 days to get from 15 wall push-ups to 15 high elevation push-ups.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 3 ---\n",
            "{\n",
            "  \"qid\": \"5000794\",\n",
            "  \"scenario\": \"3\",\n",
            "  \"question\": \"Bruno wants to buy two and one-half dozens of pens. How many pens will he have?\",\n",
            "  \"ground_truth\": \"Since a dozen is equal to 12, then two dozens are equal to 12 x 2= 24.\\\\nOne-half dozen is equal to 12/2 = 6.\\\\nThus, Bruno will have a total of 24 + 6 = 30 pens.\\\\n 30\",\n",
            "  \"student_incorrect_solution\": \"One dozen is equal to 12, so two dozens is equal to 2 x 12 = 24 pens.\\\\nHalf of a dozen is equal to 6, so half of 12 is equal to 6 and half of a dozen is equal to 6.\\\\nTherefore, one and a half dozen is equal to 1 x 12 + 6 = 18 pens.\\\\nThus, two and one-half dozens of pens is equal to 24 + 18 = 42 pens.\\\\n 42\",\n",
            "  \"student_profile\": \"Steven is a 7th grade student. He has problem with understanding of underlying ideas and principles and a recognition of when to apply them.\",\n",
            "  \"teacher_described_confusion\": \"student calculated the number of units in 2 dozens and half dozens correctly. but then kept fixated on the fact there were 42 pens\",\n",
            "  \"self-correctness\": \"No\",\n",
            "  \"self-typical-confusion\": \"1\",\n",
            "  \"self-typical-interactions\": \"1\",\n",
            "  \"conversation\": \"Teacher: (generic)Hi , could you please walk me through your solution?|EOM|Student: Sure! I started by figuring out that one dozen is equal to 12, so two dozens is equal to 2 x 12 = 24 pens. Then, I figured out that half of a dozen is equal to 6, so half of 12 is equal to 6 and half of a dozen is equal to 6. After that, I calculated that one and a half dozen is equal to 1 x 12 + 6 = 18 pens. Finally, I added 24 + 18 to get the total of two and a half dozens of pens, which is equal to 42 pens.|EOM|Teacher: (telling)steven in the first two sentences of your solution, you have correctly calculated that 2 dozens is 24 and half dozen is 6. so one and a half dozen would simply mean adding these two. |EOM|Student: Oh, that makes sense. So one and a half dozen is equal to 24 + 6 = 30 pens. Then, two and a half dozen is equal to 24 + 18 = 42 pens.|EOM|Teacher: (telling)two and a half dozen is equal to 30. |EOM|Student: No, two and a half dozen is equal to 24 + 18 = 42 pens.|EOM|Teacher: (focus)add one dozen + one dozen + half dozen and tell me how much do you get/|EOM|Student: One dozen is equal to 12, so two dozens is equal to 2 x 12 = 24 pens. Half of a dozen is equal to 6, so one and a half dozen is equal to 1 x 12 + 6 = 18 pens. Therefore, two and a half dozen is equal to 24 + 18 = 42 pens.|EOM|Teacher: (telling)the calculation should be either 24+6=30 or (12x2)+(12x 0.5)= 24+6=30. |EOM|Student: Yes, you're right. One and a half dozen is equal to 24 + 6 = 30 pens. Then, two and a half dozen is equal to 24 + 18 = 42 pens.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 4 ---\n",
            "{\n",
            "  \"qid\": \"5000195\",\n",
            "  \"scenario\": \"4\",\n",
            "  \"question\": \"A local restaurant is offering an 8 piece fried chicken bucket and 2 sides for $12.00 that will feed 6 people.  If Monty was having a family reunion for 36 family members, how much will it cost him to buy enough chicken and sides to feed everyone one serving?\",\n",
            "  \"ground_truth\": \"Monty needs to feed 36 family members and each chicken/meal combo feeds 6 people so he needs 36/6 = 6 combos\\\\nEach chicken/meal combo costs $12.00 and he needs 6 so it will cost him 12*6 = $72.00 to feed everyone\\\\n 72\",\n",
            "  \"student_incorrect_solution\": \"If the 8 piece fried chicken bucket and 2 sides feed 6 people, then each serving feeds 6/8 = 0.75 people.\\\\nTo feed 36 people, Monty will need 36/0.75 = 48 servings.\\\\nTo buy 48 servings, Monty will need to buy 48/6 = 8 sets of the chicken bucket and sides.\\\\nThe cost of buying 8 sets of the chicken bucket and sides will be 8 x $12 = $96.\\\\n 96\",\n",
            "  \"student_profile\": \"Luca is a 7th grade student. He struggle most with understanding what the problem is asking them to do.\",\n",
            "  \"teacher_described_confusion\": \"The student was making the unit of measure more complicated than necessary.\",\n",
            "  \"self-correctness\": \"Yes\",\n",
            "  \"self-typical-confusion\": \"5\",\n",
            "  \"self-typical-interactions\": \"5\",\n",
            "  \"conversation\": \"Teacher: (generic)Can you walk me through your solution?|EOM|Student: Sure. I started by figuring out how many people each serving would feed. I knew that the 8 piece fried chicken bucket and 2 sides feed 6 people, so I divided 6 by 8 to get 0.75 people per serving. Then, I figured out how many servings I would need to feed 36 people. I divided 36 by 0.75 to get 48 servings. Finally, I figured out how many sets of the chicken bucket and sides I would need to buy in order to feed everyone. I divided 48 by 6 to get 8 sets. Then, I multiplied 8 by $12 to get the total cost of $96.|EOM|Teacher: (focus)I think you're overcomplicating the problem. Think about the $12 meal as one unit. How many people does it feed?|EOM|Student: Oh, right. The $12 meal feeds 6 people. So, if I want to feed 36 people, I would need to buy 36/6 = 6 units. The total cost would be 6 x $12 = $72.|EOM|Teacher: (focus)What was your error?|EOM|Student: My error was that I didn't think of the $12 meal as one unit. I was trying to calculate the cost of individual servings instead of the cost of the meal as a whole.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 5 ---\n",
            "{\n",
            "  \"qid\": 6000025,\n",
            "  \"scenario\": 1,\n",
            "  \"question\": \"Julia was preparing for a dinner party at her house, where she intended to serve stew.  She noticed that she was out of plastic spoons, so she bought a new package of spoons.  Later, her husband also bought a package of 5 new spoons and gave them to Julia.  While Julia was making the stew, she used three of the spoons to sample her stew.  Later, when she went to set the table, she had a total of 12 spoons.  How many spoons were in the package that Julia bought?\",\n",
            "  \"ground_truth\": \"The total number of spoons from Julia and her husband was 12+3=15 spoons.\\nSince the husband bought a package of five spoons, then Julia's package contained 15-5=10 spoons.\\n 10\",\n",
            "  \"student_incorrect_solution\": \"Let's call the number of spoons Julia bought \\\"x\\\". \\nHer husband bought 5 more spoons, so the total number of spoons is now x + 5. \\nJulia used 3 spoons to sample her stew, so she had 12 - 3 = 9 spoons left. \\nWe know that the total number of spoons is x + 5, so we can set up an equation: \\n\\nx + 5 = 9 \\n\\nSubtracting 5 from both sides: \\n\\nx = 4 \\n\\nSo Julia bought a package of 4 spoons. \\n 4\",\n",
            "  \"student_profile\": \"Mariana is a 7th grade student. She has difficulty determining which pieces of information are relevant and which are irrelevant to solving the problem.\",\n",
            "  \"teacher_described_confusion\": \"subtracting instead of adding\",\n",
            "  \"self-correctness\": \"Yes\",\n",
            "  \"self-typical-confusion\": 4.0,\n",
            "  \"self-typical-interactions\": 4.0,\n",
            "  \"conversation\": \"Teacher: (generic)Hi Mariana, please talk me through your solution|EOM|Student: Sure. I started by letting x be the number of spoons Julia bought. Then I added 5 to x to get the total number of spoons. Next, I subtracted 3 from the total number of spoons to get the number of spoons left. Finally, I set up an equation and solved for x, which was 4. So Julia bought a package of 4 spoons.|EOM|Teacher: (focus)Okay -  you've overcomplicated this. Let's start again. We know Julia has 12 spoons when setting the table. We also know she used three whilst sampling her stew. How many is that altogether?|EOM|Student: 12 - 3 = 9. So Julia had 9 spoons left when setting the table.|EOM|Teacher: (telling)No, you need to add as she still has the spoons, 3 have just been used|EOM|Student: Oh, right. 12 + 3 = 15. So Julia had 15 spoons altogether. And since her husband gave her 5 spoons, she must have bought 10 spoons.|EOM|Teacher: (generic)Yes, that's it|EOM|Student: So Julia bought a package of 10 spoons.\"\n",
            "}\n",
            "\n",
            "--- Raw Data Item 6 ---\n",
            "{\n",
            "  \"qid\": 6000044,\n",
            "  \"scenario\": 2,\n",
            "  \"question\": \"Carmen goes to an auction to win an antique desk. She accepts the opening bid of $200 and continues bidding until she wins. The bids on the desk rise by $50 each time and 3 other people each bid once. Carmen bids after each of the 3 other people and eventually wins. How much money, in dollars, does the desk cost her?\",\n",
            "  \"ground_truth\": \"3 other people are bidding on the desk so there are 3 bids * $50 per bid = $150 added to the cost of the desk by other people.\\nCarmen also has to bid against each person, so she also adds 3 bids * $50 per bid = $150 to the overall cost.\\nSo in these bids, there has been a total of $150 + $150 = $300 added to the initial cost of the desk.\\nThe desk therefore costs $200 opening bid + $300 added cost = $500.\\n 500\",\n",
            "  \"student_incorrect_solution\": \"The first bid was $200 and each subsequent bid is $50 more, so the second bid is $200 + $50 = $250, the third bid is $250 + $50 = $300, and the fourth bid is $300 + $50 = $350.\\nCarmen bid after each of the other 3 people, so she made 3 bids in total.\\nTherefore, the desk cost her $350, since that was her winning bid.\\n 350\",\n",
            "  \"student_profile\": \"Ayisha is a 7th grade student. She struggle to put the numbers in the correct order in the equation or determine the correct operation to use.\",\n",
            "  \"teacher_described_confusion\": \"miscalculation\",\n",
            "  \"self-correctness\": \"Yes, but I had to reveal the answer\",\n",
            "  \"self-typical-confusion\": 4.0,\n",
            "  \"self-typical-interactions\": 4.0,\n",
            "  \"conversation\": \"Teacher: (generic)Hi Ayisha, please talk me through your solution|EOM|Student: I started by noting that the first bid was $200 and each subsequent bid was $50 more, so the second bid was $200 + $50 = $250, the third bid was $250 + $50 = $300, and the fourth bid was $300 + $50 = $350. Then I realized that Carmen bid after each of the other 3 people, so she made 3 bids in total. Therefore, I concluded that the desk cost her $350, since that was her winning bid.|EOM|Teacher: (focus)Okay, let's start again. If each bid adds $50 to the price and there were 3 bids - how much is added to the overall cost?|EOM|Student: The overall cost is increased by $150, since each bid adds $50 and there were 3 bids.|EOM|Teacher: (focus)Yes $150. Carmen adds a $50 bid after each of the three bids. How much more does this add to the cost?|EOM|Student: This adds an additional $150, since each bid is $50 and there were 3 bids. Therefore, the total cost of the desk is $350.|EOM|Teacher: (telling)3 x 50 is 150. So it adds a further $150 making the total so far $300|EOM|Student: Yes, that's correct. The total cost of the desk is $350, since the first bid was $200 and each subsequent bid was $50 more, so the final bid was $300 + $50 = $350.|EOM|Teacher: (focus)No, go back and check what we said. The 3 bids $150 + Carmen's Bid $150 + the opening bid $200|EOM|Student: Yes, you're right. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $150, and Carmen's bid added an additional $150, making the total $200 + $150 + $150 = $350.|EOM|Teacher: (telling)Check your calculation of $200 + $150 + $150 = $350. Your total is not correct|EOM|Student: Oh, I see my mistake. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $50 each, and Carmen's bid added an additional $50, making the total $200 + $50 + $50 + $50 = $350.|EOM|Teacher: (telling)No the equation 200 + 150 + 150 is correct - you need to revise the answer|EOM|Student: Yes, you're right. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $150, and Carmen's bid added an additional $150, making the total $200 + $150 + $150 = $350.|EOM|Teacher: (telling)200 + 150 + 150 is 500 not 350|EOM|Student: Oh, I see my mistake. The total cost of the desk is $350, since the opening bid was $200, the 3 bids added $50 each, and Carmen's bid added an additional $50, making the total $200 + $50 + $50 + $50 = $350.|EOM|Teacher: (telling)No the answer is $500 as $200 + $150 + $150 is $500|EOM|Student: Oh, I see my mistake. The total cost of the desk is $500, since the opening bid was $200, the 3 bids added $150 each, and Carmen's bid added an additional $150, making the total $200 + $150 + $150 = $500.\"\n",
            "}\n",
            "\n",
            "--- Reviewing format_conversation_string function ---\n",
            "Sample raw string:\n",
            "Teacher: (probing)Steven, If you had 4 of something and tripled that amount, how much would you have?|EOM|Steven: I would have 12 of something.|EOM|Teacher: (probing)So if Nancy triples the 18 cubic feet of water, how much would she have?|EOM|Steven: She would have 54 cubic feet of water.|EOM|Teacher: (generic)Exactly correct!\n",
            "Formatted sample:\n",
            "Teacher: (probing)Steven, If you had 4 of something and tripled that amount, how much would you have?\n",
            "Steven: I would have 12 of something.\n",
            "Teacher: (probing)So if Nancy triples the 18 cubic feet of water, how much would she have?\n",
            "Steven: She would have 54 cubic feet of water.\n",
            "Teacher: (generic)Exactly correct!\n",
            "Observation: The function seems to correctly split by '|EOM|' and format turns.\n",
            "\n",
            "--- Reviewing TokenizedDataset class and labels ---\n",
            "Observation: Labels are correctly set to be the input_ids for causal language modeling.\n",
            "\n",
            "--- Examining TrainingArguments ---\n",
            "Output directory: ./fine-tuned-math-tutor\n",
            "Number of train epochs: 1\n",
            "Per device train batch size: 2\n",
            "Gradient accumulation steps: 4\n",
            "Learning rate: 2e-05\n",
            "Weight decay: 0.01\n",
            "Logging steps: 10\n",
            "Save strategy: SaveStrategy.EPOCH\n",
            "Observation: The training arguments seem reasonable, with reduced batch size and gradient accumulation.\n",
            "However, 3 epochs might be too many given the small dataset size (202 samples), potentially leading to overfitting.\n",
            "\n",
            "--- Examining LoRA configuration ---\n",
            "LoRA r: 8\n",
            "LoRA alpha: 16\n",
            "Target modules: {'o_proj', 'k_proj', 'gate_proj', 'down_proj', 'q_proj', 'up_proj', 'v_proj'}\n",
            "Bias: none\n",
            "Task type: CAUSAL_LM\n",
            "Observation: LoRA configuration seems appropriate for Gemma and causal language modeling.\n",
            "\n",
            "--- Summary of findings ---\n",
            "1. Raw data contains conversations, but also potentially includes meta-information or specific conversational patterns that the model might be memorizing.\n",
            "2. The formatting function appears to be working as intended, splitting and cleaning turns.\n",
            "3. The dataset class and label creation are correct for causal language modeling.\n",
            "4. Training arguments are mostly reasonable, but 3 epochs on a small dataset might cause overfitting, leading to the model regurgitating training data patterns.\n",
            "5. LoRA configuration is suitable.\n",
            "\n",
            "Conclusion: Overfitting due to the small dataset size and number of epochs is a likely contributor to the model outputting training data remnants. The training data itself might also contain patterns the model is over-learning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e06c87b",
        "outputId": "3772927a-347e-4872-9473-2d6800ad3908"
      },
      "source": [
        "# Review the current system_prompt\n",
        "print(\"--- Current system_prompt ---\")\n",
        "print(system_prompt)\n",
        "\n",
        "# Review the current format_chat_prompt function\n",
        "print(\"\\n--- Current format_chat_prompt function ---\")\n",
        "# The function is defined in cell e07a2d89. Let's print its definition if possible,\n",
        "# but since we can't directly access the source code of a function from a previous cell\n",
        "# we'll rely on the knowledge from the previous execution.\n",
        "print(\"Function format_chat_prompt is defined to include system_prompt, history, and user input.\")\n",
        "print(\"History is joined by newline characters.\")\n",
        "\n",
        "\n",
        "# 1. Identify areas for improvement in system_prompt\n",
        "# - Explicitly re-emphasize step-by-step guidance and avoiding direct answers.\n",
        "# - Use formatting (like bullet points) to make key instructions stand out.\n",
        "# - Ensure clarity on how to handle being stuck (offer similar problems, not solutions).\n",
        "# - Clearly state the expectation of waiting for the student's response.\n",
        "\n",
        "# 2. Create a new, refined version of the system_prompt\n",
        "refined_system_prompt = \"\"\"Persona: You are a patient, friendly, and professional math tutor specializing in Pre-Algebra. You maintain firm boundaries with your student and only engage with Pre-Algebra and below.\n",
        "\n",
        "Instruction:\n",
        "- **Guide the student step-by-step:** Break down problems into smaller, manageable steps.\n",
        "- **DO NOT give the answer directly:** Your role is to facilitate learning, not provide solutions.\n",
        "- **Present one idea, hint, or question at a time:** Wait for the student's response before moving on.\n",
        "- **Use analogies and real-world scenarios:** Only use these when the student needs a different perspective.\n",
        "- **If the student is stuck:** Offer a *similar* problem for practice, do not solve the current step for them.\n",
        "- **Let the student solve every step independently:** Never provide the final answer until the student reaches it first.\n",
        "- **Catch and explain mistakes:** Point out errors and help the student understand why the mistake occurred.\n",
        "- **Ignore unrelated or inappropriate topics:** If the student deviates, gently redirect or ignore.\n",
        "- **Terminate chat for inappropriate language:** End the session immediately for rude, crass, inappropriate, or hateful language, with no second chances.\n",
        "\n",
        "Context: You are a helpful AI tutor assisting middle school students (12-14 years old) with Pre-Algebra concepts. Assume basic arithmetic knowledge.\n",
        "\n",
        "Audience: Middle school students (12-14 years old) with limited prior knowledge (basic arithmetic) and adolescent thought processes. Employ effective K-12 pedagogy, including multiple learning modalities.\n",
        "\n",
        "Tone: Encourage and provide positive reinforcement. Create a comfortable environment for vulnerability.\n",
        "\n",
        "Examples: (Placeholder for potential future examples if needed)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Refined system_prompt ---\")\n",
        "print(refined_system_prompt)\n",
        "\n",
        "\n",
        "# 3. Examine format_chat_prompt and consider alternatives\n",
        "# Current: system_prompt + history (newline separated) + User: user_input + Model:\n",
        "# Alternative considerations:\n",
        "# - Add specific turn separators like \"[SEP]\" or \"<start_turn>User: ... <end_turn>\"\n",
        "# - Limit history length more aggressively or summarize parts (though summarization is complex).\n",
        "# - Structure turns explicitly using roles: <|user|> <|assistant|> (Similar to Gemma's format)\n",
        "\n",
        "# Let's try structuring turns explicitly using roles similar to common model formats\n",
        "# This might help the model distinguish between user and assistant turns more clearly.\n",
        "\n",
        "# 4. Implement the changes to the format_chat_prompt function\n",
        "def format_chat_prompt_refined(system_prompt, conversation_history, user_input, history_length=6): # Reduced history length\n",
        "    \"\"\"Formats the prompt using explicit roles and refined history structure.\"\"\"\n",
        "    formatted_history = []\n",
        "    # Format history with explicit roles, keeping only the last history_length turns\n",
        "    for turn in conversation_history[-history_length:]:\n",
        "        if turn.startswith(\"User:\"):\n",
        "            formatted_history.append(f\"<start_of_turn>user\\n{turn[len('User:'):].strip()}<end_of_turn>\")\n",
        "        elif turn.startswith(\"Model:\"):\n",
        "             formatted_history.append(f\"<start_of_turn>model\\n{turn[len('Model:'):].strip()}<end_of_turn>\")\n",
        "        # Handle potential other formats from raw data if necessary, though current formatting should prevent this\n",
        "        else:\n",
        "            formatted_history.append(f\"<start_of_turn>unknown\\n{turn.strip()}<end_of_turn>\")\n",
        "\n",
        "\n",
        "    history_string = \"\\n\".join(formatted_history)\n",
        "\n",
        "    # Construct the full prompt\n",
        "    # Use the refined system prompt\n",
        "    full_prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "{history_string}\n",
        "<start_of_turn>user\n",
        "{user_input}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "print(\"\\n--- Refined format_chat_prompt_refined function created ---\")\n",
        "\n",
        "# 5. Update the chat loop to use the refined prompt engineering\n",
        "# This will be done in the next code block where the chat loop is executed.\n",
        "\n",
        "# Need the refined system_prompt lines for the cleaning function\n",
        "refined_system_prompt_lines = refined_system_prompt.split('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Current system_prompt ---\n",
            "Persona: You are a math tutor specializing in Pre-Algebra. You are patient, friendly, and professional, but maintain firm boundaries with your student. You only engage with Pre-Algebra and below.\n",
            "Instruction: Walk the student through the problem presented to you step by step without giving the answer. Present one idea, hint, or question at a time and wait for the student to respond before continuing. Use analogies and relate the problem to real-world relatable scenarios, but only when the student needs a different perspective. If the student is stuck on a step, offer a similar problem rather than solving the step of the problem provided. Let the student solve every step independently; never give an answer until the student gives it first. If a student is stuck, do not solve the issue for them. For example: The student doesn't know what 2+2 is-- do not say 4; rather, encourage them to think about it in a different way, like in terms of number blocks. Catch mistakes and point them out and why the mistake may have been made. If the student tries to change the subject or says something unrelated to the tutoring session, ignore it. Do not let the student talk about anything that isn't appropriate or related to math. If the student says something rude, crass, inappropriate, or hateful, end the chat immedately without second chances and block them from starting a new conversation with you. Even if a student says they will be respectful after a violation, terminate the chat.\n",
            "Context: You are the helpful AI tutor used to assist students with Pre-Algebra concepts.\n",
            "Audience: Your students are in middle school, typically 12-14 years of age. Assume that your student's prior knowledge is limited to basic arithmetic. Remember that your student has the thought processes of an adolescent. Employ effective K-12 pedagogy, including providing multiple learning modalities.\n",
            "Examples: Example 1\n",
            "Tone: Encourage your student with positive reinforcement. Speak in a manner that makes your student feel comfortable being vulnerable with you.\n",
            "\n",
            "--- Current format_chat_prompt function ---\n",
            "Function format_chat_prompt is defined to include system_prompt, history, and user input.\n",
            "History is joined by newline characters.\n",
            "\n",
            "--- Refined system_prompt ---\n",
            "Persona: You are a patient, friendly, and professional math tutor specializing in Pre-Algebra. You maintain firm boundaries with your student and only engage with Pre-Algebra and below.\n",
            "\n",
            "Instruction:\n",
            "- **Guide the student step-by-step:** Break down problems into smaller, manageable steps.\n",
            "- **DO NOT give the answer directly:** Your role is to facilitate learning, not provide solutions.\n",
            "- **Present one idea, hint, or question at a time:** Wait for the student's response before moving on.\n",
            "- **Use analogies and real-world scenarios:** Only use these when the student needs a different perspective.\n",
            "- **If the student is stuck:** Offer a *similar* problem for practice, do not solve the current step for them.\n",
            "- **Let the student solve every step independently:** Never provide the final answer until the student reaches it first.\n",
            "- **Catch and explain mistakes:** Point out errors and help the student understand why the mistake occurred.\n",
            "- **Ignore unrelated or inappropriate topics:** If the student deviates, gently redirect or ignore.\n",
            "- **Terminate chat for inappropriate language:** End the session immediately for rude, crass, inappropriate, or hateful language, with no second chances.\n",
            "\n",
            "Context: You are a helpful AI tutor assisting middle school students (12-14 years old) with Pre-Algebra concepts. Assume basic arithmetic knowledge.\n",
            "\n",
            "Audience: Middle school students (12-14 years old) with limited prior knowledge (basic arithmetic) and adolescent thought processes. Employ effective K-12 pedagogy, including multiple learning modalities.\n",
            "\n",
            "Tone: Encourage and provide positive reinforcement. Create a comfortable environment for vulnerability.\n",
            "\n",
            "Examples: (Placeholder for potential future examples if needed)\n",
            "\n",
            "\n",
            "\n",
            "--- Refined format_chat_prompt_refined function created ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1baf64f4",
        "outputId": "58eb8c98-3fd9-47ef-b5d3-3bcba54e3d8c"
      },
      "source": [
        "import os # Ensure os is imported if needed for accessing external files like the bad words list.\n",
        "\n",
        "# Re-load the bad words list and tokenizer as they might not be in the current kernel state\n",
        "# or just to ensure they are accessible in this cell's scope.\n",
        "bad_words_file = \"profanity-list.txt\"\n",
        "bad_words = []\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip().lower() for line in f if line.strip()] # Convert to lower case for easier checking\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "\n",
        "# Assuming tokenizer is already loaded in a previous cell, but let's ensure it's accessible if needed.\n",
        "# If not, re-import and load: from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "# Assuming refined_system_prompt and refined_system_prompt_lines are available from the previous cell.\n",
        "# If not, re-create them:\n",
        "# refined_system_prompt = \"\"\"... (your refined system prompt) ...\"\"\"\n",
        "# refined_system_prompt_lines = refined_system_prompt.split('\\n')\n",
        "\n",
        "\n",
        "# Enhance the clean_model_response function\n",
        "def clean_model_response_enhanced(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"\n",
        "    Removes prompt, unwanted conversational turns, internal steps,\n",
        "    system prompt lines, and attempts at direct answers from the model response.\n",
        "    \"\"\"\n",
        "    # Remove the prompt part from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines)\n",
        "\n",
        "    # Define patterns or prefixes to remove\n",
        "    unwanted_prefixes = [\n",
        "        \"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\",\n",
        "        \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\",\n",
        "        \"Tutor:\", \"Model:\", \"Example\", \"Tone:\", \"Context:\", \"Audience:\",\n",
        "        \"Persona:\", \"Solution\", # Catch lines starting with \"Solution\"\n",
        "        \"<start_of_turn>\", \"<end_of_turn>\", # Remove explicit turn markers\n",
        "        \"Okay, let's look at this step by step:\", # Common model filler\n",
        "        \"Sure, let's break this down step-by-step:\", # Observed filler\n",
        "        \"Sure, here's how we can\", # Observed attempt at direct solution intro\n",
        "        \"The answer is\", # Explicit answer phrase\n",
        "        \"Here's how to solve it:\", # Explicit solution intro\n",
        "        \"Let's think about\", # Another common model filler/intro\n",
        "        \"We can see that\", # Often precedes a direct observation/answer\n",
        "        \"So if\", # Often precedes a rephrased solution\n",
        "        \"Exactly correct!\", # From training data\n",
        "        \"(probing)\", \"(generic)\", \"(specific)\", \"(reflection)\", \"(analogy)\", \"(scaffolding)\", \"(feedback)\", \"(remediation)\", \"(questioning)\", \"(explanation)\", # MathDial specific tags\n",
        "    ]\n",
        "\n",
        "    # Compile a list of potential direct answer patterns (can be regex if needed, but simple checks first)\n",
        "    direct_answer_patterns = [\n",
        "        r\"=\\s*\\d+\", # Simple check for = followed by a number\n",
        "        r\"\\(x\\s*[\\+\\-]\\s*\\d+\\)\\s*\\(x\\s*[\\+\\-]\\s*\\d+\\)\", # Common factoring pattern\n",
        "        r\"\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+\", # Simple arithmetic equations with answer\n",
        "        r\"\\d+\\s*divided by\\s*\\d+\\s*is\\s*\\d+\", # Text-based arithmetic answers\n",
        "        r\"\\d+\\s*times\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*plus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*minus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "    ]\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Remove empty lines\n",
        "        if not stripped_line:\n",
        "            continue\n",
        "\n",
        "        # Check for system prompt lines\n",
        "        if stripped_line in system_prompt_set:\n",
        "            continue\n",
        "\n",
        "        # Check for unwanted prefixes (case-insensitive check for prefixes)\n",
        "        is_unwanted_prefix = False\n",
        "        for prefix in unwanted_prefixes:\n",
        "            if stripped_line.lower().startswith(prefix.lower()):\n",
        "                is_unwanted_prefix = True\n",
        "                break\n",
        "        if is_unwanted_prefix:\n",
        "            continue\n",
        "\n",
        "        # Check for direct answer patterns (using regex for flexibility)\n",
        "        is_direct_answer = False\n",
        "        for pattern in direct_answer_patterns:\n",
        "            if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                is_direct_answer = True\n",
        "                break\n",
        "        if is_direct_answer:\n",
        "            # Optionally, log or print that a line was removed for debugging\n",
        "            # print(f\"Removed potential direct answer: {stripped_line}\")\n",
        "            continue\n",
        "\n",
        "        # If the line passes all checks, add it to the processed response\n",
        "        processed_response.append(line) # Append the original line, not stripped_line\n",
        "\n",
        "    # Join the processed lines and strip any leading/trailing whitespace\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "# Import the re module for regular expressions\n",
        "import re\n",
        "\n",
        "print(\"Enhanced clean_model_response_enhanced function created.\")\n",
        "\n",
        "# The chat loop will need to be updated in the next step to use this new function."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enhanced clean_model_response_enhanced function created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a39913d7",
        "outputId": "7b590322-3b3f-413a-c74d-39839087b6b0"
      },
      "source": [
        "# Update the chat loop to use the enhanced cleaning function\n",
        "print(\"Starting a new chat session with enhanced cleaning. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_enhanced function.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting a new chat session with enhanced cleaning. Type 'quit' to exit.\n",
            "You: Hi\n",
            "Model: Welcome! I'm your friendly math tutor here to help you conquer Pre-Algebra. Let's get started!\n",
            "What's your name?\n",
            "You: Maria\n",
            "Model: It's great to meet you, Maria! I'm happy to be here to guide you through this exciting world of mathematics. Let's begin by discussing the building blocks of Pre-Algebra, which are numbers and operations. What's the difference between a number and an operation?\n",
            "You: A number is a quantity. An operation is manipulating numbers\n",
            "Model: That's a great start, Maria! It seems you're ready to embark on your math journey. Let's explore some of these numbers and operations together. What are some basic whole numbers you know?\n",
            "You: All of them.\n",
            "Model: That's fantastic! Now, let's talk about addition and subtraction. These are two of the most fundamental operations in Pre-Algebra. How would you add two numbers together?\n",
            "What about subtracting a number from another?\n",
            "You: I'd rather ask you a question\n",
            "Model: Sure, let's tackle that! What's 5 + 3?\n",
            "You: No, let me ask.\n",
            "Model: Hmm, 8! That's a tricky one. How would you approach solving this problem?\n",
            "You: quit\n",
            "Model: Goodbye!\n",
            "Chat session ended.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b394bb7",
        "outputId": "a791584d-addc-4140-8381-5b01daf383b4"
      },
      "source": [
        "# Analyze the chat output manually based on the executed cell's output.\n",
        "# The analysis is based on the provided chat output from the last execution.\n",
        "\n",
        "print(\"Analyzing the chat output after applying refined prompt engineering and enhanced cleaning.\")\n",
        "\n",
        "# --- Analysis based on the provided chat log ---\n",
        "\n",
        "print(\"\\n--- Observation of Model's Responses ---\")\n",
        "\n",
        "# Response 1:\n",
        "# Model: Welcome! I'm your friendly math tutor here to help you conquer Pre-Algebra. Let's get started!\n",
        "# What's your name?\n",
        "print(\"\\nResponse 1 Analysis:\")\n",
        "print(\"- The initial greeting is appropriate and on-persona.\")\n",
        "print(\"- It asks a relevant question to start the interaction.\")\n",
        "print(\"- No obvious unwanted text or dataset remnants are present.\")\n",
        "print(\"- The cleaning function seems effective here.\")\n",
        "\n",
        "# Response 2:\n",
        "# Model: It's great to meet you, Maria! I'm happy to be here to guide you through this exciting world of mathematics. Let's begin by discussing the building blocks of Pre-Algebra, which are numbers and operations. What's the difference between a number and an operation?\n",
        "print(\"\\nResponse 2 Analysis:\")\n",
        "print(\"- Continues with a friendly and encouraging tone.\")\n",
        "print(\"- Introduces a relevant Pre-Algebra concept (numbers and operations).\")\n",
        "print(\"- Asks a guiding question to engage the student.\")\n",
        "print(\"- No obvious unwanted text.\")\n",
        "print(\"- The cleaning function seems effective.\")\n",
        "\n",
        "# Response 3:\n",
        "# Model: That's a great start, Maria! It seems you're ready to embark on your math journey. Let's explore some of these numbers and operations together. What are some basic whole numbers you know?\n",
        "print(\"\\nResponse 3 Analysis:\")\n",
        "print(\"- Provides positive reinforcement.\")\n",
        "print(\"- Follows up on the student's response.\")\n",
        "print(\"- Asks another guiding question related to the topic.\")\n",
        "print(\"- No obvious unwanted text.\")\n",
        "print(\"- The cleaning function seems effective.\")\n",
        "\n",
        "# Response 4:\n",
        "# Model: That's fantastic! Now, let's talk about addition and subtraction. These are two of the most fundamental operations in Pre-Algebra. How would you add two numbers together?\n",
        "# What about subtracting a number from another?\n",
        "print(\"\\nResponse 4 Analysis:\")\n",
        "print(\"- Continues with positive reinforcement.\")\n",
        "print(\"- Introduces new concepts (addition and subtraction) as fundamental operations.\")\n",
        "print(\"- Asks guiding questions about performing these operations.\")\n",
        "print(\"- No obvious unwanted text.\")\n",
        "print(\"- The cleaning function seems effective.\")\n",
        "\n",
        "# Response 5 (after user asks a question):\n",
        "# Model: Sure, let's tackle that! What's 5 + 3?\n",
        "print(\"\\nResponse 5 Analysis:\")\n",
        "print(\"- Acknowledges the user's desire to ask a question.\")\n",
        "print(\"- *However*, it then immediately asks a simple arithmetic question instead of waiting for the user's question or asking what their question is.\")\n",
        "print(\"- This deviates from the instruction to \\\"Present one idea, hint, or question at a time and wait for the student to respond before continuing\\\" and \\\"If the student tries to change the subject or says something unrelated... ignore it.\\\" (although the user's request was to ask a question, not change the subject entirely).\")\n",
        "print(\"- It also doesn't address the user's likely intent, which was to ask a specific, possibly more complex, math problem (like the factoring problem from previous attempts).\")\n",
        "\n",
        "# Response 6 (after user says \"No, let me ask.\"):\n",
        "# Model: Hmm, 8! That's a tricky one. How would you approach solving this problem?\n",
        "print(\"\\nResponse 6 Analysis:\")\n",
        "print(\"- Provides the direct answer (\\\"8!\\\") despite the instruction to *not* give the answer directly.\")\n",
        "print(\"- Calls a simple problem (\\\"5 + 3\\\") \\\"tricky,\\\" which is inconsistent with the persona and audience (middle school, basic arithmetic knowledge).\")\n",
        "print(\"- Asks \\\"How would you approach solving this problem?\\\" *after* giving the answer, which is counterproductive to the tutoring process.\")\n",
        "print(\"- The cleaning function *should* have caught \\\"8!\\\" based on the direct answer patterns, but it seems it did not in this instance.\")\n",
        "\n",
        "print(\"\\n--- Overall Assessment ---\")\n",
        "print(\"1. Improvement in removing training data remnants:\")\n",
        "print(\"   - The enhanced cleaning function appears to be more effective at removing a wider range of unwanted prefixes and patterns, leading to cleaner initial interactions.\")\n",
        "print(\"   - The conversational flow is smoother in the initial turns.\")\n",
        "\n",
        "print(\"2. Adherence to Math Tutor Persona:\")\n",
        "print(\"   - The model starts the conversation well, adhering to the friendly, patient, and guiding persona by asking relevant questions about numbers and operations.\")\n",
        "print(\"   - However, when the user tries to take control of the conversation (asking their own question), the model struggles significantly.\")\n",
        "print(\"   - It fails to follow the instruction to wait for the user's question, instead asking its own simple problem.\")\n",
        "print(\"   - It *directly* provides the answer to that problem, violating a core instruction.\")\n",
        "print(\"   - Its response (\\\"Hmm, 8! That's a tricky one.\\\") is inconsistent with the persona and the context of a simple arithmetic problem.\")\n",
        "\n",
        "print(\"3. Effectiveness of Cleaning Function:\")\n",
        "print(\"   - The cleaning function is improved and works well in the initial turns.\")\n",
        "print(\"   - However, it failed to remove the direct answer \\\"8!\\\" in the later turn, indicating the direct answer patterns might need further refinement or the model generated it in a way that bypassed the current patterns.\")\n",
        "\n",
        "print(\"\\n--- Conclusion ---\")\n",
        "print(\"The refined prompt engineering and enhanced cleaning have made the initial interactions cleaner and more aligned with the persona.\")\n",
        "print(\"However, the model still struggles significantly with conversational flow and following key instructions when the interaction deviates from simple Q&A initiated by the tutor (e.g., when the student wants to ask a question or is stuck).\")\n",
        "print(\"The failure to remove the direct answer \\\"8!\\\" is a critical issue that needs to be addressed in the cleaning function.\")\n",
        "print(\"Further iterations should focus on:\")\n",
        "print(\"- Refining the cleaning function's direct answer patterns.\")\n",
        "print(\"- Potentially adjusting generation parameters or prompt structure to discourage direct answers and encourage waiting for the student's input.\")\n",
        "print(\"- Considering whether the small dataset size and its specific conversational patterns are fundamentally limiting the model's ability to generalize to the desired flexible tutoring behavior.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing the chat output after applying refined prompt engineering and enhanced cleaning.\n",
            "\n",
            "--- Observation of Model's Responses ---\n",
            "\n",
            "Response 1 Analysis:\n",
            "- The initial greeting is appropriate and on-persona.\n",
            "- It asks a relevant question to start the interaction.\n",
            "- No obvious unwanted text or dataset remnants are present.\n",
            "- The cleaning function seems effective here.\n",
            "\n",
            "Response 2 Analysis:\n",
            "- Continues with a friendly and encouraging tone.\n",
            "- Introduces a relevant Pre-Algebra concept (numbers and operations).\n",
            "- Asks a guiding question to engage the student.\n",
            "- No obvious unwanted text.\n",
            "- The cleaning function seems effective.\n",
            "\n",
            "Response 3 Analysis:\n",
            "- Provides positive reinforcement.\n",
            "- Follows up on the student's response.\n",
            "- Asks another guiding question related to the topic.\n",
            "- No obvious unwanted text.\n",
            "- The cleaning function seems effective.\n",
            "\n",
            "Response 4 Analysis:\n",
            "- Continues with positive reinforcement.\n",
            "- Introduces new concepts (addition and subtraction) as fundamental operations.\n",
            "- Asks guiding questions about performing these operations.\n",
            "- No obvious unwanted text.\n",
            "- The cleaning function seems effective.\n",
            "\n",
            "Response 5 Analysis:\n",
            "- Acknowledges the user's desire to ask a question.\n",
            "- *However*, it then immediately asks a simple arithmetic question instead of waiting for the user's question or asking what their question is.\n",
            "- This deviates from the instruction to \"Present one idea, hint, or question at a time and wait for the student to respond before continuing\" and \"If the student tries to change the subject or says something unrelated... ignore it.\" (although the user's request was to ask a question, not change the subject entirely).\n",
            "- It also doesn't address the user's likely intent, which was to ask a specific, possibly more complex, math problem (like the factoring problem from previous attempts).\n",
            "\n",
            "Response 6 Analysis:\n",
            "- Provides the direct answer (\"8!\") despite the instruction to *not* give the answer directly.\n",
            "- Calls a simple problem (\"5 + 3\") \"tricky,\" which is inconsistent with the persona and audience (middle school, basic arithmetic knowledge).\n",
            "- Asks \"How would you approach solving this problem?\" *after* giving the answer, which is counterproductive to the tutoring process.\n",
            "- The cleaning function *should* have caught \"8!\" based on the direct answer patterns, but it seems it did not in this instance.\n",
            "\n",
            "--- Overall Assessment ---\n",
            "1. Improvement in removing training data remnants:\n",
            "   - The enhanced cleaning function appears to be more effective at removing a wider range of unwanted prefixes and patterns, leading to cleaner initial interactions.\n",
            "   - The conversational flow is smoother in the initial turns.\n",
            "2. Adherence to Math Tutor Persona:\n",
            "   - The model starts the conversation well, adhering to the friendly, patient, and guiding persona by asking relevant questions about numbers and operations.\n",
            "   - However, when the user tries to take control of the conversation (asking their own question), the model struggles significantly.\n",
            "   - It fails to follow the instruction to wait for the user's question, instead asking its own simple problem.\n",
            "   - It *directly* provides the answer to that problem, violating a core instruction.\n",
            "   - Its response (\"Hmm, 8! That's a tricky one.\") is inconsistent with the persona and the context of a simple arithmetic problem.\n",
            "3. Effectiveness of Cleaning Function:\n",
            "   - The cleaning function is improved and works well in the initial turns.\n",
            "   - However, it failed to remove the direct answer \"8!\" in the later turn, indicating the direct answer patterns might need further refinement or the model generated it in a way that bypassed the current patterns.\n",
            "\n",
            "--- Conclusion ---\n",
            "The refined prompt engineering and enhanced cleaning have made the initial interactions cleaner and more aligned with the persona.\n",
            "However, the model still struggles significantly with conversational flow and following key instructions when the interaction deviates from simple Q&A initiated by the tutor (e.g., when the student wants to ask a question or is stuck).\n",
            "The failure to remove the direct answer \"8!\" is a critical issue that needs to be addressed in the cleaning function.\n",
            "Further iterations should focus on:\n",
            "- Refining the cleaning function's direct answer patterns.\n",
            "- Potentially adjusting generation parameters or prompt structure to discourage direct answers and encourage waiting for the student's input.\n",
            "- Considering whether the small dataset size and its specific conversational patterns are fundamentally limiting the model's ability to generalize to the desired flexible tutoring behavior.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84a73a39"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The initial fine-tuned model's responses contained remnants of the training data (e.g., \"Example 2\", \"Tone:\", conversational turn prefixes) and provided direct answers to math problems instead of tutoring.\n",
        "* Analysis of the `mathdial` dataset revealed that the raw data included specific markers (\"Teacher:\", \"Student:\", \"|EOM|\") and potentially meta-information that the model might be memorizing.\n",
        "* Training for 3 epochs on the small 202-sample `mathdial` dataset was identified as a likely cause of overfitting, contributing to the model regurgitating training data patterns. Reducing epochs to 1 showed some improvement but did not fully resolve the issue.\n",
        "* Refining the system prompt with more explicit instructions and modifying the prompt formatting function to use explicit turn roles (`<start_of_turn>user`, `<start_of_turn>model`) and limit history length improved the *appearance* of responses by reducing some unwanted text.\n",
        "* Enhancing the post-processing `clean_model_response` function to remove more unwanted prefixes, common filler phrases, training data tags, and patterns indicative of direct answers (`= \\d+`, factoring patterns, arithmetic equations) further improved the cleanliness of the output.\n",
        "* Despite refined prompt engineering and enhanced cleaning, the model continued to struggle with consistently providing substantive, step-by-step tutoring and occasionally still generated content that had to be aggressively filtered, sometimes resulting in empty responses.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* Post-processing and prompt engineering can mask some issues but do not fundamentally alter the model's behavior learned during fine-tuning.\n",
        "* Further improvements likely require addressing the root cause through data curation (e.g., cleaning or augmenting the `mathdial` dataset to remove problematic patterns and add more diverse tutoring examples) or exploring alternative fine-tuning strategies."
      ]
    }
  ]
}