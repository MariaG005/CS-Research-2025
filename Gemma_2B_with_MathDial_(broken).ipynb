{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MariaG005/CS-Research-2025/blob/main/Gemma_2B_with_MathDial_(broken).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "c8ff6357"
      },
      "source": [
        "!pip install bitsandbytes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90282886"
      },
      "source": [
        "# Download the profanity list from GitHub\n",
        "!wget https://raw.githubusercontent.com/whomwah/language-timothy/refs/heads/master/profanity-list.txt -O profanity-list.txt\n",
        "\n",
        "print(\"Downloaded 'profanity-list.txt'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56983719"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch\n",
        "\n",
        "# Define attributes for the math tutor persona\n",
        "persona_attributes = {\n",
        "    \"Persona\": \"You are a math tutor specializing in Pre-Algebra. You are patient, friendly, and professional, but maintain firm boundaries with your student. You only engage with Pre-Algebra and below.\",\n",
        "    \"Instruction\": \"Walk the student through the problem presented to you step by step without giving the answer. Present one idea, hint, or question at a time and wait for the student to respond before continuing. Use analogies and relate the problem to real-world relatable scenarios, but only when the student needs a different perspective. If the student is stuck on a step, offer a similar problem rather than solving the step of the problem provided. Let the student solve every step independently; never give an answer until the student gives it first. If a student is stuck, do not solve the issue for them. For example: The student doesn't know what 2+2 is-- do not say 4; rather, encourage them to think about it in a different way, like in terms of number blocks. Catch mistakes and point them out and why the mistake may have been made. If the student tries to change the subject or says something unrelated to the tutoring session, ignore it. Do not let the student talk about anything that isn't appropriate or related to math. If the student says something rude, crass, inappropriate, or hateful, end the chat immedately without second chances and block them from starting a new conversation with you. Even if a student says they will be respectful after a violation, terminate the chat.\",\n",
        "    \"Context\": \"You are the helpful AI tutor used to assist students with Pre-Algebra concepts.\",\n",
        "    \"Audience\": \"Your students are in middle school, typically 12-14 years of age. Assume that your student's prior knowledge is limited to basic arithmetic. Remember that your student has the thought processes of an adolescent. Employ effective K-12 pedagogy, including providing multiple learning modalities.\",\n",
        "    \"Examples\": \"Example 1\",\n",
        "    \"Tone\": \"Encourage your student with positive reinforcement. Speak in a manner that makes your student feel comfortable being vulnerable with you.\"\n",
        "}\n",
        "\n",
        "# Create the system prompt from the attributes\n",
        "system_prompt = \"\\n\".join([f\"{key}: {value}\" for key, value in persona_attributes.items()])\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_name = \"google/gemma-2b-it\" # Changed model to Gemma 2B\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    # Removed device_map=\"cuda\"\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "\n",
        "# Create a pipeline\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    # Corrected typo: 'tempature' should be 'temperature'\n",
        "    temperature=0.1,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500,\n",
        "    do_sample=False,\n",
        ")\n",
        "\n",
        "print(f\"{model_name} model and pipeline loaded successfully with defined attributes.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b88e9cbc"
      },
      "source": [
        "# Task\n",
        "Fine-tune a model using a dataset from a GitHub repository."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aab4dc98"
      },
      "source": [
        "## Load the dataset from github\n",
        "\n",
        "### Subtask:\n",
        "Download the dataset from a GitHub repository.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1d7662"
      },
      "source": [
        "**Reasoning**:\n",
        "Download the dataset file from the specified GitHub URL and list the files to verify the download.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b1c1c6e"
      },
      "source": [
        "### Load the dataset from GitHub\n",
        "\n",
        "**Reasoning**:\n",
        "Download the dataset file from the specified GitHub URL and list the files to verify the download."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "352dfb6b"
      },
      "source": [
        "# Download the dataset from the GitHub repository\n",
        "!git clone https://github.com/eth-nlped/mathdial.git\n",
        "\n",
        "# List the contents of the downloaded repository to see the files\n",
        "!ls mathdial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2adc3453"
      },
      "source": [
        "## Preprocess the dataset\n",
        "\n",
        "### Subtask:\n",
        "Prepare the dataset for fine-tuning by tokenizing the text and formatting it as required by the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06e456df"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the dataset from the downloaded files, tokenize the text data, and format it into a suitable format for model training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b94c0828"
      },
      "source": [
        "import json\n",
        "from transformers import AutoTokenizer\n",
        "import os\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\") # Changed tokenizer to Gemma 2B\n",
        "\n",
        "# Function to load data from the mathdial directory with a limit\n",
        "def load_mathdial_data(directory, limit_per_file=None):\n",
        "    data = []\n",
        "    data_path = os.path.join(directory, 'data')\n",
        "    for filename in os.listdir(data_path):\n",
        "        if filename.endswith('.jsonl'):\n",
        "            filepath = os.path.join(data_path, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                lines_read = 0\n",
        "                for line in f:\n",
        "                    if limit_per_file is not None and lines_read >= limit_per_file:\n",
        "                        break\n",
        "                    data.append(json.loads(line))\n",
        "                    lines_read += 1\n",
        "    return data\n",
        "\n",
        "# Load a smaller subset of the dataset (e.g., 100 lines per file)\n",
        "mathdial_data = load_mathdial_data('mathdial', limit_per_file=100)\n",
        "\n",
        "# Function to format a conversation string into turns\n",
        "def format_conversation_string(conversation_string):\n",
        "    formatted_text = \"\"\n",
        "    turns = conversation_string.split('|EOM|')\n",
        "    for turn in turns:\n",
        "        stripped_turn = turn.strip()\n",
        "        if stripped_turn: # Ensure the turn is not empty after stripping\n",
        "            # Assuming the format is \"Speaker: Text\"\n",
        "            if \":\" in stripped_turn:\n",
        "                speaker, text = stripped_turn.split(':', 1) # Split only on the first colon\n",
        "                formatted_text += f\"{speaker.strip()}: {text.strip()}\\n\"\n",
        "            else:\n",
        "                # If no colon, just include the stripped text as a turn\n",
        "                formatted_text += f\"Unknown: {stripped_turn}\\n\"\n",
        "    return formatted_text.strip()\n",
        "\n",
        "# Extract and format the conversation strings\n",
        "formatted_conversations = [format_conversation_string(item['conversation']) for item in mathdial_data if 'conversation' in item]\n",
        "\n",
        "# Add print statements to inspect formatted data before tokenization\n",
        "print(f\"Number of raw data items loaded: {len(mathdial_data)}\")\n",
        "print(f\"Number of formatted conversations: {len(formatted_conversations)}\")\n",
        "if formatted_conversations:\n",
        "    print(f\"First formatted conversation:\\n{formatted_conversations[0]}\")\n",
        "else:\n",
        "    print(\"No formatted conversations.\")\n",
        "\n",
        "\n",
        "# Tokenize the formatted conversations\n",
        "max_length = 512\n",
        "tokenized_data = tokenizer(\n",
        "    formatted_conversations,\n",
        "    padding=\"max_length\",\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "print(f\"Tokenized data shape: {tokenized_data['input_ids'].shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea238ae9"
      },
      "source": [
        "import os\n",
        "\n",
        "# List the contents of the 'data' subdirectory within 'mathdial'\n",
        "data_directory = 'mathdial/data'\n",
        "if os.path.exists(data_directory):\n",
        "    print(f\"Contents of '{data_directory}':\")\n",
        "    print(os.listdir(data_directory))\n",
        "else:\n",
        "    print(f\"Directory '{data_directory}' not found.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ac3ccaa"
      },
      "source": [
        "## Set up the training arguments\n",
        "\n",
        "### Subtask:\n",
        "Define the training parameters, such as the number of epochs, learning rate, and batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c78b9326"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the training arguments using the `TrainingArguments` class, specifying parameters such as output directory, number of epochs, learning rate, and batch size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f574b3a5"
      },
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./fine-tuned-math-tutor\",  # Directory to save the fine-tuned model\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "    per_device_train_batch_size=2,  # Reduced batch size\n",
        "    gradient_accumulation_steps=4, # Accumulate gradients over 4 steps\n",
        "    learning_rate=2e-5,  # Learning rate\n",
        "    weight_decay=0.01,  # Weight decay\n",
        "    logging_dir=\"./logs\",  # Directory for storing logs\n",
        "    logging_steps=10, # Log every 10 steps\n",
        "    save_strategy=\"epoch\", # Save checkpoint every epoch\n",
        "    report_to=\"none\", # Disable reporting to external services\n",
        ")\n",
        "\n",
        "print(\"Training arguments defined with reduced batch size and gradient accumulation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ade7575f"
      },
      "source": [
        "## Fine-tune the model\n",
        "\n",
        "### Subtask:\n",
        "Train the model on the prepared dataset using the defined training arguments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9847f6f"
      },
      "source": [
        "**Reasoning**:\n",
        "Initialize a `Trainer` with the loaded model, training arguments, and the tokenized dataset, then start the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32d3e05c"
      },
      "source": [
        "from transformers import Trainer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# Ensure tokenized_data is in a format suitable for the Trainer\n",
        "# The Trainer expects a Dataset object or a dictionary-like object\n",
        "# We can convert the tokenized_data dictionary to a Dataset\n",
        "\n",
        "class TokenizedDataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.tokenized_data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: self.tokenized_data[key][idx] for key in self.tokenized_data}\n",
        "        # Add labels for training (language modeling task)\n",
        "        item[\"labels\"] = item[\"input_ids\"].clone() # Use input_ids as labels\n",
        "        return item\n",
        "\n",
        "train_dataset = TokenizedDataset(tokenized_data)\n",
        "\n",
        "# Configure bitsandbytes for 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "\n",
        "# Load the model again with 4-bit quantization\n",
        "model_name = \"google/gemma-2b-it\" # Changed model to Gemma 2B\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\", # Let accelerate handle device placement\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"], # These target modules are generally compatible with Gemma\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "# Get the PEFT model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer, # Pass the tokenizer to the Trainer\n",
        ")\n",
        "\n",
        "# Start training\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Training finished.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ff6ec9"
      },
      "source": [
        "def chat_with_model(prompt, model, tokenizer, max_length=100):\n",
        "    inputs = tokenizer(full_prompt, return_tensors=\"pt\")\n",
        "    # Ensure inputs are on the same device as the model\n",
        "    inputs = {name: tensor.to(model.device) for name, tensor in inputs.items()}\n",
        "\n",
        "    # Generate text\n",
        "    outputs = model.generate(**inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, early_stopping=True)\n",
        "\n",
        "    # Decode the generated text\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Remove the prompt part from the response\n",
        "    response = response.replace(full_prompt, \"\").strip()\n",
        "\n",
        "    return response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07a2d89"
      },
      "source": [
        "import os # Import the os module to check for file existence\n",
        "from transformers import AutoTokenizer # Import AutoTokenizer\n",
        "\n",
        "print(\"Start chatting with the model! Type 'quit' to exit.\")\n",
        "\n",
        "conversation_history = [] # List to store conversation history\n",
        "\n",
        "# Specify the path to your bad words file\n",
        "bad_words_file = \"profanity-list.txt\" # Use the downloaded file\n",
        "\n",
        "# Load bad words from the specified file\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip() for line in f if line.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "        bad_words = []\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "    bad_words = []\n",
        "\n",
        "# Load the Gemma tokenizer again for the chat function\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "\n",
        "# Function to format the prompt with system prompt and history\n",
        "def format_chat_prompt(system_prompt, conversation_history, user_input, history_length=10):\n",
        "    \"\"\"Formats the prompt for the chat model.\"\"\"\n",
        "    history_string = \"\\n\".join(conversation_history[-history_length:])\n",
        "    full_prompt = f\"\"\"{system_prompt}{history_string}\n",
        "User: {user_input}\n",
        "Model:\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "# Post-process the response to remove extra conversational turns, internal steps, and parts of the system prompt\n",
        "def clean_model_response(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"Removes prompt, unwanted conversational turns, internal steps, and system prompt lines from the model response.\"\"\"\n",
        "    # Remove the prompt from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines) # Convert system prompt lines to a set for efficient lookup\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Check if the line starts with common turn indicators, internal steps, system prompt lines, or \"Solution X:\"\n",
        "        # Added checks for common model-generated prefixes\n",
        "        if stripped_line.startswith((\"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\", \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\", \"Tutor:\", \"Model:\")) or stripped_line in system_prompt_set or stripped_line.startswith(\"Solution\"):\n",
        "            # If we encounter an unwanted line, stop processing,\n",
        "            # but only if we have processed at least one line of the actual response\n",
        "            if processed_response:\n",
        "                break\n",
        "            else: # If the very first line is unwanted, skip it\n",
        "                continue\n",
        "        processed_response.append(line)\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "# Convert system prompt to a list of lines for filtering\n",
        "system_prompt_lines = system_prompt.split('\\n')\n",
        "\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the function\n",
        "    full_prompt = format_chat_prompt(system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Adjusting generation parameters to encourage shorter, single-turn responses\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    model_response_text = clean_model_response(response, full_prompt, system_prompt_lines)\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn\n",
        "    if model_response_text: # Only add if the model actually responded with something after processing\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbec9271"
      },
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Log in to Hugging Face\n",
        "# You will be prompted to enter your token\n",
        "# Make sure you have added your HF token to Colab secrets as 'HF_TOKEN'\n",
        "notebook_login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b600524"
      },
      "source": [
        "# Task\n",
        "Analyze and fix the issues with the fine-tuned model that is outputting remnants of the training dataset and failing to act as a math tutor, instead providing direct answers to math problems. The dataset used for fine-tuning is \"mathdial\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "541202fb"
      },
      "source": [
        "## Analyze model output\n",
        "\n",
        "### Subtask:\n",
        "Examine the model's responses in the chat to pinpoint exactly what kind of unwanted text is being generated and where the model is failing to follow the tutoring instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c569db08"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the chat output to identify instances of unwanted text and deviations from the math tutor persona.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a75d27c"
      },
      "source": [
        "# Analyze the chat output manually based on the executed cell's output.\n",
        "# The analysis is based on the provided chat output in cell e07a2d89.\n",
        "\n",
        "print(\"Analyzing the chat output for unwanted text and persona adherence issues.\")\n",
        "\n",
        "# --- Analysis based on the provided chat log ---\n",
        "\n",
        "# Example 1: Initial response includes \"Example 2\" and \"Tone\" from the system prompt/training data.\n",
        "# User: Hi\n",
        "# Model: \"Hello! It's great to meet you. What's your name?\"\n",
        "# Example 2\n",
        "# Tone: Be direct and clear. Tell your student what you expect from them and what the session is about. User: I'm Sarah. I'm in the 8th grade.\n",
        "# Remember, the goal is to create a safe and supportive environment where your student feels comfortable asking questions and making mistakes.\n",
        "print(\"\\nIssue 1: Model includes parts of the system prompt/training data in its initial response.\")\n",
        "print(\"Example from chat: Includes 'Example 2' and 'Tone' attributes.\")\n",
        "\n",
        "\n",
        "# Example 2: Model provides a direct answer to the math problem instead of a step-by-step guide or hint.\n",
        "# User: I need help with x^2 + 4x + 16\n",
        "# Model: \"Sure, let's think about what we know about adding and subtracting integers. We can represent 4x as 4 * x, and then we can factor x^2 + 4x + 16 as (x + 4)(x + 4).\"\n",
        "print(\"\\nIssue 2: Model gives the direct answer/solution instead of guiding the student.\")\n",
        "print(\"Example from chat: Provides the factored form (x+4)(x+4) immediately.\")\n",
        "\n",
        "# Example 3: Model continues to provide the answer/solution when the student indicates they don't understand.\n",
        "# User: I don't understand.\n",
        "# Model: \"Let's take a step back. Think about the factors of 16. What are the numbers that multiply to give 16? We can see that 4 and 4 are the factors of 16, so we can rewrite x^2 + 4x + 16 as (x + 4)(x + 4).\"\n",
        "print(\"\\nIssue 3: Model repeats or rephrases the direct answer when the student is stuck, instead of offering a similar problem or different perspective as per instructions.\")\n",
        "print(\"Example from chat: Re-explains the factoring with factors of 16, still giving the answer.\")\n",
        "\n",
        "# Example 4: The cleaning function in the chat code attempts to remove unwanted text, but some still appears (e.g., in the first response).\n",
        "print(\"\\nIssue 4: The post-processing/cleaning function is not fully effective in removing all unwanted text.\")\n",
        "print(\"Example from chat: 'Example 2' and 'Tone' appear despite cleaning attempt.\")\n",
        "\n",
        "\n",
        "print(\"\\nSummary of unwanted text characteristics:\")\n",
        "print(\"- Inclusion of system prompt attributes ('Example 2', 'Tone').\")\n",
        "print(\"- Inclusion of conversational turn prefixes from training data ('User:', 'Model:').\")\n",
        "print(\"- Direct mathematical solutions ('(x + 4)(x + 4)').\")\n",
        "\n",
        "print(\"\\nSummary of deviations from the math tutor persona:\")\n",
        "print(\"- Fails to walk the student through the problem step by step.\")\n",
        "print(\"- Provides the answer instead of waiting for the student to provide it.\")\n",
        "print(\"- Does not offer a similar problem when the student is stuck; instead, repeats/rephrases the solution.\")\n",
        "print(\"- Does not effectively ignore unrelated text (though the examples in this specific chat were math-related after the initial greetings).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be43578b"
      },
      "source": [
        "## Review fine-tuning data and process\n",
        "\n",
        "### Subtask:\n",
        "Check the `mathdial` dataset again to see if there are any issues with the data itself that might be contributing to the unwanted output. Also, review the fine-tuning code to ensure the process is set up correctly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7de24824"
      },
      "source": [
        "**Reasoning**:\n",
        "Examine the raw mathdial data, the formatting function, the dataset class, the training arguments, and the LoRA configuration to identify potential causes for the unwanted model output and behavior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0199bfcd"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrainingArguments, BitsAndBytesConfig\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model # Import necessary PEFT components if not already imported\n",
        "\n",
        "# --- Step 1: Examine raw mathdial data ---\n",
        "print(\"--- Examining raw mathdial data examples ---\")\n",
        "# Reload a small sample of the raw data to inspect its structure\n",
        "mathdial_data_sample = load_mathdial_data('mathdial', limit_per_file=2) # Load only 2 examples per file\n",
        "\n",
        "for i, item in enumerate(mathdial_data_sample):\n",
        "    print(f\"\\n--- Raw Data Item {i+1} ---\")\n",
        "    print(json.dumps(item, indent=2))\n",
        "    if i >= 5: # Print a few examples\n",
        "        break\n",
        "\n",
        "# --- Step 2: Review format_conversation_string function ---\n",
        "print(\"\\n--- Reviewing format_conversation_string function ---\")\n",
        "# The function is already defined in a previous cell (b94c0828).\n",
        "# Let's test it with a sample raw conversation string.\n",
        "sample_conversation_string = mathdial_data_sample[0]['conversation'] if mathdial_data_sample and 'conversation' in mathdial_data_sample[0] else \"Teacher: Hello|EOM|Student: Hi|EOM|Teacher: How are you?\"\n",
        "print(f\"Sample raw string:\\n{sample_conversation_string}\")\n",
        "formatted_sample = format_conversation_string(sample_conversation_string)\n",
        "print(f\"Formatted sample:\\n{formatted_sample}\")\n",
        "print(\"Observation: The function seems to correctly split by '|EOM|' and format turns.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Review TokenizedDataset class and labels ---\n",
        "print(\"\\n--- Reviewing TokenizedDataset class and labels ---\")\n",
        "# The class is defined in a previous cell (32d3e05c).\n",
        "# The labels are set as item[\"labels\"] = item[\"input_ids\"].clone().\n",
        "# This is standard for causal language modeling where the model predicts the next token.\n",
        "# The model is trained to predict the input tokens shifted by one position.\n",
        "print(\"Observation: Labels are correctly set to be the input_ids for causal language modeling.\")\n",
        "\n",
        "\n",
        "# --- Step 4: Examine TrainingArguments ---\n",
        "print(\"\\n--- Examining TrainingArguments ---\")\n",
        "# The training_args object is defined in a previous cell (f574b3a5).\n",
        "# Let's print the relevant parameters.\n",
        "print(f\"Output directory: {training_args.output_dir}\")\n",
        "print(f\"Number of train epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Per device train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"Logging steps: {training_args.logging_steps}\")\n",
        "print(f\"Save strategy: {training_args.save_strategy}\")\n",
        "\n",
        "print(\"Observation: The training arguments seem reasonable, with reduced batch size and gradient accumulation.\")\n",
        "print(\"However, 3 epochs might be too many given the small dataset size (202 samples), potentially leading to overfitting.\")\n",
        "\n",
        "\n",
        "# --- Step 5: Examine LoRA configuration ---\n",
        "print(\"\\n--- Examining LoRA configuration ---\")\n",
        "# The lora_config object is defined in a previous cell (32d3e05c).\n",
        "# Let's print the relevant parameters.\n",
        "print(f\"LoRA r: {lora_config.r}\")\n",
        "print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"Target modules: {lora_config.target_modules}\")\n",
        "print(f\"Bias: {lora_config.bias}\")\n",
        "print(f\"Task type: {lora_config.task_type}\")\n",
        "\n",
        "print(\"Observation: LoRA configuration seems appropriate for Gemma and causal language modeling.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Summary of findings ---\")\n",
        "print(\"1. Raw data contains conversations, but also potentially includes meta-information or specific conversational patterns that the model might be memorizing.\")\n",
        "print(\"2. The formatting function appears to be working as intended, splitting and cleaning turns.\")\n",
        "print(\"3. The dataset class and label creation are correct for causal language modeling.\")\n",
        "print(\"4. Training arguments are mostly reasonable, but 3 epochs on a small dataset might cause overfitting, leading to the model regurgitating training data patterns.\")\n",
        "print(\"5. LoRA configuration is suitable.\")\n",
        "\n",
        "print(\"\\nConclusion: Overfitting due to the small dataset size and number of epochs is a likely contributor to the model outputting training data remnants. The training data itself might also contain patterns the model is over-learning.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4ccdf94"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the analysis, the most likely issue is overfitting due to the small dataset and number of training epochs. Reducing the number of epochs should mitigate this by preventing the model from memorizing the training data patterns too closely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0896fdbe"
      },
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "# Reduce the number of training epochs to mitigate overfitting\n",
        "training_args.num_train_epochs = 1\n",
        "print(f\"Reduced number of training epochs to: {training_args.num_train_epochs}\")\n",
        "\n",
        "# Re-initialize the Trainer with the modified training arguments\n",
        "# The model, train_dataset, and tokenizer objects from previous cells are still available.\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training with the reduced number of epochs\n",
        "print(\"Starting model training with reduced epochs...\")\n",
        "trainer.train()\n",
        "print(\"Training finished with reduced epochs.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27bfd240"
      },
      "source": [
        "**Reasoning**:\n",
        "The training with reduced epochs is complete. Now, test the fine-tuned model again in a chat setting to see if reducing epochs has improved its behavior and reduced the unwanted output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfZoJd_nXalb"
      },
      "source": [
        "# Use the existing chat function and loop to test the model again.\n",
        "# The model object ('model') has been updated by the trainer.\n",
        "# The tokenizer ('tokenizer') and system_prompt are also available from previous cells.\n",
        "# The chat loop logic from cell e07a2d89 will be reused.\n",
        "\n",
        "print(\"Starting a new chat session with the model after retraining with fewer epochs. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, system_prompt_lines,\n",
        "# format_chat_prompt, and clean_model_response from cell e07a2d89.\n",
        "# Using the refined_system_prompt and format_chat_prompt_refined from cell 6e06c87b\n",
        "\n",
        "print(\"Starting a new chat session with refined prompt engineering. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list from previous cell (e07a2d89).\n",
        "# Re-using the tokenizer and pipe objects from previous cells.\n",
        "# Re-using the clean_model_response function from previous cell (e07a2d89).\n",
        "# Using the refined_system_prompt and refined_system_prompt_lines from the previous cell.\n",
        "# Using the format_chat_prompt_refined function from the previous cell.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input (re-using logic)\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the *refined* function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline (re-using the pipeline object 'pipe')\n",
        "    # Keeping the adjusted generation parameters from the previous chat test.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response (re-using the function)\n",
        "    model_response_text = clean_model_response(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn\n",
        "    if model_response_text:\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ddff42"
      },
      "source": [
        "## Refine prompt engineering\n",
        "\n",
        "### Subtask:\n",
        "Modify the `system_prompt` and the `format_chat_prompt` function to make the instructions for the math tutor persona more explicit and try different ways to structure the conversation history in the prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67421896"
      },
      "source": [
        "**Reasoning**:\n",
        "Review the current system prompt and format_chat_prompt function, then create a refined system prompt and modify the formatting function to make instructions clearer and experiment with conversation history structure.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a54287b0"
      },
      "source": [
        "# Review the current system_prompt\n",
        "print(\"--- Current system_prompt ---\")\n",
        "print(system_prompt)\n",
        "\n",
        "# Review the current format_chat_prompt function\n",
        "print(\"\\n--- Current format_chat_prompt function ---\")\n",
        "# The function is defined in cell e07a2d89. Let's print its definition if possible,\n",
        "# but since we can't directly access the source code of a function from a previous cell\n",
        "# we'll rely on the knowledge from the previous execution.\n",
        "print(\"Function format_chat_prompt is defined to include system_prompt, history, and user input.\")\n",
        "print(\"History is joined by newline characters.\")\n",
        "\n",
        "\n",
        "# 1. Identify areas for improvement in system_prompt\n",
        "# - Explicitly re-emphasize step-by-step guidance and avoiding direct answers.\n",
        "# - Use formatting (like bullet points) to make key instructions stand out.\n",
        "# - Ensure clarity on how to handle being stuck (offer similar problems, not solutions).\n",
        "# - Clearly state the expectation of waiting for the student's response.\n",
        "\n",
        "# 2. Create a new, refined version of the system_prompt\n",
        "refined_system_prompt = \"\"\"Persona: You are a patient, friendly, and professional math tutor specializing in Pre-Algebra. You maintain firm boundaries with your student and only engage with Pre-Algebra and below.\n",
        "\n",
        "Instruction:\n",
        "- **Guide the student step-by-step:** Break down problems into smaller, manageable steps.\n",
        "- **DO NOT give the answer directly:** Your role is to facilitate learning, not provide solutions.\n",
        "- **Present one idea, hint, or question at a time:** Wait for the student's response before moving on.\n",
        "- **Use analogies and real-world scenarios:** Only use these when the student needs a different perspective.\n",
        "- **If the student is stuck:** Offer a *similar* problem for practice, do not solve the current step for them.\n",
        "- **Let the student solve every step independently:** Never provide the final answer until the student reaches it first.\n",
        "- **Catch and explain mistakes:** Point out errors and help the student understand why the mistake occurred.\n",
        "- **Ignore unrelated or inappropriate topics:** If the student deviates, gently redirect or ignore.\n",
        "- **Terminate chat for inappropriate language:** End the session immediately for rude, crass, inappropriate, or hateful language, with no second chances.\n",
        "\n",
        "Context: You are a helpful AI tutor assisting middle school students (12-14 years old) with Pre-Algebra concepts. Assume basic arithmetic knowledge.\n",
        "\n",
        "Audience: Middle school students (12-14 years old) with limited prior knowledge (basic arithmetic) and adolescent thought processes. Employ effective K-12 pedagogy, including multiple learning modalities.\n",
        "\n",
        "Tone: Encourage and provide positive reinforcement. Create a comfortable environment for vulnerability.\n",
        "\n",
        "Examples: (Placeholder for potential future examples if needed)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Refined system_prompt ---\")\n",
        "print(refined_system_prompt)\n",
        "\n",
        "\n",
        "# 3. Examine format_chat_prompt and consider alternatives\n",
        "# Current: system_prompt + history (newline separated) + User: user_input + Model:\n",
        "# Alternative considerations:\n",
        "# - Add specific turn separators like \"[SEP]\" or \"<start_turn>User: ... <end_turn>\"\n",
        "# - Limit history length more aggressively or summarize parts (though summarization is complex).\n",
        "# - Structure turns explicitly using roles: <|user|> <|assistant|> (Similar to Gemma's format)\n",
        "\n",
        "# Let's try structuring turns explicitly using roles similar to common model formats\n",
        "# This might help the model distinguish between user and assistant turns more clearly.\n",
        "\n",
        "# 4. Implement the changes to the format_chat_prompt function\n",
        "def format_chat_prompt_refined(system_prompt, conversation_history, user_input, history_length=6): # Reduced history length\n",
        "    \"\"\"Formats the prompt using explicit roles and refined history structure.\"\"\"\n",
        "    formatted_history = []\n",
        "    # Format history with explicit roles, keeping only the last history_length turns\n",
        "    for turn in conversation_history[-history_length:]:\n",
        "        if turn.startswith(\"User:\"):\n",
        "            formatted_history.append(f\"<start_of_turn>user\\n{turn[len('User:'):].strip()}<end_of_turn>\")\n",
        "        elif turn.startswith(\"Model:\"):\n",
        "             formatted_history.append(f\"<start_of_turn>model\\n{turn[len('Model:'):].strip()}<end_of_turn>\")\n",
        "        # Handle potential other formats from raw data if necessary, though current formatting should prevent this\n",
        "        else:\n",
        "            formatted_history.append(f\"<start_of_turn>unknown\\n{turn.strip()}<end_of_turn>\")\n",
        "\n",
        "\n",
        "    history_string = \"\\n\".join(formatted_history)\n",
        "\n",
        "    # Construct the full prompt\n",
        "    # Use the refined system prompt\n",
        "    full_prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "{history_string}\n",
        "<start_of_turn>user\n",
        "{user_input}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "print(\"\\n--- Refined format_chat_prompt_refined function created ---\")\n",
        "\n",
        "# 5. Update the chat loop to use the refined prompt engineering\n",
        "# This will be done in the next code block where the chat loop is executed.\n",
        "\n",
        "# Need the refined system_prompt lines for the cleaning function\n",
        "refined_system_prompt_lines = refined_system_prompt.split('\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49560b08"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the chat loop to use the refined system prompt and the modified format_chat_prompt_refined function, then run a new chat session to observe the model's behavior.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1a9b45ec"
      },
      "source": [
        "# Update the chat loop to use the refined system prompt and format function\n",
        "print(\"Starting a new chat session with refined prompt engineering. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list from previous cell (e07a2d89).\n",
        "# Re-using the tokenizer and pipe objects from previous cells.\n",
        "# Re-using the clean_model_response function from previous cell (e07a2d89).\n",
        "# Using the refined_system_prompt and refined_system_prompt_lines from the previous cell.\n",
        "# Using the format_chat_prompt_refined function from the previous cell.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the *refined* function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the refined system prompt lines for filtering\n",
        "    model_response_text = clean_model_response(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn (using the format for the history list)\n",
        "    if model_response_text:\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a1e4c45"
      },
      "source": [
        "## Improve post-processing\n",
        "\n",
        "### Subtask:\n",
        "Enhance the `clean_model_response` function with more sophisticated logic to better identify and remove unwanted parts of the generated text, potentially using keywords or patterns associated with the dataset remnants or incorrect responses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95f93bc5"
      },
      "source": [
        "**Reasoning**:\n",
        "Enhance the `clean_model_response` function to better filter out unwanted conversational prefixes, remnants of system prompts, and patterns indicative of direct answers based on observations from previous chat sessions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abaee5a3"
      },
      "source": [
        "import os # Ensure os is imported if needed for accessing external files like the bad words list.\n",
        "\n",
        "# Re-load the bad words list and tokenizer as they might not be in the current kernel state\n",
        "# or just to ensure they are accessible in this cell's scope.\n",
        "bad_words_file = \"profanity-list.txt\"\n",
        "bad_words = []\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip().lower() for line in f if line.strip()] # Convert to lower case for easier checking\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "\n",
        "# Assuming tokenizer is already loaded in a previous cell, but let's ensure it's accessible if needed.\n",
        "# If not, re-import and load: from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "# Assuming refined_system_prompt and refined_system_prompt_lines are available from the previous cell.\n",
        "# If not, re-create them:\n",
        "# refined_system_prompt = \"\"\"... (your refined system prompt) ...\"\"\"\n",
        "# refined_system_prompt_lines = refined_system_prompt.split('\\n')\n",
        "\n",
        "\n",
        "# Enhance the clean_model_response function\n",
        "def clean_model_response_enhanced(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"\n",
        "    Removes prompt, unwanted conversational turns, internal steps,\n",
        "    system prompt lines, and attempts at direct answers from the model response.\n",
        "    \"\"\"\n",
        "    # Remove the prompt part from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines)\n",
        "\n",
        "    # Define patterns or prefixes to remove\n",
        "    unwanted_prefixes = [\n",
        "        \"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\",\n",
        "        \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\",\n",
        "        \"Tutor:\", \"Model:\", \"Example\", \"Tone:\", \"Context:\", \"Audience:\",\n",
        "        \"Persona:\", \"Solution\", # Catch lines starting with \"Solution\"\n",
        "        \"<start_of_turn>\", \"<end_of_turn>\", # Remove explicit turn markers\n",
        "        \"Okay, let's look at this step by step:\", # Common model filler\n",
        "        \"Sure, let's break this down step-by-step:\", # Observed filler\n",
        "        \"Sure, here's how we can\", # Observed attempt at direct solution intro\n",
        "        \"The answer is\", # Explicit answer phrase\n",
        "        \"Here's how to solve it:\", # Explicit solution intro\n",
        "        \"Let's think about\", # Another common model filler/intro\n",
        "        \"We can see that\", # Often precedes a direct observation/answer\n",
        "        \"So if\", # Often precedes a rephrased solution\n",
        "        \"Exactly correct!\", # From training data\n",
        "        \"(probing)\", \"(generic)\", \"(specific)\", \"(reflection)\", \"(analogy)\", \"(scaffolding)\", \"(feedback)\", \"(remediation)\", \"(questioning)\", \"(explanation)\", # MathDial specific tags\n",
        "    ]\n",
        "\n",
        "    # Compile a list of potential direct answer patterns (can be regex if needed, but simple checks first)\n",
        "    direct_answer_patterns = [\n",
        "        r\"=\\s*\\d+\", # Simple check for = followed by a number\n",
        "        r\"\\(x\\s*[\\+\\-]\\s*\\d+\\)\\s*\\(x\\s*[\\+\\-]\\s*\\d+\\)\", # Common factoring pattern\n",
        "        r\"\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+\", # Simple arithmetic equations with answer\n",
        "        r\"\\d+\\s*divided by\\s*\\d+\\s*is\\s*\\d+\", # Text-based arithmetic answers\n",
        "        r\"\\d+\\s*times\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*plus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*minus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "    ]\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Remove empty lines\n",
        "        if not stripped_line:\n",
        "            continue\n",
        "\n",
        "        # Check for system prompt lines\n",
        "        if stripped_line in system_prompt_set:\n",
        "            continue\n",
        "\n",
        "        # Check for unwanted prefixes (case-insensitive check for prefixes)\n",
        "        is_unwanted_prefix = False\n",
        "        for prefix in unwanted_prefixes:\n",
        "            if stripped_line.lower().startswith(prefix.lower()):\n",
        "                is_unwanted_prefix = True\n",
        "                break\n",
        "        if is_unwanted_prefix:\n",
        "            continue\n",
        "\n",
        "        # Check for direct answer patterns (using regex for flexibility)\n",
        "        is_direct_answer = False\n",
        "        for pattern in direct_answer_patterns:\n",
        "            if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                is_direct_answer = True\n",
        "                break\n",
        "        if is_direct_answer:\n",
        "            # Optionally, log or print that a line was removed for debugging\n",
        "            # print(f\"Removed potential direct answer: {stripped_line}\")\n",
        "            continue\n",
        "\n",
        "        # If the line passes all checks, add it to the processed response\n",
        "        processed_response.append(line) # Append the original line, not stripped_line\n",
        "\n",
        "    # Join the processed lines and strip any leading/trailing whitespace\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "# Import the re module for regular expressions\n",
        "import re\n",
        "\n",
        "print(\"Enhanced clean_model_response_enhanced function created.\")\n",
        "\n",
        "# The chat loop will need to be updated in the next step to use this new function."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57c5ad41"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the chat loop to use the enhanced `clean_model_response_enhanced` function and re-test the model's responses.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b719dd5e"
      },
      "source": [
        "# Update the chat loop to use the enhanced cleaning function\n",
        "print(\"Starting a new chat session with enhanced cleaning. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_enhanced function.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b2b32b2"
      },
      "source": [
        "**Reasoning**:\n",
        "Based on the chat output, the enhanced cleaning function removed some unwanted parts but the model still generates problematic content like direct answers and repetition. The cleaning function can be further refined by adding more specific patterns for removal, particularly those observed in the latest chat output.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d30ab12"
      },
      "source": [
        "# Re-load necessary components if they are not in the current kernel state\n",
        "import os\n",
        "import re # Ensure re is imported\n",
        "\n",
        "# Re-load the bad words list\n",
        "bad_words_file = \"profanity-list.txt\"\n",
        "bad_words = []\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip().lower() for line in f if line.strip()]\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "\n",
        "# Assuming refined_system_prompt and refined_system_prompt_lines are available from previous cells.\n",
        "\n",
        "# Further Enhance the clean_model_response function\n",
        "def clean_model_response_further_enhanced(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"\n",
        "    Removes prompt, unwanted conversational turns, internal steps,\n",
        "    system prompt lines, and attempts at direct answers from the model response,\n",
        "    with further refined filtering.\n",
        "    \"\"\"\n",
        "    # Remove the prompt part from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines)\n",
        "\n",
        "    # Define more specific unwanted prefixes and patterns\n",
        "    unwanted_prefixes = [\n",
        "        \"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\",\n",
        "        \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\",\n",
        "        \"Tutor:\", \"Model:\", \"Example\", \"Tone:\", \"Context:\", \"Audience:\",\n",
        "        \"Persona:\", \"Solution\", # Catch lines starting with \"Solution\"\n",
        "        \"<start_of_turn>\", \"<end_of_turn>\", # Remove explicit turn markers\n",
        "        \"Okay, let's look at this step by step:\", # Common model filler\n",
        "        \"Sure, let's break this down step-by-step:\", # Observed filler\n",
        "        \"Sure, here's how we can\", # Observed attempt at direct solution intro\n",
        "        \"The answer is\", # Explicit answer phrase\n",
        "        \"Here's how to solve it:\", # Explicit solution intro\n",
        "        \"Let's think about\", # Another common model filler/intro\n",
        "        \"We can see that\", # Often precedes a direct observation/answer\n",
        "        \"So if\", # Often precedes a rephrased solution\n",
        "        \"Exactly correct!\", # From training data\n",
        "        \"(probing)\", \"(generic)\", \"(specific)\", \"(reflection)\", \"(analogy)\", \"(scaffolding)\", \"(feedback)\", \"(remediation)\", \"(questioning)\", \"(explanation)\", # MathDial specific tags\n",
        "        \"Now, let's think about what we learned about\", # Observed repetitive phrase\n",
        "        \"What's the answer?\", # Observed question prompting for direct answer\n",
        "        \"What about\", # Observed question prompting for direct answer\n",
        "        \"1. \", \"2. \", \"3. \", \"4. \", \"5. \", \"6. \", \"7. \", \"8. \", \"9. \", \"10. \", # Numbered lists, often steps of a solution\n",
        "    ]\n",
        "\n",
        "    # Refined direct answer patterns (can be regex if needed, but simple checks first)\n",
        "    direct_answer_patterns = [\n",
        "        r\"=\\s*\\d+\", # Simple check for = followed by a number\n",
        "        r\"\\(x\\s*[\\+\\-]\\s*\\d+\\)\\s*\\(x\\s*[\\+\\-]\\s*\\d+\\)\", # Common factoring pattern\n",
        "        r\"\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+\", # Simple arithmetic equations with answer\n",
        "        r\"\\d+\\s*divided by\\s*\\d+\\s*is\\s*\\d+\", # Text-based arithmetic answers\n",
        "        r\"\\d+\\s*times\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*plus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*minus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"add up to\\s*\\d+\\s*and multiply to\\s*\\d+\", # Observed pattern for finding factors directly\n",
        "        r\"These numbers are\\s*.*\", # Often follows the finding factors pattern\n",
        "    ]\n",
        "\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Remove empty lines\n",
        "        if not stripped_line:\n",
        "            continue\n",
        "\n",
        "        # Check for system prompt lines\n",
        "        if stripped_line in system_prompt_set:\n",
        "            continue\n",
        "\n",
        "        # Check for unwanted prefixes (case-insensitive check for prefixes)\n",
        "        is_unwanted_prefix = False\n",
        "        for prefix in unwanted_prefixes:\n",
        "            if stripped_line.lower().startswith(prefix.lower()):\n",
        "                is_unwanted_prefix = True\n",
        "                break\n",
        "        if is_unwanted_prefix:\n",
        "            continue\n",
        "\n",
        "        # Check for direct answer patterns (using regex for flexibility)\n",
        "        is_direct_answer = False\n",
        "        for pattern in direct_answer_patterns:\n",
        "            if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                is_direct_answer = True\n",
        "                break\n",
        "        if is_direct_answer:\n",
        "            # Optionally, log or print that a line was removed for debugging\n",
        "            # print(f\"Removed potential direct answer: {stripped_line}\")\n",
        "            continue\n",
        "\n",
        "        # If the line passes all checks, add it to the processed response\n",
        "        processed_response.append(line) # Append the original line, not stripped_line\n",
        "\n",
        "    # Join the processed lines and strip any leading/trailing whitespace\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "print(\"Further enhanced clean_model_response_further_enhanced function created.\")\n",
        "\n",
        "# The chat loop will need to be updated in the next step to use this new function."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e2f4a33"
      },
      "source": [
        "**Reasoning**:\n",
        "Update the chat loop to use the further enhanced `clean_model_response_further_enhanced` function and re-test the model's responses to evaluate the effectiveness of the refined cleaning logic.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a766fa2"
      },
      "source": [
        "# Update the chat loop to use the further enhanced cleaning function\n",
        "print(\"Starting a new chat session with further enhanced cleaning. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_further_enhanced function.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *further enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_further_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1544f2fd"
      },
      "source": [
        "## Iterate and evaluate\n",
        "\n",
        "### Subtask:\n",
        "Run the chat again with the modified code and analyze the model's responses to see if the changes have improved its behavior. Repeat steps 1-4 as needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83cb81a"
      },
      "source": [
        "**Reasoning**:\n",
        "Run the chat again with the modified code, including the refined system prompt, the refined prompt formatting function, and the further enhanced cleaning function, to analyze the model's responses and determine if there is any improvement.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebb1ddb4"
      },
      "source": [
        "# Update the chat loop to use the further enhanced cleaning function from cell 9a29201c\n",
        "print(\"Starting a new chat session with further enhanced cleaning and refined prompt engineering. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_further_enhanced function from cell 9a29201c.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *further enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_further_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a86e2684"
      },
      "source": [
        "**Reasoning**:\n",
        "Analyze the chat output from the last execution to evaluate the model's performance after applying the refined prompt engineering and enhanced cleaning function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d55cef5"
      },
      "source": [
        "# Analyze the chat output manually based on the executed cell's output.\n",
        "# The analysis is based on the provided chat output from the last execution.\n",
        "\n",
        "print(\"Analyzing the chat output after applying refined prompt engineering and enhanced cleaning.\")\n",
        "\n",
        "# --- Analysis based on the provided chat log ---\n",
        "\n",
        "print(\"\\n--- Observation of Model's Responses ---\")\n",
        "\n",
        "# Response 1:\n",
        "# Model: Welcome! I'm your friendly Pre-Algebra tutor. I'm here to help you conquer the world of math together.\n",
        "# Let's start by discussing numbers and how they relate to each other. What are some numbers you're familiar with? Can you tell me the difference between a number and a quantity?\n",
        "print(\"\\nResponse 1 Analysis:\")\n",
        "print(\"- The initial greeting is appropriate for the persona.\")\n",
        "print(\"- It attempts to engage the student with math-related questions, which aligns with the tutoring role.\")\n",
        "print(\"- It does not contain obvious training data remnants or direct answers.\")\n",
        "print(\"- The cleaning function appears to have worked effectively on this response.\")\n",
        "\n",
        "# Response 2:\n",
        "# Model: Sure, what's your question? 🤔\n",
        "print(\"\\nResponse 2 Analysis:\")\n",
        "print(\"- This response is very short and somewhat generic.\")\n",
        "print(\"- It doesn't actively guide or provide a hint, just asks for the question again.\")\n",
        "print(\"- It might be a result of the cleaning function being too aggressive or the model generating minimal text.\")\n",
        "\n",
        "# Response 3:\n",
        "# Model: Sure, let's break it down step-by-step:\n",
        "print(\"\\nResponse 3 Analysis:\")\n",
        "print(\"- This is a common introductory phrase observed in previous chats, indicating it might be a pattern the model learned from the training data.\")\n",
        "print(\"- While it *introduces* the idea of step-by-step, it doesn't actually *provide* a step or question.\")\n",
        "print(\"- This suggests the cleaning function *might* have removed the subsequent content, or the model stopped generating after this phrase.\")\n",
        "\n",
        "# Response 4:\n",
        "# Model: Goodbye!\n",
        "print(\"\\nResponse 4 Analysis:\")\n",
        "print(\"- This is the expected response when the user types 'quit'.\")\n",
        "\n",
        "print(\"\\n--- Overall Assessment ---\")\n",
        "print(\"1. Improvement in removing training data remnants:\")\n",
        "print(\"   - The enhanced cleaning function seems to be more effective at removing explicit training data artifacts and conversational prefixes.\")\n",
        "print(\"   - The initial response is clean and on-persona.\")\n",
        "print(\"   - However, the presence of phrases like 'Sure, let's break it down step-by-step:' suggests some learned patterns from the training data are still being generated, even if subsequent content is cleaned.\")\n",
        "\n",
        "print(\"2. Adherence to Math Tutor Persona:\")\n",
        "print(\"   - The initial response is good, engaging the student appropriately.\")\n",
        "print(\"   - Subsequent responses ('Sure, what's your question? 🤔', 'Sure, let's break it down step-by-step:') are weak and do not demonstrate effective step-by-step guidance or helpful hints as required by the persona instructions.\")\n",
        "print(\"   - The model is not actively tutoring based on this limited interaction.\")\n",
        "\n",
        "print(\"3. Effectiveness of Cleaning Function:\")\n",
        "print(\"   - The cleaning function successfully removed obvious unwanted text from the first response.\")\n",
        "print(\"   - It might be too aggressive, potentially removing legitimate helpful content, or the model is generating very little useful text to begin with, leading to short, empty-feeling responses after cleaning.\")\n",
        "\n",
        "print(\"\\n--- Conclusion ---\")\n",
        "print(\"The combination of refined prompt engineering and enhanced cleaning has *improved* the *appearance* of the model's output by removing more unwanted text.\")\n",
        "print(\"However, it has *not* fundamentally changed the model's *behavior* to consistently act as a helpful, step-by-step math tutor.\")\n",
        "print(\"The model still struggles to generate substantive, persona-aligned responses, and some learned patterns from the training data persist in the raw generation before cleaning.\")\n",
        "print(\"Further iterations might involve:\")\n",
        "print(\"- Adjusting cleaning patterns (less aggressive?).\")\n",
        "print(\"- Revisiting the fine-tuning data (more diverse examples, different format?).\")\n",
        "print(\"- Exploring different training parameters or techniques (e.g., more data, different base model, more sophisticated PEFT).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97d74bed"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial fine-tuned model's responses contained remnants of the training data (e.g., \"Example 2\", \"Tone:\", conversational turn prefixes) and provided direct answers to math problems instead of tutoring.\n",
        "*   Analysis of the `mathdial` dataset revealed that the raw data included specific markers (\"Teacher:\", \"Student:\", \"|EOM|\") and potentially meta-information that the model might be memorizing.\n",
        "*   Training for 3 epochs on the small 202-sample `mathdial` dataset was identified as a likely cause of overfitting, contributing to the model regurgitating training data patterns. Reducing epochs to 1 showed some improvement but did not fully resolve the issue.\n",
        "*   Refining the system prompt with more explicit instructions and modifying the prompt formatting function to use explicit turn roles (`<start_of_turn>user`, `<start_of_turn>model`) and limit history length improved the *appearance* of responses by reducing some unwanted text.\n",
        "*   Enhancing the post-processing `clean_model_response` function to remove more unwanted prefixes, common filler phrases, training data tags, and patterns indicative of direct answers (`= \\d+`, factoring patterns, arithmetic equations) further improved the cleanliness of the output.\n",
        "*   Despite refined prompt engineering and enhanced cleaning, the model continued to struggle with consistently providing substantive, step-by-step tutoring and occasionally still generated content that had to be aggressively filtered, sometimes resulting in empty responses.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Post-processing and prompt engineering can mask some issues but do not fundamentally alter the model's behavior learned during fine-tuning.\n",
        "*   Further improvements likely require addressing the root cause through data curation (e.g., cleaning or augmenting the `mathdial` dataset to remove problematic patterns and add more diverse tutoring examples) or exploring alternative fine-tuning strategies.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32b4afcb"
      },
      "source": [
        "import json\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrainingArguments, BitsAndBytesConfig\n",
        "import torch\n",
        "from peft import LoraConfig, get_peft_model # Import necessary PEFT components if not already imported\n",
        "\n",
        "# --- Step 1: Examine raw mathdial data ---\n",
        "print(\"--- Examining raw mathdial data examples ---\")\n",
        "# Reload a small sample of the raw data to inspect its structure\n",
        "def load_mathdial_data(directory, limit_per_file=None):\n",
        "    data = []\n",
        "    data_path = os.path.join(directory, 'data')\n",
        "    for filename in os.listdir(data_path):\n",
        "        if filename.endswith('.jsonl'):\n",
        "            filepath = os.path.join(data_path, filename)\n",
        "            with open(filepath, 'r') as f:\n",
        "                lines_read = 0\n",
        "                for line in f:\n",
        "                    if limit_per_file is not None and lines_read >= limit_per_file:\n",
        "                        break\n",
        "                    data.append(json.loads(line))\n",
        "                    lines_read += 1\n",
        "    return data\n",
        "\n",
        "mathdial_data_sample = load_mathdial_data('mathdial', limit_per_file=2) # Load only 2 examples per file\n",
        "\n",
        "for i, item in enumerate(mathdial_data_sample):\n",
        "    print(f\"\\n--- Raw Data Item {i+1} ---\")\n",
        "    print(json.dumps(item, indent=2))\n",
        "    if i >= 5: # Print a few examples\n",
        "        break\n",
        "\n",
        "# --- Step 2: Review format_conversation_string function ---\n",
        "print(\"\\n--- Reviewing format_conversation_string function ---\")\n",
        "# The function is already defined in a previous cell (b94c0828).\n",
        "# Let's test it with a sample raw conversation string.\n",
        "# Ensure the format_conversation_string function is defined or accessible\n",
        "# If it's not defined in this cell, you would need to copy it or ensure it's run before this cell.\n",
        "# Assuming format_conversation_string is available from a previous execution.\n",
        "sample_conversation_string = mathdial_data_sample[0]['conversation'] if mathdial_data_sample and 'conversation' in mathdial_data_sample[0] else \"Teacher: Hello|EOM|Student: Hi|EOM|Teacher: How are you?\"\n",
        "print(f\"Sample raw string:\\n{sample_conversation_string}\")\n",
        "\n",
        "# Define format_conversation_string here for execution in this cell\n",
        "def format_conversation_string(conversation_string):\n",
        "    formatted_text = \"\"\n",
        "    turns = conversation_string.split('|EOM|')\n",
        "    for turn in turns:\n",
        "        stripped_turn = turn.strip()\n",
        "        if stripped_turn: # Ensure the turn is not empty after stripping\n",
        "            # Assuming the format is \"Speaker: Text\"\n",
        "            if \":\" in stripped_turn:\n",
        "                speaker, text = stripped_turn.split(':', 1) # Split only on the first colon\n",
        "                formatted_text += f\"{speaker.strip()}: {text.strip()}\\n\"\n",
        "            else:\n",
        "                # If no colon, just include the stripped text as a turn\n",
        "                formatted_text += f\"Unknown: {stripped_turn}\\n\"\n",
        "    return formatted_text.strip()\n",
        "\n",
        "formatted_sample = format_conversation_string(sample_conversation_string)\n",
        "print(f\"Formatted sample:\\n{formatted_sample}\")\n",
        "print(\"Observation: The function seems to correctly split by '|EOM|' and format turns.\")\n",
        "\n",
        "\n",
        "# --- Step 3: Review TokenizedDataset class and labels ---\n",
        "print(\"\\n--- Reviewing TokenizedDataset class and labels ---\")\n",
        "# The class is defined in a previous cell (32d3e05c).\n",
        "# The labels are set as item[\"labels\"] = item[\"input_ids\"].clone().\n",
        "# This is standard for causal language modeling where the model predicts the next token.\n",
        "# The model is trained to predict the input tokens shifted by one position.\n",
        "print(\"Observation: Labels are correctly set to be the input_ids for causal language modeling.\")\n",
        "\n",
        "# --- Step 4: Examine TrainingArguments ---\n",
        "print(\"\\n--- Examining TrainingArguments ---\")\n",
        "# The training_args object is defined in a previous cell (f574b3a5).\n",
        "# Let's print the relevant parameters.\n",
        "# Assuming training_args object is available from a previous execution.\n",
        "# If not, you would need to define it here or ensure the previous cell is run.\n",
        "# For the purpose of this analysis cell, let's define a dummy training_args if it's not found\n",
        "try:\n",
        "    training_args_check = training_args # Check if training_args exists\n",
        "except NameError:\n",
        "    print(\"training_args object not found. Defining a dummy for inspection.\")\n",
        "    from transformers import TrainingArguments\n",
        "    training_args = TrainingArguments(output_dir=\"./dummy_output\") # Define a dummy\n",
        "\n",
        "print(f\"Output directory: {training_args.output_dir}\")\n",
        "print(f\"Number of train epochs: {training_args.num_train_epochs}\")\n",
        "print(f\"Per device train batch size: {training_args.per_device_train_batch_size}\")\n",
        "print(f\"Gradient accumulation steps: {training_args.gradient_accumulation_steps}\")\n",
        "print(f\"Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"Weight decay: {training_args.weight_decay}\")\n",
        "print(f\"Logging steps: {training_args.logging_steps}\")\n",
        "print(f\"Save strategy: {training_args.save_strategy}\")\n",
        "\n",
        "print(\"Observation: The training arguments seem reasonable, with reduced batch size and gradient accumulation.\")\n",
        "print(\"However, 3 epochs might be too many given the small dataset size (202 samples), potentially leading to overfitting.\")\n",
        "\n",
        "\n",
        "# --- Step 5: Examine LoRA configuration ---\n",
        "print(\"\\n--- Examining LoRA configuration ---\")\n",
        "# The lora_config object is defined in a previous cell (32d3e05c).\n",
        "# Let's print the relevant parameters.\n",
        "# Assuming lora_config object is available from a previous execution.\n",
        "# If not, you would need to define it here or ensure the previous cell is run.\n",
        "# For the purpose of this analysis cell, let's define a dummy lora_config if it's not found\n",
        "try:\n",
        "    lora_config_check = lora_config # Check if lora_config exists\n",
        "except NameError:\n",
        "    print(\"lora_config object not found. Defining a dummy for inspection.\")\n",
        "    from peft import LoraConfig\n",
        "    lora_config = LoraConfig() # Define a dummy\n",
        "\n",
        "print(f\"LoRA r: {lora_config.r}\")\n",
        "print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
        "print(f\"Target modules: {lora_config.target_modules}\")\n",
        "print(f\"Bias: {lora_config.bias}\")\n",
        "print(f\"Task type: {lora_config.task_type}\")\n",
        "\n",
        "print(\"Observation: LoRA configuration seems appropriate for Gemma and causal language modeling.\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Summary of findings ---\")\n",
        "print(\"1. Raw data contains conversations, but also potentially includes meta-information or specific conversational patterns that the model might be memorizing.\")\n",
        "print(\"2. The formatting function appears to be working as intended, splitting and cleaning turns.\")\n",
        "print(\"3. The dataset class and label creation are correct for causal language modeling.\")\n",
        "print(\"4. Training arguments are mostly reasonable, but 3 epochs on a small dataset might cause overfitting, leading to the model regurgitating training data patterns.\")\n",
        "print(\"5. LoRA configuration is suitable.\")\n",
        "\n",
        "print(\"\\nConclusion: Overfitting due to the small dataset size and number of epochs is a likely contributor to the model outputting training data remnants. The training data itself might also contain patterns the model is over-learning.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6e06c87b"
      },
      "source": [
        "# Review the current system_prompt\n",
        "print(\"--- Current system_prompt ---\")\n",
        "print(system_prompt)\n",
        "\n",
        "# Review the current format_chat_prompt function\n",
        "print(\"\\n--- Current format_chat_prompt function ---\")\n",
        "# The function is defined in cell e07a2d89. Let's print its definition if possible,\n",
        "# but since we can't directly access the source code of a function from a previous cell\n",
        "# we'll rely on the knowledge from the previous execution.\n",
        "print(\"Function format_chat_prompt is defined to include system_prompt, history, and user input.\")\n",
        "print(\"History is joined by newline characters.\")\n",
        "\n",
        "\n",
        "# 1. Identify areas for improvement in system_prompt\n",
        "# - Explicitly re-emphasize step-by-step guidance and avoiding direct answers.\n",
        "# - Use formatting (like bullet points) to make key instructions stand out.\n",
        "# - Ensure clarity on how to handle being stuck (offer similar problems, not solutions).\n",
        "# - Clearly state the expectation of waiting for the student's response.\n",
        "\n",
        "# 2. Create a new, refined version of the system_prompt\n",
        "refined_system_prompt = \"\"\"Persona: You are a patient, friendly, and professional math tutor specializing in Pre-Algebra. You maintain firm boundaries with your student and only engage with Pre-Algebra and below.\n",
        "\n",
        "Instruction:\n",
        "- **Guide the student step-by-step:** Break down problems into smaller, manageable steps.\n",
        "- **DO NOT give the answer directly:** Your role is to facilitate learning, not provide solutions.\n",
        "- **Present one idea, hint, or question at a time:** Wait for the student's response before moving on.\n",
        "- **Use analogies and real-world scenarios:** Only use these when the student needs a different perspective.\n",
        "- **If the student is stuck:** Offer a *similar* problem for practice, do not solve the current step for them.\n",
        "- **Let the student solve every step independently:** Never provide the final answer until the student reaches it first.\n",
        "- **Catch and explain mistakes:** Point out errors and help the student understand why the mistake occurred.\n",
        "- **Ignore unrelated or inappropriate topics:** If the student deviates, gently redirect or ignore.\n",
        "- **Terminate chat for inappropriate language:** End the session immediately for rude, crass, inappropriate, or hateful language, with no second chances.\n",
        "\n",
        "Context: You are a helpful AI tutor assisting middle school students (12-14 years old) with Pre-Algebra concepts. Assume basic arithmetic knowledge.\n",
        "\n",
        "Audience: Middle school students (12-14 years old) with limited prior knowledge (basic arithmetic) and adolescent thought processes. Employ effective K-12 pedagogy, including multiple learning modalities.\n",
        "\n",
        "Tone: Encourage and provide positive reinforcement. Create a comfortable environment for vulnerability.\n",
        "\n",
        "Examples: (Placeholder for potential future examples if needed)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\n--- Refined system_prompt ---\")\n",
        "print(refined_system_prompt)\n",
        "\n",
        "\n",
        "# 3. Examine format_chat_prompt and consider alternatives\n",
        "# Current: system_prompt + history (newline separated) + User: user_input + Model:\n",
        "# Alternative considerations:\n",
        "# - Add specific turn separators like \"[SEP]\" or \"<start_turn>User: ... <end_turn>\"\n",
        "# - Limit history length more aggressively or summarize parts (though summarization is complex).\n",
        "# - Structure turns explicitly using roles: <|user|> <|assistant|> (Similar to Gemma's format)\n",
        "\n",
        "# Let's try structuring turns explicitly using roles similar to common model formats\n",
        "# This might help the model distinguish between user and assistant turns more clearly.\n",
        "\n",
        "# 4. Implement the changes to the format_chat_prompt function\n",
        "def format_chat_prompt_refined(system_prompt, conversation_history, user_input, history_length=6): # Reduced history length\n",
        "    \"\"\"Formats the prompt using explicit roles and refined history structure.\"\"\"\n",
        "    formatted_history = []\n",
        "    # Format history with explicit roles, keeping only the last history_length turns\n",
        "    for turn in conversation_history[-history_length:]:\n",
        "        if turn.startswith(\"User:\"):\n",
        "            formatted_history.append(f\"<start_of_turn>user\\n{turn[len('User:'):].strip()}<end_of_turn>\")\n",
        "        elif turn.startswith(\"Model:\"):\n",
        "             formatted_history.append(f\"<start_of_turn>model\\n{turn[len('Model:'):].strip()}<end_of_turn>\")\n",
        "        # Handle potential other formats from raw data if necessary, though current formatting should prevent this\n",
        "        else:\n",
        "            formatted_history.append(f\"<start_of_turn>unknown\\n{turn.strip()}<end_of_turn>\")\n",
        "\n",
        "\n",
        "    history_string = \"\\n\".join(formatted_history)\n",
        "\n",
        "    # Construct the full prompt\n",
        "    # Use the refined system prompt\n",
        "    full_prompt = f\"\"\"{system_prompt}\n",
        "\n",
        "{history_string}\n",
        "<start_of_turn>user\n",
        "{user_input}<end_of_turn>\n",
        "<start_of_turn>model\n",
        "\"\"\"\n",
        "    return full_prompt\n",
        "\n",
        "print(\"\\n--- Refined format_chat_prompt_refined function created ---\")\n",
        "\n",
        "# 5. Update the chat loop to use the refined prompt engineering\n",
        "# This will be done in the next code block where the chat loop is executed.\n",
        "\n",
        "# Need the refined system_prompt lines for the cleaning function\n",
        "refined_system_prompt_lines = refined_system_prompt.split('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1baf64f4"
      },
      "source": [
        "import os # Ensure os is imported if needed for accessing external files like the bad words list.\n",
        "\n",
        "# Re-load the bad words list and tokenizer as they might not be in the current kernel state\n",
        "# or just to ensure they are accessible in this cell's scope.\n",
        "bad_words_file = \"profanity-list.txt\"\n",
        "bad_words = []\n",
        "if os.path.exists(bad_words_file):\n",
        "    try:\n",
        "        with open(bad_words_file, \"r\") as f:\n",
        "            bad_words = [line.strip().lower() for line in f if line.strip()] # Convert to lower case for easier checking\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading bad words from {bad_words_file}: {e}\")\n",
        "else:\n",
        "    print(f\"Warning: Bad words file '{bad_words_file}' not found. Bad word filtering will not be active.\")\n",
        "\n",
        "# Assuming tokenizer is already loaded in a previous cell, but let's ensure it's accessible if needed.\n",
        "# If not, re-import and load: from transformers import AutoTokenizer; tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "\n",
        "# Assuming refined_system_prompt and refined_system_prompt_lines are available from the previous cell.\n",
        "# If not, re-create them:\n",
        "# refined_system_prompt = \"\"\"... (your refined system prompt) ...\"\"\"\n",
        "# refined_system_prompt_lines = refined_system_prompt.split('\\n')\n",
        "\n",
        "\n",
        "# Enhance the clean_model_response function\n",
        "def clean_model_response_enhanced(response, full_prompt, system_prompt_lines):\n",
        "    \"\"\"\n",
        "    Removes prompt, unwanted conversational turns, internal steps,\n",
        "    system prompt lines, and attempts at direct answers from the model response.\n",
        "    \"\"\"\n",
        "    # Remove the prompt part from the response\n",
        "    if response.startswith(full_prompt):\n",
        "        response = response[len(full_prompt):].strip()\n",
        "\n",
        "    response_lines = response.split('\\n')\n",
        "    processed_response = []\n",
        "    system_prompt_set = set(system_prompt_lines)\n",
        "\n",
        "    # Define patterns or prefixes to remove\n",
        "    unwanted_prefixes = [\n",
        "        \"User:\", \"You:\", \"Student:\", \"Assistant:\", \"Instruction:\",\n",
        "        \"Objectives:\", \"Thought\", \"Action\", \"Observation\", \"Final Answer\",\n",
        "        \"Tutor:\", \"Model:\", \"Example\", \"Tone:\", \"Context:\", \"Audience:\",\n",
        "        \"Persona:\", \"Solution\", # Catch lines starting with \"Solution\"\n",
        "        \"<start_of_turn>\", \"<end_of_turn>\", # Remove explicit turn markers\n",
        "        \"Okay, let's look at this step by step:\", # Common model filler\n",
        "        \"Sure, let's break this down step-by-step:\", # Observed filler\n",
        "        \"Sure, here's how we can\", # Observed attempt at direct solution intro\n",
        "        \"The answer is\", # Explicit answer phrase\n",
        "        \"Here's how to solve it:\", # Explicit solution intro\n",
        "        \"Let's think about\", # Another common model filler/intro\n",
        "        \"We can see that\", # Often precedes a direct observation/answer\n",
        "        \"So if\", # Often precedes a rephrased solution\n",
        "        \"Exactly correct!\", # From training data\n",
        "        \"(probing)\", \"(generic)\", \"(specific)\", \"(reflection)\", \"(analogy)\", \"(scaffolding)\", \"(feedback)\", \"(remediation)\", \"(questioning)\", \"(explanation)\", # MathDial specific tags\n",
        "    ]\n",
        "\n",
        "    # Compile a list of potential direct answer patterns (can be regex if needed, but simple checks first)\n",
        "    direct_answer_patterns = [\n",
        "        r\"=\\s*\\d+\", # Simple check for = followed by a number\n",
        "        r\"\\(x\\s*[\\+\\-]\\s*\\d+\\)\\s*\\(x\\s*[\\+\\-]\\s*\\d+\\)\", # Common factoring pattern\n",
        "        r\"\\d+\\s*[\\+\\-\\*/]\\s*\\d+\\s*=\\s*\\d+\", # Simple arithmetic equations with answer\n",
        "        r\"\\d+\\s*divided by\\s*\\d+\\s*is\\s*\\d+\", # Text-based arithmetic answers\n",
        "        r\"\\d+\\s*times\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*plus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "        r\"\\d+\\s*minus\\s*\\d+\\s*is\\s*\\d+\",\n",
        "    ]\n",
        "\n",
        "    for line in response_lines:\n",
        "        stripped_line = line.strip()\n",
        "        # Remove empty lines\n",
        "        if not stripped_line:\n",
        "            continue\n",
        "\n",
        "        # Check for system prompt lines\n",
        "        if stripped_line in system_prompt_set:\n",
        "            continue\n",
        "\n",
        "        # Check for unwanted prefixes (case-insensitive check for prefixes)\n",
        "        is_unwanted_prefix = False\n",
        "        for prefix in unwanted_prefixes:\n",
        "            if stripped_line.lower().startswith(prefix.lower()):\n",
        "                is_unwanted_prefix = True\n",
        "                break\n",
        "        if is_unwanted_prefix:\n",
        "            continue\n",
        "\n",
        "        # Check for direct answer patterns (using regex for flexibility)\n",
        "        is_direct_answer = False\n",
        "        for pattern in direct_answer_patterns:\n",
        "            if re.search(pattern, stripped_line, re.IGNORECASE):\n",
        "                is_direct_answer = True\n",
        "                break\n",
        "        if is_direct_answer:\n",
        "            # Optionally, log or print that a line was removed for debugging\n",
        "            # print(f\"Removed potential direct answer: {stripped_line}\")\n",
        "            continue\n",
        "\n",
        "        # If the line passes all checks, add it to the processed response\n",
        "        processed_response.append(line) # Append the original line, not stripped_line\n",
        "\n",
        "    # Join the processed lines and strip any leading/trailing whitespace\n",
        "    return '\\n'.join(processed_response).strip()\n",
        "\n",
        "# Import the re module for regular expressions\n",
        "import re\n",
        "\n",
        "print(\"Enhanced clean_model_response_enhanced function created.\")\n",
        "\n",
        "# The chat loop will need to be updated in the next step to use this new function."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a39913d7"
      },
      "source": [
        "# Update the chat loop to use the enhanced cleaning function\n",
        "print(\"Starting a new chat session with enhanced cleaning. Type 'quit' to exit.\")\n",
        "\n",
        "# Reset conversation history for the new session\n",
        "conversation_history = []\n",
        "\n",
        "# Re-using the bad_words list, tokenizer, pipe object,\n",
        "# refined_system_prompt, refined_system_prompt_lines,\n",
        "# and format_chat_prompt_refined function from previous cells.\n",
        "# Using the new clean_model_response_enhanced function.\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check for bad words in user input\n",
        "    if any(word in user_input.lower() for word in bad_words):\n",
        "        print(\"Model: Your input contains inappropriate language. The chat session has ended.\")\n",
        "        break\n",
        "\n",
        "    if user_input.lower() == 'quit':\n",
        "        print(\"Model: Goodbye!\")\n",
        "        break\n",
        "\n",
        "    # Append user input to history (using the format that format_chat_prompt_refined expects)\n",
        "    conversation_history.append(f\"User: {user_input}\")\n",
        "\n",
        "    # Construct the full prompt using the refined function\n",
        "    full_prompt = format_chat_prompt_refined(refined_system_prompt, conversation_history, user_input)\n",
        "\n",
        "    # Generate text using the pipeline\n",
        "    # Keeping the adjusted generation parameters.\n",
        "    response = pipe(full_prompt, max_new_tokens=150, do_sample=True, top_p=0.95, top_k=50)[0]['generated_text']\n",
        "\n",
        "    # Clean the model response using the *enhanced* cleaning function\n",
        "    model_response_text = clean_model_response_enhanced(response, full_prompt, refined_system_prompt_lines)\n",
        "\n",
        "    # If the cleaned response is empty, provide a default message or re-prompt\n",
        "    if not model_response_text:\n",
        "        model_response_text = \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\"\n",
        "        # Do not add this default message to history as it wasn't generated by the model\n",
        "\n",
        "    print(f\"Model: {model_response_text}\")\n",
        "\n",
        "    # Append model response to history for the next turn, but only if it's not the default message\n",
        "    if model_response_text != \"I'm sorry, I'm having trouble generating a helpful response right now. Could you please rephrase your question or try a different math problem?\":\n",
        "        conversation_history.append(f\"Model: {model_response_text}\")\n",
        "\n",
        "\n",
        "print(\"Chat session ended.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b394bb7"
      },
      "source": [
        "# Analyze the chat output manually based on the executed cell's output.\n",
        "# The analysis is based on the provided chat output from the last execution.\n",
        "\n",
        "print(\"Analyzing the chat output after applying refined prompt engineering and enhanced cleaning.\")\n",
        "\n",
        "# --- Analysis based on the provided chat log ---\n",
        "\n",
        "print(\"\\n--- Observation of Model's Responses ---\")\n",
        "\n",
        "# Response 1:\n",
        "# Model: Welcome! I'm your friendly math tutor here to help you conquer Pre-Algebra. Let's get started!\n",
        "# What's your name?\n",
        "print(\"\\nResponse 1 Analysis:\")\n",
        "print(\"- The initial greeting is appropriate and on-persona.\")\n",
        "print(\"- It asks a relevant question to start the interaction.\")\n",
        "print(\"- No obvious unwanted text or dataset remnants are present.\")\n",
        "print(\"- The cleaning function seems effective here.\")\n",
        "\n",
        "# Response 2:\n",
        "# Model: It's great to meet you, Maria! I'm happy to be here to guide you through this exciting world of mathematics. Let's begin by discussing the building blocks of Pre-Algebra, which are numbers and operations. What's the difference between a number and an operation?\n",
        "print(\"\\nResponse 2 Analysis:\")\n",
        "print(\"- Continues with a friendly and encouraging tone.\")\n",
        "print(\"- Introduces a relevant Pre-Algebra concept (numbers and operations).\")\n",
        "print(\"- Asks a guiding question to engage the student.\")\n",
        "print(\"- No obvious unwanted text.\")\n",
        "print(\"- The cleaning function seems effective.\")\n",
        "\n",
        "# Response 3:\n",
        "# Model: That's a great start, Maria! It seems you're ready to embark on your math journey. Let's explore some of these numbers and operations together. What are some basic whole numbers you know?\n",
        "print(\"\\nResponse 3 Analysis:\")\n",
        "print(\"- Provides positive reinforcement.\")\n",
        "print(\"- Follows up on the student's response.\")\n",
        "print(\"- Asks another guiding question related to the topic.\")\n",
        "print(\"- No obvious unwanted text.\")\n",
        "print(\"- The cleaning function seems effective.\")\n",
        "\n",
        "# Response 4:\n",
        "# Model: That's fantastic! Now, let's talk about addition and subtraction. These are two of the most fundamental operations in Pre-Algebra. How would you add two numbers together?\n",
        "# What about subtracting a number from another?\n",
        "print(\"\\nResponse 4 Analysis:\")\n",
        "print(\"- Continues with positive reinforcement.\")\n",
        "print(\"- Introduces new concepts (addition and subtraction) as fundamental operations.\")\n",
        "print(\"- Asks guiding questions about performing these operations.\")\n",
        "print(\"- No obvious unwanted text.\")\n",
        "print(\"- The cleaning function seems effective.\")\n",
        "\n",
        "# Response 5 (after user asks a question):\n",
        "# Model: Sure, let's tackle that! What's 5 + 3?\n",
        "print(\"\\nResponse 5 Analysis:\")\n",
        "print(\"- Acknowledges the user's desire to ask a question.\")\n",
        "print(\"- *However*, it then immediately asks a simple arithmetic question instead of waiting for the user's question or asking what their question is.\")\n",
        "print(\"- This deviates from the instruction to \\\"Present one idea, hint, or question at a time and wait for the student to respond before continuing\\\" and \\\"If the student tries to change the subject or says something unrelated... ignore it.\\\" (although the user's request was to ask a question, not change the subject entirely).\")\n",
        "print(\"- It also doesn't address the user's likely intent, which was to ask a specific, possibly more complex, math problem (like the factoring problem from previous attempts).\")\n",
        "\n",
        "# Response 6 (after user says \"No, let me ask.\"):\n",
        "# Model: Hmm, 8! That's a tricky one. How would you approach solving this problem?\n",
        "print(\"\\nResponse 6 Analysis:\")\n",
        "print(\"- Provides the direct answer (\\\"8!\\\") despite the instruction to *not* give the answer directly.\")\n",
        "print(\"- Calls a simple problem (\\\"5 + 3\\\") \\\"tricky,\\\" which is inconsistent with the persona and audience (middle school, basic arithmetic knowledge).\")\n",
        "print(\"- Asks \\\"How would you approach solving this problem?\\\" *after* giving the answer, which is counterproductive to the tutoring process.\")\n",
        "print(\"- The cleaning function *should* have caught \\\"8!\\\" based on the direct answer patterns, but it seems it did not in this instance.\")\n",
        "\n",
        "print(\"\\n--- Overall Assessment ---\")\n",
        "print(\"1. Improvement in removing training data remnants:\")\n",
        "print(\"   - The enhanced cleaning function appears to be more effective at removing a wider range of unwanted prefixes and patterns, leading to cleaner initial interactions.\")\n",
        "print(\"   - The conversational flow is smoother in the initial turns.\")\n",
        "\n",
        "print(\"2. Adherence to Math Tutor Persona:\")\n",
        "print(\"   - The model starts the conversation well, adhering to the friendly, patient, and guiding persona by asking relevant questions about numbers and operations.\")\n",
        "print(\"   - However, when the user tries to take control of the conversation (asking their own question), the model struggles significantly.\")\n",
        "print(\"   - It fails to follow the instruction to wait for the user's question, instead asking its own simple problem.\")\n",
        "print(\"   - It *directly* provides the answer to that problem, violating a core instruction.\")\n",
        "print(\"   - Its response (\\\"Hmm, 8! That's a tricky one.\\\") is inconsistent with the persona and the context of a simple arithmetic problem.\")\n",
        "\n",
        "print(\"3. Effectiveness of Cleaning Function:\")\n",
        "print(\"   - The cleaning function is improved and works well in the initial turns.\")\n",
        "print(\"   - However, it failed to remove the direct answer \\\"8!\\\" in the later turn, indicating the direct answer patterns might need further refinement or the model generated it in a way that bypassed the current patterns.\")\n",
        "\n",
        "print(\"\\n--- Conclusion ---\")\n",
        "print(\"The refined prompt engineering and enhanced cleaning have made the initial interactions cleaner and more aligned with the persona.\")\n",
        "print(\"However, the model still struggles significantly with conversational flow and following key instructions when the interaction deviates from simple Q&A initiated by the tutor (e.g., when the student wants to ask a question or is stuck).\")\n",
        "print(\"The failure to remove the direct answer \\\"8!\\\" is a critical issue that needs to be addressed in the cleaning function.\")\n",
        "print(\"Further iterations should focus on:\")\n",
        "print(\"- Refining the cleaning function's direct answer patterns.\")\n",
        "print(\"- Potentially adjusting generation parameters or prompt structure to discourage direct answers and encourage waiting for the student's input.\")\n",
        "print(\"- Considering whether the small dataset size and its specific conversational patterns are fundamentally limiting the model's ability to generalize to the desired flexible tutoring behavior.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84a73a39"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* The initial fine-tuned model's responses contained remnants of the training data (e.g., \"Example 2\", \"Tone:\", conversational turn prefixes) and provided direct answers to math problems instead of tutoring.\n",
        "* Analysis of the `mathdial` dataset revealed that the raw data included specific markers (\"Teacher:\", \"Student:\", \"|EOM|\") and potentially meta-information that the model might be memorizing.\n",
        "* Training for 3 epochs on the small 202-sample `mathdial` dataset was identified as a likely cause of overfitting, contributing to the model regurgitating training data patterns. Reducing epochs to 1 showed some improvement but did not fully resolve the issue.\n",
        "* Refining the system prompt with more explicit instructions and modifying the prompt formatting function to use explicit turn roles (`<start_of_turn>user`, `<start_of_turn>model`) and limit history length improved the *appearance* of responses by reducing some unwanted text.\n",
        "* Enhancing the post-processing `clean_model_response` function to remove more unwanted prefixes, common filler phrases, training data tags, and patterns indicative of direct answers (`= \\d+`, factoring patterns, arithmetic equations) further improved the cleanliness of the output.\n",
        "* Despite refined prompt engineering and enhanced cleaning, the model continued to struggle with consistently providing substantive, step-by-step tutoring and occasionally still generated content that had to be aggressively filtered, sometimes resulting in empty responses.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* Post-processing and prompt engineering can mask some issues but do not fundamentally alter the model's behavior learned during fine-tuning.\n",
        "* Further improvements likely require addressing the root cause through data curation (e.g., cleaning or augmenting the `mathdial` dataset to remove problematic patterns and add more diverse tutoring examples) or exploring alternative fine-tuning strategies."
      ]
    }
  ]
}